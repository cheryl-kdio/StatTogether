{
  "hash": "48df67c521eea36b6c49062ba02177d0",
  "result": {
    "markdown": "---\ntitle: \"TD - Séries temporelles 1\"\nsidebar: auto\nauthor:\n  - Cheryl Kouadio\n---\n\n\nDans cette section, il s'agit d'une proposition de réponse au TD de Séries temporelles 1 dispensé par Mr José GÓMEZ GARCÍA à l'ENSAI en 2023-2024 au deuxième année.\n\nVous trouverez l'énoncé du td via le lien ci-joint .\n\n## Exercice 1\n\nSoit $\\epsilon_t$ un bruit blanc. Pour chacun des processus suivants, dire s'il s'agit d'un processus ARMA stationnaire. Si oui, déterminer les ordres p et q et préciser si le processus admet une représentation AR($\\infty$) et/ou MA($\\infty$) et si la représentation ARMA est minimale.\n\n### 1.\n\nNous savons que : \\begin{equation*}\n\\begin{aligned}\nX_t &= \\frac{5}{6} X_{t-1} - \\frac{1}{6} X_{t-2} + \\epsilon_t + \\epsilon_{t-1} \\\\\n&\\Leftrightarrow X_t - \\frac{5}{6} X_{t-1} + \\frac{1}{6} X_{t-2} = \\epsilon_t + \\epsilon_{t-1} \\\\\n&\\Leftrightarrow (I - \\frac{5}{6}B + \\frac{1}{6}B^2) X_t = (1 + B) \\epsilon_t \\\\\n&\\Leftrightarrow \\Phi(B)X_t = \\Psi(B) \\epsilon_t\n\\end{aligned}\n\\end{equation*}\n\nDéterminons les racines de $\\Phi(B)$ : \\begin{equation*}\n\\begin{aligned}\n\\Phi(B) &= I-\\frac{5}{6}B + \\frac{1}{6}B^2  \\\\\n&= \\frac{1}{6}(B^2 - 5B + 6I) \\\\\n&= \\frac{1}{6} (B - 2I)(B - 3I)  \\\\\n&= (I - \\frac{1}{2}B)(I - \\frac{1}{3}B)\n\\end{aligned}\n\\end{equation*}\n\nLes racines de $\\Phi$ sont de modules 2 et 3, de modules supérieurs à 1 donc, $X_t$ est un processus ARMA(2,1) stationnaire. De plus, il admet une représentation MA($\\infty$).\n\n-   $\\Psi(B) = (I+B)$ admet comme racine (évidente) 1. Donc $X_t$ n'admet aucune représentation AR($\\infty$).\n\nComme $\\Phi(B)$ et $\\Psi(B)$ n'admettent aucune racine commune, on ne peut réduire la représentation. le processus ARMA est donc minimale.\n\nD'accord, je vais tenter de transcrire les équations et commentaires de votre image dans un format similaire à celui que vous avez fourni précédemment.\n\n### 2.2\n\nNous pouvons voir que $X_t$ suit une $AR(\\infty)$: \\begin{equation*}\n\\begin{aligned}\n\\sum_{k=0}^{\\infty} \\frac{1}{2^k} X_{t-k} &= \\epsilon_t \\\\  \n&\\Leftrightarrow (\\sum_{k=0}^{\\infty} (\\frac{B}{2})^k ) X_t = \\epsilon_t \\\\\n&\\Leftrightarrow \\frac{1}{I-\\frac{B}{2}}X_t = \\epsilon_t\\\\\n&\\Leftrightarrow X_t=(I-\\frac{B}{2})\\epsilon_t\n\\end{aligned}\n\\end{equation*} Ainsi, $X_t$ est un processus MA(1). De plus, la racine de $(I-\\frac{B}{2})$ est 2 $\\geq 1$, donc ce processus ARMA(0,1) est stationnaire et admet uniquement une représentation AR($\\infty$).\n\n### 2.3\n\n\\begin{equation*}\n\\begin{aligned}\nX_t &= \\frac{5}{4} X_{t-1} - \\frac{1}{4} X_{t-2} + \\epsilon_t + 2 \\epsilon_{t-1} \\\\\n&\\Leftrightarrow X_t - \\frac{5}{4} X_{t-1} + \\frac{1}{4} X_{t-2} &= \\epsilon_t + 2 \\epsilon_{t-1} \\\\\n&\\Leftrightarrow (1 - \\frac{5}{4}B + \\frac{1}{4}B^2)X_t &= (1 + 2B) \\epsilon_t \\\\\n&\\Leftrightarrow (1 - B)(1-\\frac{1}{4}B)X_t &= (1 + 2B) \\epsilon_t \\\\\n&\\Leftrightarrow \\Phi(B)X_t = \\Psi(B) \\epsilon_t\n\\end{aligned}\n\\end{equation*} $\\Phi(B)$ admet pour racine de 1 et 4. Comme l'une des racines est de module égal à 1, $X_t$ n'est pas stationnaire.\n\n### 2.4\n\n\\begin{equation*}\n\\begin{aligned}\nX_t &= 2X_{t-1} - X_{t-2} + \\epsilon_t + \\frac{1}{4}\\epsilon_{t-1} \\\\\n&\\Leftrightarrow X_t - 2X_{t-1} + X_{t-2} &= \\epsilon_t + \\frac{1}{4}\\epsilon_{t-1} \\\\\n&\\Leftrightarrow (1 - 2B + B^2)X_t &= (1 + \\frac{B}{4}) \\epsilon_t \\\\\n&\\Leftrightarrow \\Phi(B)X_t = \\Psi(B) \\epsilon_t\n\\end{aligned}\n\\end{equation*} $\\Phi(B)$ admet comme racine (évidente) 1 donc $X_t$ n'est pas stationnaire.\n\n## Exercice 2\n\n### 2.1\n\n$(X_t) \\sim ARIMA(2,1,1)$ $\\Rightarrow Y_t \\sim ARMA(1,1)$ avec $Y_t = (I-B)X_t$.\n\n$Y_t \\sim ARMA(1,1) \\Leftrightarrow Y_t = c + \\phi_1 Y_{t-1} + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1}$\n\nOr :\n\n\n```{=tex}\n\\begin{equation*}\n\\begin{aligned}\nY_t = X_t - X_{t-1} &= c + \\phi_1 Y_{t-1} + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1}\\\\\n&\\Leftrightarrow X_t - X_{t-1} = c + \\phi_1 (X_{t-1} - X_{t-2}) + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} \\\\\n&\\Leftrightarrow X_t - X_{t-1} = c + \\phi_1 X_{t-1} - \\phi_1 X_{t-2} + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} \\\\\n&\\Leftrightarrow X_t  = c + (1-\\phi_1) X_{t-1} - \\phi_1 X_{t-2} + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} \\\\\n\\end{aligned}\n\\end{equation*}\n```\n\n### 2.2\n\n$(X_t) \\sim ARI(2,1)$ $\\Rightarrow Y_t \\sim AR(1)$ avec $Y_t = (I-B)X_t$.\n\n$Y_t \\sim AR(1) \\Leftrightarrow Y_t = c + \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2}$\n\nOr :\n\n\n```{=tex}\n\\begin{equation*}\n\\begin{aligned}\nY_t = X_t - X_{t-1} &= c +\\phi_1 Y_{t-1} + \\phi_2 Y_{t-2}\\\\\n&\\Leftrightarrow X_t - X_{t-1} = c + \\phi_1 (X_{t-1} - X_{t-2}) + \\phi_2 (X_{t-2} - X_{t-3}) \\\\\n&\\Leftrightarrow X_t = c + (\\phi_1+1) (X_{t-1} + (\\phi_2-\\phi_1)X_{t-2} - \\phi_2 X_{t-3}) \\\\\n\\end{aligned}\n\\end{equation*}\n```\n\n### 2.3\n\n$(X_t) \\sim I(2)$ $\\Rightarrow Y_t = (I-B)X_t$ est stationnaire.\n\nSelon le théorème de Wold, on peut écrire donc:\n\n$$\nY_t = c + \\sum_{j=0}^{\\infty} \\psi_j \\varepsilon_{t-j} + V_t\n$$\n\nOù $V_t$ est connu tel que $cov(\\varepsilon_t,V_s) = 0$ pour $s \\neq t$.\n\nAinsi, \\begin{equation*}\n\\begin{aligned}\nY_t = X_t - X_{t-1} &= c + \\sum_{j=0}^{\\infty} \\psi_j \\varepsilon_{t-j} + V_t\\\\\n&\\Leftrightarrow X_t  = c + X_{t-1}+ \\sum_{j=0}^{\\infty} \\psi_j \\varepsilon_{t-j} + V_t\\\\\n\\end{aligned}\n\\end{equation*}\n\n### 2.4\n\n$(X_t)$ est $SARIMA(0,2,1)*(1,1,0)_7$\n\nPosons:\n\n$$ Y_t = (1-B^7)^1(1- B)^0X_t =(1-B^7)X_t=X_t - X_{t-7} $$\n\nNous savons que cette équation est une ARMA de forme :\n\n$$\n\\Phi_7^1(B^7)\\Phi^2(B)Y_t = \\Theta_7^0(B^7)\\Theta^1(B)\\varepsilon_t\n$$\n\nOù $\\Phi$ et $\\Theta$ sont les polynômes AR et MA, respectivement.\n\nNous avons donc : $$\n(1 - \\phi B^7)(1 -\\alpha_1 B - \\alpha_2 B^2)Y_t = 1*(1 - \\theta_1 B)\\varepsilon_t\n$$ $$\n(1 - \\phi B^7)(1 -\\alpha_1 B - \\alpha_2 B^2)(X_t - X_{t-7} ) = 1*(1 - \\theta_1 B)\\varepsilon_t\n$$\n\n### 2.5\n\n$(X_t)$ est $SARMA(2,0)*(1,2)_{12}$\n\nPosons:\n\n$$ Y_t = (1-B^{12})^0(1- B)^0X_t =X_t $$\n\nNous savons que cette équation est une ARMA de forme :\n\n$$\n\\Phi_{12}^1(B^{12})\\Phi^2(B)Y_t = \\Theta_{12}^2(B^7)\\Theta^0(B)\\varepsilon_t\n$$\n\nOù $\\Phi$ et $\\Theta$ sont les polynômes AR et MA, respectivement.\n\nNous avons donc : $$\n(1 - \\phi B^{12})(1 -\\alpha_1 B - \\alpha_2 B^2)Y_t = (1 - \\theta_1 B^{12}-\\theta_2 B^{24})\\varepsilon_t\n$$ $$\n(1 - \\phi B^{12})(1 -\\alpha_1 B - \\alpha_2 B^2)X_t = (1 - \\theta_1 B^{12}-\\theta_2 B^{24})\\varepsilon_t\n$$\n\n## Exercice 3\n\n### 3.1\n\nSoit $M = (I - B)(I - B^2)$ Ainsi, $$M(a + bt + ct^2) = (I - B)(I - B^2)(a + bt + ct^2) = (I - B-B^2+B^3)(a + bt + ct^2) $$\n\nAppliquons M à chaque composantes de $a + bt + ct^2$: 1. M appliqué à $a$:\n\n$$M(a)=a-a-a+a=0$$\n\n2.  M appliqué à $bt$\n\n$$ M(bt) = bt - b(t-1) - b(t-2) + b(t-3)$$ $$ M(bt) = bt - bt + b - bt + 2b + bt - 3b $$ $$ M(bt) = 0$$\n\n3.  M appliqué à $ct^2$ $$ M(ct^2) = ct^2 - c(t-1)^2 - c(t-2)^2 + c(t-3)^2$$ $$ M(bt) =  ct^2 - c(t^2 - 2t + 1) - c(t^2 - 4t + 4) + c(t^2 - 6t + 9)  $$ $$ M(bt) = -c-4c+9c  $$ $$ M(bt) = 4c  $$\n\nEn combinant les résultats de M appliqué à $a, bt$ et $ct^2$, nous trouvons :\n\n$$ M(a + bt + ct^2) = 0 + 0 + 4c = 4c $$\n\nCela confirme que $M$ transforme le polynôme de degré 2 en une constante $4c$.\n\n### 3.2\n\n$$ M(\\cos(\\pi t)) = \\cos(\\pi t) - \\cos(\\pi(t-2)) - \\cos(\\pi(t-1)) +  \\cos(\\pi(t-3))  $$ $$ M(\\cos(\\pi t)) = \\cos(\\pi t) - \\cos(\\pi-2\\pi) - \\cos(\\pi t- \\pi)) +  \\cos(\\pi t-3\\pi)  $$ $$ M(\\cos(\\pi t)) = \\cos(\\pi t) - \\cos(\\pi t) +  \\cos(\\pi t)) -  \\cos(\\pi )  $$ $$ M(\\cos(\\pi t)) = 0 $$\n\nDOnc M absorbe les saisonnalités $cos(\\pi t)$\n\n### 3.3\n\n$$ E(X_t)=E(a+bt+ct^2+cos(\\pi t) + \\varepsilon_t)$$ $$ E(X_t)=a+bt+ct^2+cos(\\pi t) + E(\\varepsilon_t))$$ $$ E(X_t)=a+bt+ct^2+cos(\\pi t) $$ car $\\varepsilon_t$ est un bruit blanc.\n\nIci, l'espérance dépend du temps donc $X_t$ n'est pas stationnaire\n\n### 3.4\n\nSoit $Y_t=M(X_t)$ On a donc : $$Y_t=M(a+bt+ct^2)+M(cos(\\pi t))+M(\\varepsilon_t)$$ $$Y_t=4c+0+\\varepsilon_t-\\varepsilon_{t-1}-\\varepsilon_{t-2}+\\varepsilon_{t-3}$$ $$Y_t=4c-\\sum_{i=0}^{3} \\theta_i \\varepsilon_{t-i})$$, avec $\\theta_0=-1$, $\\theta_1=1$,$\\theta_2=1$ et $\\theta_3=-1$\n\nDonc $Y_t$ est bien un processus MA(3).\n\n### i\n\n-   $$E(Y_t)=4c$$\n-   Comme $Y_t$ est un processus MA(3), sa fonction d'autocovariance s'écrit : $$Cov(Y_t,Y_{t+h}=\\sigma^2(\\sum_{j=0}^{3-h} \\theta_j \\theta_{j+h})$$\n\nAinsi, $$\\gamma(h)=\n\\begin{cases} \n\\sigma^2 & \\text{si } h = 0 \\\\\n\\sigma^2(-1+1-1) & \\text{si } |h| = 1 \\\\\n\\sigma^2(-1-1) & \\text{si } |h| = 2 \\\\\n\\sigma^2(1) & \\text{si } |h| = 3 \\\\\n0& \\text{si } |h| >3 \n\\end{cases}\n$$\n\n$$\\gamma(h)=\n\\begin{cases} \n\\sigma^2 & \\text{si } h = 0 \\\\\n-\\sigma^2 & \\text{si } |h| = 1 \\\\\n-2\\sigma^2 & \\text{si } |h| = 2 \\\\\n\\sigma^2 & \\text{si } |h| = 3 \\\\\n0& \\text{si } |h| >3 \n\\end{cases}\n$$\n\n### ii\n\n$Y_t=(I-B)(I-B^2)X_t$ avec $Y_t \\sim MA(3)$\n\nAinsi, on identifie facilement un processus SARIMA : $$Y_t=(I-B^{s=2})^{D=1}(I-B)^{d=1}X_t \\sim SARIMA(0,1,3)*(0,1,0)_2$$\n\n## Exercice 4\n\n### 4.1\n\nNous avons que : \\begin{equation*}\n\\begin{aligned}\nX_t &= \\frac{1}{2} X_{t-1} + \\varepsilon_t - \\frac{1}{4}\\varepsilon_{t-1} \\\\\n&\\Leftrightarrow X_t - \\frac{1}{2} X_{t-1} = \\varepsilon_t - \\frac{1}{4}\\varepsilon_{t-1} \\\\\n&\\Leftrightarrow (I - \\frac{1}{2}B ) X_t = (1 - \\frac{1}{4} B) \\epsilon_t \\\\\n&\\Leftrightarrow \\Phi(B)X_t = \\Psi(B) \\epsilon_t\n\\end{aligned}\n\\end{equation*}\n\nLa racine de $\\Phi$ est 2 et la racine de $\\Psi$ est 4. Les deux polynômes n'admettent aucune racine commune, donc le processus est minimale.\n\n### 4.2\n\nDéterminons d'abord l'inverse de $\\Phi(B)=(I - \\frac{1}{2}B )$. $\\Phi(B)$ est inversible car c'est un polynôme de racine de module 2 \\> 1. Ainsi, $\\Phi(B)^{-1}=\\sum_{i=0}^{\\infty} (\\frac{B}{2})^i$. De fait, nous avons : $$X_t=\\Phi(B)^{-1}(\\Psi(B)\\varepsilon_t)$$ $$X_t=\\sum_{i=0}^{\\infty} (\\frac{B}{2})^i( \\varepsilon_t - \\frac{1}{4}\\varepsilon_{t-1})$$ $$X_t=\\sum_{i=0}^{\\infty} (\\frac{1}{2})^i( \\varepsilon_{t-i} - \\frac{1}{4}\\varepsilon_{t-1-i})$$ $$X_t=\\sum_{i=0}^{\\infty} \\frac{1}{2^i} \\varepsilon_{t-i} - \\sum_{i=0}^{\\infty} \\frac{1}{2^{i+2}}\\varepsilon_{t-1-i})$$ À l'aide d'un changement de variable j=j-1, on a : $$X_t=\\sum_{i=0}^{\\infty} \\frac{1}{2^i} \\varepsilon_{t-i} - \\sum_{i=1}^{\\infty} \\frac{1}{2^{i+1}}\\varepsilon_{t-i})$$ $$X_t=\\varepsilon_t + \\sum_{i=1}^{\\infty} \\frac{1}{2^i} \\varepsilon_{t-i} - \\sum_{i=1}^{\\infty} \\frac{1}{2^{i+1}}\\varepsilon_{t-i})$$ $$X_t=\\varepsilon_t + \\sum_{i=1}^{\\infty} \\frac{1}{2^{i+1}}\\varepsilon_{t-i})$$\n\n### 4.3\n\n$$Cov(X_t,X_{t+h})=Cov(\\varepsilon_t + \\sum_{j=1}^{\\infty} \\frac{1}{2^{j+1}}\\varepsilon_{t-j}\\ , \\ \\varepsilon_{t+h} + \\sum_{i=1}^{\\infty} \\frac{1}{2^{i+1}}\\varepsilon_{t-i+h})$$ $$Cov(X_t,X_{t+h})=Cov(\\varepsilon_t,\\varepsilon_{t+h})+\\sum_{i=1}^{\\infty} \\frac{1}{2^{i+1}}Cov(\\varepsilon_t,\\varepsilon_{t+h-i})+\\sum_{j=1}^{\\infty} \\frac{1}{2^{j+1}}Cov(\\varepsilon_{t-j},\\varepsilon_{t+h})+\\sum_{j=1}^{\\infty}\\sum_{i=1}^{\\infty} \\frac{1}{2^{j+1}}\\frac{1}{2^{i+1}}Cov(\\varepsilon_{t-j},\\varepsilon_{t+h-i})$$ $$Cov(X_t,X_{t+h})=Cov(\\varepsilon_t,\\varepsilon_{t+h})+\\sum_{i=1}^{\\infty} \\frac{1}{2^{i+1}}Cov(\\varepsilon_t,\\varepsilon_{t+h-i})+\\sum_{j=1}^{\\infty} \\frac{1}{2^{j+1}}Cov(\\varepsilon_{t-j},\\varepsilon_{t+h})+\\sum_{j=1}^{\\infty}\\sum_{i=1}^{\\infty} \\frac{1}{2^{i+j+1}}Cov(\\varepsilon_{t-j},\\varepsilon_{t+h-i})$$\n\n-   Pour h=0, l'équation devient : $$Cov(X_t,X_t)=Cov(\\varepsilon_t,\\varepsilon_t)+\\sum_{j=1}^{\\infty}\\sum_{i=1}^{\\infty} \\frac{1}{2^{i+j+1}}Cov(\\varepsilon_{t-j},\\varepsilon_{t-i})$$\n\nLorsque i=j, l'équation devient donc : \\begin{equation*}\n\\begin{aligned}\nCov(X_t,X_t)&=Cov(\\varepsilon_t,\\varepsilon_t)+\\sum_{i=1}^{\\infty} \\frac{1}{2^{2i+1}}Cov(\\varepsilon_{t-i},\\varepsilon_{t-i})\\\\\n&=\\sigma^2+\\sigma^2\\sum_{i=1}^{\\infty} \\frac{1}{2^{2i+1}}\\\\\n&=\\sigma^2+\\frac{\\sigma^2}{4}\\sum_{i=1}^{\\infty} (\\frac{1}{4})^2\\\\\n&=\\sigma^2+\\frac{\\sigma^2}{4}\\times \\frac{1}{3}\\\\\n&=\\frac{13\\sigma^2}{12}\n\\end{aligned}\n\\end{equation*}\n\n-   Pour h\\>0, l'équation devient : $$Cov(X_t,X_t+h)=\\sum_{i=1}^{\\infty} \\frac{1}{2^{i+1}}Cov(\\varepsilon_t,\\varepsilon_{t+h-i})+\\sum_{j=1}^{\\infty} \\frac{1}{2^{j+1}}Cov(\\varepsilon_{t-j},\\varepsilon_{t+h})+\\sum_{j=1}^{\\infty}\\sum_{i=1}^{\\infty} \\frac{1}{2^{i+j+1}}Cov(\\varepsilon_{t-j},\\varepsilon_{t+h-i})$$\n\n\n```{=tex}\n\\begin{equation*}\n\\begin{aligned}\nCov(X_t,X_t+h)&=\\frac{\\sigma^2}{2^{h+1}}+0+\\frac{\\sigma^2}{2^{h+2}} \\times \\frac{1}{3}\\\\\n&=\\frac{\\sigma^2}{2^{h+1}}+0+\\frac{\\sigma^2}{2^{h+2}} \\times \\frac{1}{3}\\\\\n&=\\frac{\\sigma^2}{2^{h+1}}+0+\\frac{\\sigma^2}{2^{h+2}} \\times \\frac{1}{3}\\\\\n\\end{aligned}\n\\end{equation*}\n```\n\nAinsi donc : $$\\gamma(h)=\n\\begin{cases} \n\\frac{13\\sigma^2}{12} & \\text{si } h = 0 \\\\\n\\frac{7 \\sigma^2}{2^{h+2} \\times 3}& \\text{si } |h| >0 \\\\\n\\end{cases}\n$$\n\n$$\\Rightarrow \\rho(h)=\n\\begin{cases} \n\\frac{13\\sigma^2}{12} & \\text{si } h = 0 \\\\\n\\frac{7 \\sigma^2}{2^{h+2} \\times 3}& \\text{si } |h| >0 \\\\\n\\end{cases}\n$$\n\n### 4.4\n\nPosons $F=Vect(X_{t-1}, X_{t-2}, \\dots)$. $(\\varepsilon_t)_t$ est le processus d'innovation de $X_t$, car $X_t$ est de représentation minimale. Ainsi, $\\varepsilon_t=X_t-P_F(X_t) \\Leftrightarrow P_F(X_t) = X_t - \\varepsilon_t$ De fait, \\begin{equation*}\n\\begin{aligned}\nP_F(X_t)&= \\frac{1}{2}X_{t-1}+ \\varepsilon_t - \\frac{1}{4}\\varepsilon_{t-1}-\\varepsilon_t \\\\\n&= \\frac{1}{2}X_{t-1}- \\frac{1}{4}\\varepsilon_{t-1} \\\\\n\\text{Or   } &\\varepsilon_t=\\sum_{j=0}^{\\infty}(\\frac{1}{4})^j(X_t-\\frac{1}{2}X_{t-1}) \\Rightarrow \\varepsilon_t=X_t - \\sum_{j=1}^{\\infty}\\frac{1}{4^j}X_{t-j}\\\\\n\\text{Donc   } &P_F(X_t)= \\frac{1}{2}X_{t-1}- \\frac{1}{4}(X_{t-1}-\\sum_{j=1}^{\\infty}\\frac{1}{4^j}X_{t-1-j})\n\\end{aligned}\n\\end{equation*}\n\n## Exercice 5\n\n### 5.1\n\n\n```{=tex}\n\\begin{equation*}\n\\begin{aligned}\n\\gamma_\\omega(h) &= Cov(\\omega_t,\\omega_{t+h}) \\\\\n&= Cov(v_t+\\eta_t-2\\eta_{t-1}, v_{t+h}+\\eta_{t+h}-2\\eta_{t+h-1}) \\\\\n&= Cov(v_t, v_{t+h})+Cov(\\eta_{t},\\eta_{t+h})-2Cov(\\eta_{t},\\eta_{t+h-1})-2Cov(\\eta_{t-1}+2\\eta_{t+h})+4Cov(\\eta_{t-1},\\eta_{t+h-1})\n\\end{aligned}\n\\end{equation*}\n```\n\n```{=tex}\n\\begin{equation}\n\\begin{aligned}\n\\text{Donc   } &\\gamma_\\omega(h)=\n\\begin{cases} \n\\frac{10}{9} & \\text{si } h = 0 \\\\\n\\frac{-1}{3}& \\text{si } |h| =1 \\\\\n\\text{0 sinon}\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n```\n\nComme $\\gamma_\\omega(h)=0 \\ \\forall |h|>1$ alors $\\omega_t$ est un MA(1). Il peut donc s'écrire :\n\n$\\omega_t=\\varepsilon_t-\\phi_1\\varepsilon_{t-1}$\n\nLa fonction d'autocovariance d'un tel processus est : \\begin{equation}\n\\begin{aligned}\n\\Rightarrow Cov(\\omega_t,\\omega_{t+h})=\n\\begin{cases}\n\\sigma^2 + \\phi_1^2\\sigma^2 & \\text{si } h = 0 \\\\\n-\\phi_1\\sigma^2 & \\text{si } |h| = 1 \\\\\n0\n\\end{cases}\n\\end{aligned}\n\\end{equation}\n\nAinsi (1) et (2) nous permettent de determiner $\\rho_w(1)=\\frac{-\\phi_1\\sigma^2 }{\\sigma^2 + \\phi_1^2\\sigma^2 } \\Rightarrow (\\phi_1-\\frac{1}{3})(\\phi_1-3)=0$ . Donc, $\\omega_t$ admet une représentation MA inversible pour $\\phi_1=\\frac{1}{3}<1$.\n\n### 5.2\n\n\n```{=tex}\n\\begin{equation*}\n\\begin{aligned}\nX_t&=Y_t+\\eta_t\\\\\n&=Y_t + \\eta_t - 2BY_t + 2BY_t\\\\\n&= (I-2B)Y_t+\\eta_t+2BY_t\\\\\n&= v_t+ \\eta_t + 2BY_t\\\\\n&= v_t + \\eta_t + 2 (X_{t-1}-\\eta_{t-1})\\\\\n&=\\omega_t+2X_{t-1}\\\\\n\\text{Donc  }&\\omega_t=X_t-2X_{t-1} \\sim ARMA(1,1)\n\\end{aligned}\n\\end{equation*}\n```\n\n$(I-2B)X_t=(I-\\frac{1}{3}B)\\varepsilon_t$ est minimale car (I-2B) et $(I-\\frac{1}{3}B)$ n'ont pas de racine commune et $\\varepsilon_t$ est son rocessus d'innovation de variance $\\sigma^2=1$.\n\n### 5.3\n\nOn sait que $(I-2B)X_t=(I-\\frac{1}{3}B)\\varepsilon_t$, cela implique : \\begin{equation*}\n\\begin{aligned}\n\\varepsilon_t&=(\\sum_{i=0}^{\\infty} \\frac{B}{3}^i)(I-2B)X_t\\\\\n&=(\\sum_{i=0}^{\\infty} \\frac{B}{3}^i)(X_t-2X_{t-1})\\\\\n&=(\\sum_{i=0}^{\\infty} \\frac{1}{3}^i)X_{t-1} -2X_{t-1-I}) \\sim AR(\\infty)\n\\end{aligned}\n\\end{equation*}\n\n## Exercice 6\n\n### 6.1\n\n$\\hat{X}_{t+1}=\\alpha X_t + (1-\\alpha)X_t$ Montrons par récurrence que $\\hat{X}_{t+1}=\\sum_{j=0}^{t-1}\\alpha(1-\\alpha)^jX_{t-j}$\n\n\\underline{Initialisation} Pour t=1, nous devons vérifier que:\n\n$$ \\hat{X}_2 = \\alpha X_1 + (1-\\alpha) \\hat{X}_1 $$\n\nÀ $t=1$, nous n'avons pas de terme antérieur, donc $\\hat{X}_1 = X_1$, et notre formule de prédiction devient:\n\n$$ \\hat{X}_2 = \\alpha X_1 + (1-\\alpha) X_1 = X_1 $$\n\nCela montre que pour $t=1$, la formule de prédiction se réduit simplement à $\\hat{X}_2 = X_1$, qui est équivalent à une somme qui n'a qu'un seul terme :\n\n$$ \\hat{X}_2 = \\sum_{j=0}^{0}\\alpha(1-\\alpha)^jX_{1-j} = \\alpha(1-\\alpha)^0X_1 = X_1 $$\n\nC'est notre équation de base pour $t=1$, et c'est vrai par définition.\n\n\\underline{Hérédité} Supposons que la formule soit vraie pour un certain $t$, c'est-à-dire que:\n\n$\\hat{X}_{t+1} = \\sum_{j=0}^{t-1}\\alpha(1-\\alpha)^jX_{t-j}$\n\nIl faut maintenant prouver que la formule est vraie pour $t+1$, c'est-à-dire que:\n\n$\\hat{X}_{t+2} = \\sum_{j=0}^{t}\\alpha(1-\\alpha)^jX_{t-j+1}$\n\nEn remplaçant $\\hat{X}_{t+1}$ par l'expression de l'hypothèse de récurrence, on obtient: \\begin{equation*}\n\\begin{aligned}\n\\hat{X}_{t+2} &= \\alpha X_{t+1} + (1-\\alpha) \\sum_{j=0}^{t-1}\\alpha(1-\\alpha)^jX_{t-j}\\\\\n&= \\alpha X_{t+1} + \\sum_{j=0}^{t-1}\\alpha(1-\\alpha)^{j+1}X_{t-j}\\\\\n&= \\alpha X_{t+1}  + \\sum_{j=1}^{t}\\alpha(1-\\alpha)^{j}X_{t-j+1}\\\\\n&= \\sum_{j=0}^{t}\\alpha(1-\\alpha)^j X_{t+1-j}\n\\end{aligned}\n\\end{equation*}\n\n\\underline{Conclusion}\n\nNous avons prouvé par récurrence que $\\hat{X}_{t+1}=\\sum_{j=0}^{\\infty}\\alpha(1-\\alpha)^jX_{t-j}$.\n\n### 6.2\n\n-   Lorsque $\\alpha \\approx 0$, nous avons $\\hat{X}_{t+1}$ qui dépend de $\\hat{X}_t$, or $\\hat{X}_{t}=\\sum_{j=0}^{t-2}\\alpha(1-\\alpha)^jX_{t-j}=\\dots=\\alpha X_{t-1}$. Ainsi donc, $\\alpha \\approx 0$ $\\hat{X}_{t+1}$ dépend d'un passé lointain.\n\n-   Lorsque $\\alpha \\approx 1$, nous avons $\\hat{X}_{t+1}$ qui dépend de $X_t$, donc d'un passé plus proche.\n\n### 6.3\n\n\n```{=tex}\n\\begin{equation*}\n\\begin{aligned}\n\\hat{X}_{t+1}&=\\alpha X_t+(1-\\alpha)\\hat{X}_t\n&=\\alpha X_t + \\hat{X}_t + \\alpha \\hat{X}_t\n&= \\hat{X}_t + \\alpha(X_t-\\hat{X}_t)\n\\end{aligned}\n\\end{equation*}\n```\n\n### 6.4\n\nPosons$f(a) = \\sum_{j=0}^{t-1} (1 - \\alpha)^j (X_{t-j} - a)^2$ Ainsi donc :\n\n\n```{=tex}\n\\begin{equation*}\n\\begin{aligned}\nf'(a) &= -2 \\sum_{j=0}^{t-1} (1 - \\alpha)^j (X_{t-j} - a) = 0 \\\\\n&\\Leftrightarrow \\sum_{j=0}^{t-1} (1 - \\alpha)^j X_{t-j} = a \\sum_{j=0}^{t-1} (1 - \\alpha)^j \\\\\n&\\Leftrightarrow a = \\frac{\\sum_{j=0}^{t-1} (1 - \\alpha)^j X_{t-j}}{\\sum_{j=0}^{t-1} (1 - \\alpha)^j}\n\\end{aligned}\n\\end{equation*}\n```\n\nÀ mesure que $t$ devient grand, la série géométrique converge et l'expression ci-dessus pour $a$ se simplifie en $\\hat{X}_{t+1}$.\n\nDe plus, $f''(a)=2 \\sum_{j=0}^{t-1} (1 - \\alpha)^j >0$ donc $\\hat{X}_{t+1} \\approx a$.\n\n### 6.5\n\nPour estimer $\\alpha$, on peut utiliser la méthode des moindres carrés.\n\n### 6.6\n\nNous pouvons écrire la fonction de coût $H_t(l, b)$ comme suit : $$ H_t(l, b) = \\sum_{j=0}^{t-1} (1 - \\alpha)^j (X_{t-j} - (l - bj))^2 $$\n\nPour minimiser cette fonction par rapport à $l$ et $b$, nous dérivons $H_t(l, b)$ par rapport à $l$ et $b$ et égalons les dérivées à zéro pour trouver les points critiques.\n\nEn différentiant $H_t(l, b)$ par rapport à $l$ et $b$ et en égalant à zéro, nous obtenons un système de deux équations (les conditions du premier ordre pour un minimum) :\n\n$$ \\frac{\\partial H_t(l, b)}{\\partial l} = -2\\sum_{j=0}^{t-1} (1 - \\alpha)^j (X_{t-j} - (l - bj)) = 0 $$ $$ \\frac{\\partial H_t(l, b)}{\\partial b} = -2\\sum_{j=0}^{t-1} j(1 - \\alpha)^j (X_{t-j} - (l - bj)) = 0 $$\n\nLe minimum se réalisant où les dérivées s'annulent (fonction convexe) et en remarquant que $$\n\\sum_{i=0}^{\\infty}(1-\\alpha)^i = \\frac{1}{\\alpha},\n$$ $$\n\\sum_{i=0}^{\\infty} i(1 - \\alpha)^i = \\frac{1-\\alpha}{\\alpha^2},\n$$ $$\n\\sum_{i=0}^{\\infty} i^2(1 - \\alpha)^i = \\frac{(1-\\alpha)(2-\\alpha)}{\\alpha^3},\n$$ on a: $$\n\\left\\{\n\\begin{array}{l}\n\\sum_{i=0}^{\\infty}(1 - \\alpha)^i y_{t-i} - \\frac{l}{\\alpha} + b\\frac{(1-\\alpha)}{\\alpha^2} =0 \\\\\n\\sum_{i=0}^{\\infty} i(1 - \\alpha)^i y_{t-i} - l\\frac{1-\\alpha}{\\alpha^2} + b \\frac{(1-\\alpha)(2-\\alpha)}{\\alpha^3} =0\n\\end{array}\n\\right.\n$$ Soit $$\n\\left\\{\n\\begin{array}{l}\n\\alpha \\sum_{i=0}^{\\infty}(1 - \\alpha)^i y_{t-i} - l + b\\frac{(1-\\alpha)}{\\alpha} =0 \\\\\n\\alpha^2 \\sum_{i=0}^{\\infty} i(1 - \\alpha)^i y_{t-i} - l(1 - \\alpha) + b \\frac{(1-\\alpha)(2-\\alpha)}{\\alpha} =0\n\\end{array}\n\\right.\n$$ On note: $L_1(t) = \\alpha \\sum_{i=0}^{\\infty}(1 - \\alpha)^i y_{t-i}$ et $L_2(t) = \\alpha \\sum_{i=0}^{\\infty}(1 - \\alpha)^i L_1(t - i)$ Remarquons que $L_2(t) - \\alpha L_1(t) = \\alpha^2 \\sum_{i=1}^{\\infty} i(1 - \\alpha)^i y_{t-i}$ car: $$\nL_2(t) - \\alpha L_1(t) = \\alpha \\sum_{i=1}^{\\infty}(1 - \\alpha)^i L_1(t - i)\n$$ $$\n= \\alpha^2 \\sum_{i=1}^{\\infty}\\sum_{j=0}^{\\infty}(1 - \\alpha)^{i+j} y_{t-(i+j)}\n$$ $$\n= \\alpha^2 \\sum_{i=1}^{\\infty}\\sum_{k=i}^{\\infty}(1 - \\alpha)^k y_{t-k} = \\alpha^2 \\sum_{i=1}^{\\infty} i(1 - \\alpha)^i y_{t-i}\n$$ Alors: $$\n\\left\\{\n\\begin{array}{l}\nL_1(t) - l + b\\frac{(1-\\alpha)}{\\alpha} =0 \\\\\nL_2(t) - \\alpha L_1(t) - l(1 - \\alpha) + b \\frac{(1-\\alpha)(2-\\alpha)}{\\alpha} =0\n\\end{array}\n\\right.\n$$ Et $$\n\\left\\{\n\\begin{array}{l}\nL_1(t)= l - b\\frac{(1-\\alpha)}{\\alpha} \\\\\nL_2(t)=l - b\\frac{2(1-\\alpha)}{\\alpha}\n\\end{array}\n\\right.\n$$ Ainsi $$\n\\left\\{\n\\begin{array}{l}\nl = 2L_1(t) - L_2(t) \\\\\nb= \\frac{\\alpha}{1-\\alpha} (L_1(t) - L_2(t))\n\\end{array}\n\\right.\n$$ Nous pouvons ensuite en déduire les formules de récurrences. $$\nL_1(t) = \\alpha y_t + (1 - \\alpha)L_1(t - 1)\n$$ $$\n\\left\\{\n\\begin{array}{l}\nL_2(t)=\\alpha L_1(t) + (1 - \\alpha)L_2(t - 1) \\\\\n=\\alpha^2 y_t + \\alpha(1 - \\alpha)L_1(t - 1) + (1 - \\alpha)L_2(t - 1)\n\\end{array}\n\\right.\n$$ Ainsi $$\nl = (1 - (1 - \\alpha)^2)y_t + (1 - \\alpha)(2 - \\alpha)L_1(t - 1) - (1 - \\alpha)L_2(t - 1)\n$$ En incorporant $L_1(t) = l_{t - 1} - \\frac{1-\\alpha}{\\alpha} b_t$ et $L_2(t) = l_{t} - b_t \\frac{2(1-\\alpha)}{\\alpha}$, on en déduit après simplification: $$\nl_t = (1 - (1 - \\alpha)^2)y_t + (1 - \\alpha)^2(l_{t-1} + b_{t-1})\n$$ De même, $$\nb_t = \\alpha^2 y_t + \\alpha(1 - \\alpha)L_1(t - 1) - \\alpha L_2(t - 1)\n$$ $$\nb_t = \\alpha^2 y_t - \\alpha^2 l_{t-1} + (1 - \\alpha)^2 b_{t-1}\n$$\n\n$$\n\\begin{cases}\nl_t=l_{t-1}+b_{t-1}+(1-(1-\\alpha)^2)(X_t-\\hat{X}_t)\\\\\nb_t=b_{t-1}+\\alpha^2 (X_t-\\hat{X}_t)\n\\end{cases}\n$$ avec comme initialisation $l_0=X_1$ et $b_0=X_2-X_1$.\n",
    "supporting": [],
    "filters": [
      "rmarkdown/pagebreak.lua"
    ],
    "includes": {
      "include-in-header": [
        "<link href=\"../../site_libs/pagedtable-1.1/css/pagedtable.css\" rel=\"stylesheet\" />\n<script src=\"../../site_libs/pagedtable-1.1/js/pagedtable.js\"></script>\n"
      ]
    },
    "engineDependencies": {},
    "preserve": {},
    "postProcess": true
  }
}