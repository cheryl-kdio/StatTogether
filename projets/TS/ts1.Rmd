---
title: "TD - Séries temporelles 1"
sidebar: auto
author:
  - Cheryl Kouadio
---

Dans cette section, il s'agit d'une proposition de réponse au TD de Séries temporelles 1 dispensé par Mr José GÓMEZ GARCÍA à l'ENSAI en 2023-2024 au deuxième année.

Vous trouverez l'énoncé du td via le lien ci-joint .

## Exercice 1

Soit $\epsilon_t$ un bruit blanc. Pour chacun des processus suivants, dire s'il s'agit d'un processus ARMA stationnaire. Si oui, déterminer les ordres p et q et préciser si le processus admet une représentation AR($\infty$) et/ou MA($\infty$) et si la représentation ARMA est minimale.

### 1.

Nous savons que : \begin{equation*}
\begin{aligned}
X_t &= \frac{5}{6} X_{t-1} - \frac{1}{6} X_{t-2} + \epsilon_t + \epsilon_{t-1} \\
&\Leftrightarrow X_t - \frac{5}{6} X_{t-1} + \frac{1}{6} X_{t-2} = \epsilon_t + \epsilon_{t-1} \\
&\Leftrightarrow (I - \frac{5}{6}B + \frac{1}{6}B^2) X_t = (1 + B) \epsilon_t \\
&\Leftrightarrow \Phi(B)X_t = \Psi(B) \epsilon_t
\end{aligned}
\end{equation*}

Déterminons les racines de $\Phi(B)$ : \begin{equation*}
\begin{aligned}
\Phi(B) &= I-\frac{5}{6}B + \frac{1}{6}B^2  \\
&= \frac{1}{6}(B^2 - 5B + 6I) \\
&= \frac{1}{6} (B - 2I)(B - 3I)  \\
&= (I - \frac{1}{2}B)(I - \frac{1}{3}B)
\end{aligned}
\end{equation*}

Les racines de $\Phi$ sont de modules 2 et 3, de modules supérieurs à 1 donc, $X_t$ est un processus ARMA(2,1) stationnaire. De plus, il admet une représentation MA($\infty$).

-   $\Psi(B) = (I+B)$ admet comme racine (évidente) 1. Donc $X_t$ n'admet aucune représentation AR($\infty$).

Comme $\Phi(B)$ et $\Psi(B)$ n'admettent aucune racine commune, on ne peut réduire la représentation. le processus ARMA est donc minimale.

D'accord, je vais tenter de transcrire les équations et commentaires de votre image dans un format similaire à celui que vous avez fourni précédemment.

### 2.2

Nous pouvons voir que $X_t$ suit une $AR(\infty)$: \begin{equation*}
\begin{aligned}
\sum_{k=0}^{\infty} \frac{1}{2^k} X_{t-k} &= \epsilon_t \\  
&\Leftrightarrow (\sum_{k=0}^{\infty} (\frac{B}{2})^k ) X_t = \epsilon_t \\
&\Leftrightarrow \frac{1}{I-\frac{B}{2}}X_t = \epsilon_t\\
&\Leftrightarrow X_t=(I-\frac{B}{2})\epsilon_t
\end{aligned}
\end{equation*} Ainsi, $X_t$ est un processus MA(1). De plus, la racine de $(I-\frac{B}{2})$ est 2 $\geq 1$, donc ce processus ARMA(0,1) est stationnaire et admet uniquement une représentation AR($\infty$).

### 2.3

\begin{equation*}
\begin{aligned}
X_t &= \frac{5}{4} X_{t-1} - \frac{1}{4} X_{t-2} + \epsilon_t + 2 \epsilon_{t-1} \\
&\Leftrightarrow X_t - \frac{5}{4} X_{t-1} + \frac{1}{4} X_{t-2} &= \epsilon_t + 2 \epsilon_{t-1} \\
&\Leftrightarrow (1 - \frac{5}{4}B + \frac{1}{4}B^2)X_t &= (1 + 2B) \epsilon_t \\
&\Leftrightarrow (1 - B)(1-\frac{1}{4}B)X_t &= (1 + 2B) \epsilon_t \\
&\Leftrightarrow \Phi(B)X_t = \Psi(B) \epsilon_t
\end{aligned}
\end{equation*} $\Phi(B)$ admet pour racine de 1 et 4. Comme l'une des racines est de module égal à 1, $X_t$ n'est pas stationnaire.

### 2.4

\begin{equation*}
\begin{aligned}
X_t &= 2X_{t-1} - X_{t-2} + \epsilon_t + \frac{1}{4}\epsilon_{t-1} \\
&\Leftrightarrow X_t - 2X_{t-1} + X_{t-2} &= \epsilon_t + \frac{1}{4}\epsilon_{t-1} \\
&\Leftrightarrow (1 - 2B + B^2)X_t &= (1 + \frac{B}{4}) \epsilon_t \\
&\Leftrightarrow \Phi(B)X_t = \Psi(B) \epsilon_t
\end{aligned}
\end{equation*} $\Phi(B)$ admet comme racine (évidente) 1 donc $X_t$ n'est pas stationnaire.

## Exercice 2

### 2.1

$(X_t) \sim ARIMA(2,1,1)$ $\Rightarrow Y_t \sim ARMA(1,1)$ avec $Y_t = (I-B)X_t$.

$Y_t \sim ARMA(1,1) \Leftrightarrow Y_t = c + \phi_1 Y_{t-1} + \varepsilon_t + \theta_1 \varepsilon_{t-1}$

Or :

```{=tex}
\begin{equation*}
\begin{aligned}
Y_t = X_t - X_{t-1} &= c + \phi_1 Y_{t-1} + \varepsilon_t + \theta_1 \varepsilon_{t-1}\\
&\Leftrightarrow X_t - X_{t-1} = c + \phi_1 (X_{t-1} - X_{t-2}) + \varepsilon_t + \theta_1 \varepsilon_{t-1} \\
&\Leftrightarrow X_t - X_{t-1} = c + \phi_1 X_{t-1} - \phi_1 X_{t-2} + \varepsilon_t + \theta_1 \varepsilon_{t-1} \\
&\Leftrightarrow X_t  = c + (1-\phi_1) X_{t-1} - \phi_1 X_{t-2} + \varepsilon_t + \theta_1 \varepsilon_{t-1} \\
\end{aligned}
\end{equation*}
```
### 2.2

$(X_t) \sim ARI(2,1)$ $\Rightarrow Y_t \sim AR(1)$ avec $Y_t = (I-B)X_t$.

$Y_t \sim AR(1) \Leftrightarrow Y_t = c + \phi_1 Y_{t-1} + \phi_2 Y_{t-2}$

Or :

```{=tex}
\begin{equation*}
\begin{aligned}
Y_t = X_t - X_{t-1} &= c +\phi_1 Y_{t-1} + \phi_2 Y_{t-2}\\
&\Leftrightarrow X_t - X_{t-1} = c + \phi_1 (X_{t-1} - X_{t-2}) + \phi_2 (X_{t-2} - X_{t-3}) \\
&\Leftrightarrow X_t = c + (\phi_1+1) (X_{t-1} + (\phi_2-\phi_1)X_{t-2} - \phi_2 X_{t-3}) \\
\end{aligned}
\end{equation*}
```
### 2.3

$(X_t) \sim I(2)$ $\Rightarrow Y_t = (I-B)X_t$ est stationnaire.

Selon le théorème de Wold, on peut écrire donc:

$$
Y_t = c + \sum_{j=0}^{\infty} \psi_j \varepsilon_{t-j} + V_t
$$

Où $V_t$ est connu tel que $cov(\varepsilon_t,V_s) = 0$ pour $s \neq t$.

Ainsi, \begin{equation*}
\begin{aligned}
Y_t = X_t - X_{t-1} &= c + \sum_{j=0}^{\infty} \psi_j \varepsilon_{t-j} + V_t\\
&\Leftrightarrow X_t  = c + X_{t-1}+ \sum_{j=0}^{\infty} \psi_j \varepsilon_{t-j} + V_t\\
\end{aligned}
\end{equation*}

### 2.4

$(X_t)$ est $SARIMA(0,2,1)*(1,1,0)_7$

Posons:

$$ Y_t = (1-B^7)^1(1- B)^0X_t =(1-B^7)X_t=X_t - X_{t-7} $$

Nous savons que cette équation est une ARMA de forme :

$$
\Phi_7^1(B^7)\Phi^2(B)Y_t = \Theta_7^0(B^7)\Theta^1(B)\varepsilon_t
$$

Où $\Phi$ et $\Theta$ sont les polynômes AR et MA, respectivement.

Nous avons donc : $$
(1 - \phi B^7)(1 -\alpha_1 B - \alpha_2 B^2)Y_t = 1*(1 - \theta_1 B)\varepsilon_t
$$ $$
(1 - \phi B^7)(1 -\alpha_1 B - \alpha_2 B^2)(X_t - X_{t-7} ) = 1*(1 - \theta_1 B)\varepsilon_t
$$

### 2.5

$(X_t)$ est $SARMA(2,0)*(1,2)_{12}$

Posons:

$$ Y_t = (1-B^{12})^0(1- B)^0X_t =X_t $$

Nous savons que cette équation est une ARMA de forme :

$$
\Phi_{12}^1(B^{12})\Phi^2(B)Y_t = \Theta_{12}^2(B^7)\Theta^0(B)\varepsilon_t
$$

Où $\Phi$ et $\Theta$ sont les polynômes AR et MA, respectivement.

Nous avons donc : $$
(1 - \phi B^{12})(1 -\alpha_1 B - \alpha_2 B^2)Y_t = (1 - \theta_1 B^{12}-\theta_2 B^{24})\varepsilon_t
$$ $$
(1 - \phi B^{12})(1 -\alpha_1 B - \alpha_2 B^2)X_t = (1 - \theta_1 B^{12}-\theta_2 B^{24})\varepsilon_t
$$

## Exercice 3

### 3.1

Soit $M = (I - B)(I - B^2)$ Ainsi, $$M(a + bt + ct^2) = (I - B)(I - B^2)(a + bt + ct^2) = (I - B-B^2+B^3)(a + bt + ct^2) $$

Appliquons M à chaque composantes de $a + bt + ct^2$: 1. M appliqué à $a$:

$$M(a)=a-a-a+a=0$$

2.  M appliqué à $bt$

$$ M(bt) = bt - b(t-1) - b(t-2) + b(t-3)$$ $$ M(bt) = bt - bt + b - bt + 2b + bt - 3b $$ $$ M(bt) = 0$$

3.  M appliqué à $ct^2$ $$ M(ct^2) = ct^2 - c(t-1)^2 - c(t-2)^2 + c(t-3)^2$$ $$ M(bt) =  ct^2 - c(t^2 - 2t + 1) - c(t^2 - 4t + 4) + c(t^2 - 6t + 9)  $$ $$ M(bt) = -c-4c+9c  $$ $$ M(bt) = 4c  $$

En combinant les résultats de M appliqué à $a, bt$ et $ct^2$, nous trouvons :

$$ M(a + bt + ct^2) = 0 + 0 + 4c = 4c $$

Cela confirme que $M$ transforme le polynôme de degré 2 en une constante $4c$.

### 3.2

$$ M(\cos(\pi t)) = \cos(\pi t) - \cos(\pi(t-2)) - \cos(\pi(t-1)) +  \cos(\pi(t-3))  $$ $$ M(\cos(\pi t)) = \cos(\pi t) - \cos(\pi-2\pi) - \cos(\pi t- \pi)) +  \cos(\pi t-3\pi)  $$ $$ M(\cos(\pi t)) = \cos(\pi t) - \cos(\pi t) +  \cos(\pi t)) -  \cos(\pi )  $$ $$ M(\cos(\pi t)) = 0 $$

DOnc M absorbe les saisonnalités $cos(\pi t)$

### 3.3

$$ E(X_t)=E(a+bt+ct^2+cos(\pi t) + \varepsilon_t)$$ $$ E(X_t)=a+bt+ct^2+cos(\pi t) + E(\varepsilon_t))$$ $$ E(X_t)=a+bt+ct^2+cos(\pi t) $$ car $\varepsilon_t$ est un bruit blanc.

Ici, l'espérance dépend du temps donc $X_t$ n'est pas stationnaire

### 3.4

Soit $Y_t=M(X_t)$ On a donc : $$Y_t=M(a+bt+ct^2)+M(cos(\pi t))+M(\varepsilon_t)$$ $$Y_t=4c+0+\varepsilon_t-\varepsilon_{t-1}-\varepsilon_{t-2}+\varepsilon_{t-3}$$ $$Y_t=4c-\sum_{i=0}^{3} \theta_i \varepsilon_{t-i})$$, avec $\theta_0=-1$, $\theta_1=1$,$\theta_2=1$ et $\theta_3=-1$

Donc $Y_t$ est bien un processus MA(3).

### i

-   $$E(Y_t)=4c$$
-   Comme $Y_t$ est un processus MA(3), sa fonction d'autocovariance s'écrit : $$Cov(Y_t,Y_{t+h}=\sigma^2(\sum_{j=0}^{3-h} \theta_j \theta_{j+h})$$

Ainsi, $$\gamma(h)=
\begin{cases} 
\sigma^2 & \text{si } h = 0 \\
\sigma^2(-1+1-1) & \text{si } |h| = 1 \\
\sigma^2(-1-1) & \text{si } |h| = 2 \\
\sigma^2(1) & \text{si } |h| = 3 \\
0& \text{si } |h| >3 
\end{cases}
$$

$$\gamma(h)=
\begin{cases} 
\sigma^2 & \text{si } h = 0 \\
-\sigma^2 & \text{si } |h| = 1 \\
-2\sigma^2 & \text{si } |h| = 2 \\
\sigma^2 & \text{si } |h| = 3 \\
0& \text{si } |h| >3 
\end{cases}
$$

### ii

$Y_t=(I-B)(I-B^2)X_t$ avec $Y_t \sim MA(3)$

Ainsi, on identifie facilement un processus SARIMA : $$Y_t=(I-B^{s=2})^{D=1}(I-B)^{d=1}X_t \sim SARIMA(0,1,3)*(0,1,0)_2$$

## Exercice 4

### 4.1

Nous avons que : \begin{equation*}
\begin{aligned}
X_t &= \frac{1}{2} X_{t-1} + \varepsilon_t - \frac{1}{4}\varepsilon_{t-1} \\
&\Leftrightarrow X_t - \frac{1}{2} X_{t-1} = \varepsilon_t - \frac{1}{4}\varepsilon_{t-1} \\
&\Leftrightarrow (I - \frac{1}{2}B ) X_t = (1 - \frac{1}{4} B) \epsilon_t \\
&\Leftrightarrow \Phi(B)X_t = \Psi(B) \epsilon_t
\end{aligned}
\end{equation*}

La racine de $\Phi$ est 2 et la racine de $\Psi$ est 4. Les deux polynômes n'admettent aucune racine commune, donc le processus est minimale.

### 4.2

Déterminons d'abord l'inverse de $\Phi(B)=(I - \frac{1}{2}B )$. $\Phi(B)$ est inversible car c'est un polynôme de racine de module 2 \> 1. Ainsi, $\Phi(B)^{-1}=\sum_{i=0}^{\infty} (\frac{B}{2})^i$. De fait, nous avons : $$X_t=\Phi(B)^{-1}(\Psi(B)\varepsilon_t)$$ $$X_t=\sum_{i=0}^{\infty} (\frac{B}{2})^i( \varepsilon_t - \frac{1}{4}\varepsilon_{t-1})$$ $$X_t=\sum_{i=0}^{\infty} (\frac{1}{2})^i( \varepsilon_{t-i} - \frac{1}{4}\varepsilon_{t-1-i})$$ $$X_t=\sum_{i=0}^{\infty} \frac{1}{2^i} \varepsilon_{t-i} - \sum_{i=0}^{\infty} \frac{1}{2^{i+2}}\varepsilon_{t-1-i})$$ À l'aide d'un changement de variable j=j-1, on a : $$X_t=\sum_{i=0}^{\infty} \frac{1}{2^i} \varepsilon_{t-i} - \sum_{i=1}^{\infty} \frac{1}{2^{i+1}}\varepsilon_{t-i})$$ $$X_t=\varepsilon_t + \sum_{i=1}^{\infty} \frac{1}{2^i} \varepsilon_{t-i} - \sum_{i=1}^{\infty} \frac{1}{2^{i+1}}\varepsilon_{t-i})$$ $$X_t=\varepsilon_t + \sum_{i=1}^{\infty} \frac{1}{2^{i+1}}\varepsilon_{t-i})$$

### 4.3

$$Cov(X_t,X_{t+h})=Cov(\varepsilon_t + \sum_{j=1}^{\infty} \frac{1}{2^{j+1}}\varepsilon_{t-j}\ , \ \varepsilon_{t+h} + \sum_{i=1}^{\infty} \frac{1}{2^{i+1}}\varepsilon_{t-i+h})$$ $$Cov(X_t,X_{t+h})=Cov(\varepsilon_t,\varepsilon_{t+h})+\sum_{i=1}^{\infty} \frac{1}{2^{i+1}}Cov(\varepsilon_t,\varepsilon_{t+h-i})+\sum_{j=1}^{\infty} \frac{1}{2^{j+1}}Cov(\varepsilon_{t-j},\varepsilon_{t+h})+\sum_{j=1}^{\infty}\sum_{i=1}^{\infty} \frac{1}{2^{j+1}}\frac{1}{2^{i+1}}Cov(\varepsilon_{t-j},\varepsilon_{t+h-i})$$ $$Cov(X_t,X_{t+h})=Cov(\varepsilon_t,\varepsilon_{t+h})+\sum_{i=1}^{\infty} \frac{1}{2^{i+1}}Cov(\varepsilon_t,\varepsilon_{t+h-i})+\sum_{j=1}^{\infty} \frac{1}{2^{j+1}}Cov(\varepsilon_{t-j},\varepsilon_{t+h})+\sum_{j=1}^{\infty}\sum_{i=1}^{\infty} \frac{1}{2^{i+j+1}}Cov(\varepsilon_{t-j},\varepsilon_{t+h-i})$$

-   Pour h=0, l'équation devient : $$Cov(X_t,X_t)=Cov(\varepsilon_t,\varepsilon_t)+\sum_{j=1}^{\infty}\sum_{i=1}^{\infty} \frac{1}{2^{i+j+1}}Cov(\varepsilon_{t-j},\varepsilon_{t-i})$$

Lorsque i=j, l'équation devient donc : \begin{equation*}
\begin{aligned}
Cov(X_t,X_t)&=Cov(\varepsilon_t,\varepsilon_t)+\sum_{i=1}^{\infty} \frac{1}{2^{2i+1}}Cov(\varepsilon_{t-i},\varepsilon_{t-i})\\
&=\sigma^2+\sigma^2\sum_{i=1}^{\infty} \frac{1}{2^{2i+1}}\\
&=\sigma^2+\frac{\sigma^2}{4}\sum_{i=1}^{\infty} (\frac{1}{4})^2\\
&=\sigma^2+\frac{\sigma^2}{4}\times \frac{1}{3}\\
&=\frac{13\sigma^2}{12}
\end{aligned}
\end{equation*}

-   Pour h\>0, l'équation devient : $$Cov(X_t,X_t+h)=\sum_{i=1}^{\infty} \frac{1}{2^{i+1}}Cov(\varepsilon_t,\varepsilon_{t+h-i})+\sum_{j=1}^{\infty} \frac{1}{2^{j+1}}Cov(\varepsilon_{t-j},\varepsilon_{t+h})+\sum_{j=1}^{\infty}\sum_{i=1}^{\infty} \frac{1}{2^{i+j+1}}Cov(\varepsilon_{t-j},\varepsilon_{t+h-i})$$

```{=tex}
\begin{equation*}
\begin{aligned}
Cov(X_t,X_t+h)&=\frac{\sigma^2}{2^{h+1}}+0+\frac{\sigma^2}{2^{h+2}} \times \frac{1}{3}\\
&=\frac{\sigma^2}{2^{h+1}}+0+\frac{\sigma^2}{2^{h+2}} \times \frac{1}{3}\\
&=\frac{\sigma^2}{2^{h+1}}+0+\frac{\sigma^2}{2^{h+2}} \times \frac{1}{3}\\
\end{aligned}
\end{equation*}
```
Ainsi donc : $$\gamma(h)=
\begin{cases} 
\frac{13\sigma^2}{12} & \text{si } h = 0 \\
\frac{7 \sigma^2}{2^{h+2} \times 3}& \text{si } |h| >0 \\
\end{cases}
$$

$$\Rightarrow \rho(h)=
\begin{cases} 
\frac{13\sigma^2}{12} & \text{si } h = 0 \\
\frac{7 \sigma^2}{2^{h+2} \times 3}& \text{si } |h| >0 \\
\end{cases}
$$

### 4.4

Posons $F=Vect(X_{t-1}, X_{t-2}, \dots)$. $(\varepsilon_t)_t$ est le processus d'innovation de $X_t$, car $X_t$ est de représentation minimale. Ainsi, $\varepsilon_t=X_t-P_F(X_t) \Leftrightarrow P_F(X_t) = X_t - \varepsilon_t$ De fait, \begin{equation*}
\begin{aligned}
P_F(X_t)&= \frac{1}{2}X_{t-1}+ \varepsilon_t - \frac{1}{4}\varepsilon_{t-1}-\varepsilon_t \\
&= \frac{1}{2}X_{t-1}- \frac{1}{4}\varepsilon_{t-1} \\
\text{Or   } &\varepsilon_t=\sum_{j=0}^{\infty}(\frac{1}{4})^j(X_t-\frac{1}{2}X_{t-1}) \Rightarrow \varepsilon_t=X_t - \sum_{j=1}^{\infty}\frac{1}{4^j}X_{t-j}\\
\text{Donc   } &P_F(X_t)= \frac{1}{2}X_{t-1}- \frac{1}{4}(X_{t-1}-\sum_{j=1}^{\infty}\frac{1}{4^j}X_{t-1-j})
\end{aligned}
\end{equation*}

## Exercice 5

### 5.1

```{=tex}
\begin{equation*}
\begin{aligned}
\gamma_\omega(h) &= Cov(\omega_t,\omega_{t+h}) \\
&= Cov(v_t+\eta_t-2\eta_{t-1}, v_{t+h}+\eta_{t+h}-2\eta_{t+h-1}) \\
&= Cov(v_t, v_{t+h})+Cov(\eta_{t},\eta_{t+h})-2Cov(\eta_{t},\eta_{t+h-1})-2Cov(\eta_{t-1}+2\eta_{t+h})+4Cov(\eta_{t-1},\eta_{t+h-1})
\end{aligned}
\end{equation*}
```
```{=tex}
\begin{equation}
\begin{aligned}
\text{Donc   } &\gamma_\omega(h)=
\begin{cases} 
\frac{10}{9} & \text{si } h = 0 \\
\frac{-1}{3}& \text{si } |h| =1 \\
\text{0 sinon}
\end{cases}
\end{aligned}
\end{equation}
```
Comme $\gamma_\omega(h)=0 \ \forall |h|>1$ alors $\omega_t$ est un MA(1). Il peut donc s'écrire :

$\omega_t=\varepsilon_t-\phi_1\varepsilon_{t-1}$

La fonction d'autocovariance d'un tel processus est : \begin{equation}
\begin{aligned}
\Rightarrow Cov(\omega_t,\omega_{t+h})=
\begin{cases}
\sigma^2 + \phi_1^2\sigma^2 & \text{si } h = 0 \\
-\phi_1\sigma^2 & \text{si } |h| = 1 \\
0
\end{cases}
\end{aligned}
\end{equation}

Ainsi (1) et (2) nous permettent de determiner $\rho_w(1)=\frac{-\phi_1\sigma^2 }{\sigma^2 + \phi_1^2\sigma^2 } \Rightarrow (\phi_1-\frac{1}{3})(\phi_1-3)=0$ . Donc, $\omega_t$ admet une représentation MA inversible pour $\phi_1=\frac{1}{3}<1$.

### 5.2

```{=tex}
\begin{equation*}
\begin{aligned}
X_t&=Y_t+\eta_t\\
&=Y_t + \eta_t - 2BY_t + 2BY_t\\
&= (I-2B)Y_t+\eta_t+2BY_t\\
&= v_t+ \eta_t + 2BY_t\\
&= v_t + \eta_t + 2 (X_{t-1}-\eta_{t-1})\\
&=\omega_t+2X_{t-1}\\
\text{Donc  }&\omega_t=X_t-2X_{t-1} \sim ARMA(1,1)
\end{aligned}
\end{equation*}
```
$(I-2B)X_t=(I-\frac{1}{3}B)\varepsilon_t$ est minimale car (I-2B) et $(I-\frac{1}{3}B)$ n'ont pas de racine commune et $\varepsilon_t$ est son rocessus d'innovation de variance $\sigma^2=1$.

### 5.3

On sait que $(I-2B)X_t=(I-\frac{1}{3}B)\varepsilon_t$, cela implique : \begin{equation*}
\begin{aligned}
\varepsilon_t&=(\sum_{i=0}^{\infty} \frac{B}{3}^i)(I-2B)X_t\\
&=(\sum_{i=0}^{\infty} \frac{B}{3}^i)(X_t-2X_{t-1})\\
&=(\sum_{i=0}^{\infty} \frac{1}{3}^i)X_{t-1} -2X_{t-1-I}) \sim AR(\infty)
\end{aligned}
\end{equation*}

## Exercice 6

### 6.1

$\hat{X}_{t+1}=\alpha X_t + (1-\alpha)X_t$ Montrons par récurrence que $\hat{X}_{t+1}=\sum_{j=0}^{t-1}\alpha(1-\alpha)^jX_{t-j}$

\underline{Initialisation} Pour t=1, nous devons vérifier que:

$$ \hat{X}_2 = \alpha X_1 + (1-\alpha) \hat{X}_1 $$

À $t=1$, nous n'avons pas de terme antérieur, donc $\hat{X}_1 = X_1$, et notre formule de prédiction devient:

$$ \hat{X}_2 = \alpha X_1 + (1-\alpha) X_1 = X_1 $$

Cela montre que pour $t=1$, la formule de prédiction se réduit simplement à $\hat{X}_2 = X_1$, qui est équivalent à une somme qui n'a qu'un seul terme :

$$ \hat{X}_2 = \sum_{j=0}^{0}\alpha(1-\alpha)^jX_{1-j} = \alpha(1-\alpha)^0X_1 = X_1 $$

C'est notre équation de base pour $t=1$, et c'est vrai par définition.

\underline{Hérédité} Supposons que la formule soit vraie pour un certain $t$, c'est-à-dire que:

$\hat{X}_{t+1} = \sum_{j=0}^{t-1}\alpha(1-\alpha)^jX_{t-j}$

Il faut maintenant prouver que la formule est vraie pour $t+1$, c'est-à-dire que:

$\hat{X}_{t+2} = \sum_{j=0}^{t}\alpha(1-\alpha)^jX_{t-j+1}$

En remplaçant $\hat{X}_{t+1}$ par l'expression de l'hypothèse de récurrence, on obtient: \begin{equation*}
\begin{aligned}
\hat{X}_{t+2} &= \alpha X_{t+1} + (1-\alpha) \sum_{j=0}^{t-1}\alpha(1-\alpha)^jX_{t-j}\\
&= \alpha X_{t+1} + \sum_{j=0}^{t-1}\alpha(1-\alpha)^{j+1}X_{t-j}\\
&= \alpha X_{t+1}  + \sum_{j=1}^{t}\alpha(1-\alpha)^{j}X_{t-j+1}\\
&= \sum_{j=0}^{t}\alpha(1-\alpha)^j X_{t+1-j}
\end{aligned}
\end{equation*}

\underline{Conclusion}

Nous avons prouvé par récurrence que $\hat{X}_{t+1}=\sum_{j=0}^{\infty}\alpha(1-\alpha)^jX_{t-j}$.

### 6.2

-   Lorsque $\alpha \approx 0$, nous avons $\hat{X}_{t+1}$ qui dépend de $\hat{X}_t$, or $\hat{X}_{t}=\sum_{j=0}^{t-2}\alpha(1-\alpha)^jX_{t-j}=\dots=\alpha X_{t-1}$. Ainsi donc, $\alpha \approx 0$ $\hat{X}_{t+1}$ dépend d'un passé lointain.

-   Lorsque $\alpha \approx 1$, nous avons $\hat{X}_{t+1}$ qui dépend de $X_t$, donc d'un passé plus proche.

### 6.3

```{=tex}
\begin{equation*}
\begin{aligned}
\hat{X}_{t+1}&=\alpha X_t+(1-\alpha)\hat{X}_t
&=\alpha X_t + \hat{X}_t + \alpha \hat{X}_t
&= \hat{X}_t + \alpha(X_t-\hat{X}_t)
\end{aligned}
\end{equation*}
```
### 6.4

Posons$f(a) = \sum_{j=0}^{t-1} (1 - \alpha)^j (X_{t-j} - a)^2$ Ainsi donc :

```{=tex}
\begin{equation*}
\begin{aligned}
f'(a) &= -2 \sum_{j=0}^{t-1} (1 - \alpha)^j (X_{t-j} - a) = 0 \\
&\Leftrightarrow \sum_{j=0}^{t-1} (1 - \alpha)^j X_{t-j} = a \sum_{j=0}^{t-1} (1 - \alpha)^j \\
&\Leftrightarrow a = \frac{\sum_{j=0}^{t-1} (1 - \alpha)^j X_{t-j}}{\sum_{j=0}^{t-1} (1 - \alpha)^j}
\end{aligned}
\end{equation*}
```
À mesure que $t$ devient grand, la série géométrique converge et l'expression ci-dessus pour $a$ se simplifie en $\hat{X}_{t+1}$.

De plus, $f''(a)=2 \sum_{j=0}^{t-1} (1 - \alpha)^j >0$ donc $\hat{X}_{t+1} \approx a$.

### 6.5

Pour estimer $\alpha$, on peut utiliser la méthode des moindres carrés.

### 6.6

Nous pouvons écrire la fonction de coût $H_t(l, b)$ comme suit : $$ H_t(l, b) = \sum_{j=0}^{t-1} (1 - \alpha)^j (X_{t-j} - (l - bj))^2 $$

Pour minimiser cette fonction par rapport à $l$ et $b$, nous dérivons $H_t(l, b)$ par rapport à $l$ et $b$ et égalons les dérivées à zéro pour trouver les points critiques.

En différentiant $H_t(l, b)$ par rapport à $l$ et $b$ et en égalant à zéro, nous obtenons un système de deux équations (les conditions du premier ordre pour un minimum) :

$$ \frac{\partial H_t(l, b)}{\partial l} = -2\sum_{j=0}^{t-1} (1 - \alpha)^j (X_{t-j} - (l - bj)) = 0 $$ $$ \frac{\partial H_t(l, b)}{\partial b} = -2\sum_{j=0}^{t-1} j(1 - \alpha)^j (X_{t-j} - (l - bj)) = 0 $$

Le minimum se réalisant où les dérivées s'annulent (fonction convexe) et en remarquant que $$
\sum_{i=0}^{\infty}(1-\alpha)^i = \frac{1}{\alpha},
$$ $$
\sum_{i=0}^{\infty} i(1 - \alpha)^i = \frac{1-\alpha}{\alpha^2},
$$ $$
\sum_{i=0}^{\infty} i^2(1 - \alpha)^i = \frac{(1-\alpha)(2-\alpha)}{\alpha^3},
$$ on a: $$
\left\{
\begin{array}{l}
\sum_{i=0}^{\infty}(1 - \alpha)^i y_{t-i} - \frac{l}{\alpha} + b\frac{(1-\alpha)}{\alpha^2} =0 \\
\sum_{i=0}^{\infty} i(1 - \alpha)^i y_{t-i} - l\frac{1-\alpha}{\alpha^2} + b \frac{(1-\alpha)(2-\alpha)}{\alpha^3} =0
\end{array}
\right.
$$ Soit $$
\left\{
\begin{array}{l}
\alpha \sum_{i=0}^{\infty}(1 - \alpha)^i y_{t-i} - l + b\frac{(1-\alpha)}{\alpha} =0 \\
\alpha^2 \sum_{i=0}^{\infty} i(1 - \alpha)^i y_{t-i} - l(1 - \alpha) + b \frac{(1-\alpha)(2-\alpha)}{\alpha} =0
\end{array}
\right.
$$ On note: $L_1(t) = \alpha \sum_{i=0}^{\infty}(1 - \alpha)^i y_{t-i}$ et $L_2(t) = \alpha \sum_{i=0}^{\infty}(1 - \alpha)^i L_1(t - i)$ Remarquons que $L_2(t) - \alpha L_1(t) = \alpha^2 \sum_{i=1}^{\infty} i(1 - \alpha)^i y_{t-i}$ car: $$
L_2(t) - \alpha L_1(t) = \alpha \sum_{i=1}^{\infty}(1 - \alpha)^i L_1(t - i)
$$ $$
= \alpha^2 \sum_{i=1}^{\infty}\sum_{j=0}^{\infty}(1 - \alpha)^{i+j} y_{t-(i+j)}
$$ $$
= \alpha^2 \sum_{i=1}^{\infty}\sum_{k=i}^{\infty}(1 - \alpha)^k y_{t-k} = \alpha^2 \sum_{i=1}^{\infty} i(1 - \alpha)^i y_{t-i}
$$ Alors: $$
\left\{
\begin{array}{l}
L_1(t) - l + b\frac{(1-\alpha)}{\alpha} =0 \\
L_2(t) - \alpha L_1(t) - l(1 - \alpha) + b \frac{(1-\alpha)(2-\alpha)}{\alpha} =0
\end{array}
\right.
$$ Et $$
\left\{
\begin{array}{l}
L_1(t)= l - b\frac{(1-\alpha)}{\alpha} \\
L_2(t)=l - b\frac{2(1-\alpha)}{\alpha}
\end{array}
\right.
$$ Ainsi $$
\left\{
\begin{array}{l}
l = 2L_1(t) - L_2(t) \\
b= \frac{\alpha}{1-\alpha} (L_1(t) - L_2(t))
\end{array}
\right.
$$ Nous pouvons ensuite en déduire les formules de récurrences. $$
L_1(t) = \alpha y_t + (1 - \alpha)L_1(t - 1)
$$ $$
\left\{
\begin{array}{l}
L_2(t)=\alpha L_1(t) + (1 - \alpha)L_2(t - 1) \\
=\alpha^2 y_t + \alpha(1 - \alpha)L_1(t - 1) + (1 - \alpha)L_2(t - 1)
\end{array}
\right.
$$ Ainsi $$
l = (1 - (1 - \alpha)^2)y_t + (1 - \alpha)(2 - \alpha)L_1(t - 1) - (1 - \alpha)L_2(t - 1)
$$ En incorporant $L_1(t) = l_{t - 1} - \frac{1-\alpha}{\alpha} b_t$ et $L_2(t) = l_{t} - b_t \frac{2(1-\alpha)}{\alpha}$, on en déduit après simplification: $$
l_t = (1 - (1 - \alpha)^2)y_t + (1 - \alpha)^2(l_{t-1} + b_{t-1})
$$ De même, $$
b_t = \alpha^2 y_t + \alpha(1 - \alpha)L_1(t - 1) - \alpha L_2(t - 1)
$$ $$
b_t = \alpha^2 y_t - \alpha^2 l_{t-1} + (1 - \alpha)^2 b_{t-1}
$$

$$
\begin{cases}
l_t=l_{t-1}+b_{t-1}+(1-(1-\alpha)^2)(X_t-\hat{X}_t)\\
b_t=b_{t-1}+\alpha^2 (X_t-\hat{X}_t)
\end{cases}
$$ avec comme initialisation $l_0=X_1$ et $b_0=X_2-X_1$.
