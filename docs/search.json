[
  {
    "objectID": "index.html",
    "href": "index.html",
    "title": "",
    "section": "",
    "text": "Order By\n       Default\n         \n          Title\n        \n         \n          Date - Oldest\n        \n         \n          Date - Newest\n        \n     \n  \n\n\n\n\n\n\n\n\n\n\nApplication de la VaR\n\n\n\n\n\n\nrisque\n\n\nfinance\n\n\ngdr\n\n\n\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nComment fonctionne le bilan et le compte de résultat d’une entreprise\n\n\n\n\n\n\nfinance\n\n\ngdr\n\n\n\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nCours de microéconométrie\n\n\n\n\n\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nImprove Speaking Fluency\n\n\n\n\n\n\nAnglais\n\n\n\n\n\n\n\n\n\nApr 3, 2024\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nInvestissement Socialement Responsable (ISR): De quoi parle-t-on ?\n\n\n\n\n\n\nISR\n\n\nFinance\n\n\nESG\n\n\nArticle\n\n\n\n\n\n\n\n\n\nFeb 26, 2024\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nLa VaR\n\n\n\n\n\n\nfinance\n\n\ngdr\n\n\n\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nLa réglementation prudentielle\n\n\n\n\n\n\nfinance\n\n\ngdr\n\n\n\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nLa régression linéaire\n\n\n\n\n\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nLe risque, qu’est ce que c’est ?\n\n\n\n\n\n\nrisque\n\n\nfinance\n\n\ngdr\n\n\n\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nNotes de statistique non paramétrique\n\n\n\n\n\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nProjet statistique : Temps de survie ajusté sur la qualité de vie- Estimation de l’effet d’un traitement en vue d’une médecine personnalisée\n\n\n\n\n\n\nR\n\n\nMédecine personnalisée\n\n\nTemps de survie\n\n\nQualité de vie\n\n\nEnsai2A\n\n\n\n\n\n\n\n\n\nCheryl Kouadio, Néné Traore, Suzanne Heidsieck\n\n\n\n\n\n\n\n\n\n\n\n\nPrésentation\n\n\n\n\n\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nTD - Séries temporelles 1\n\n\n\n\n\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nTp noté - Apprentissage supervisé 2A\n\n\n\n\n\n\nR\n\n\nApprentissage supervisé\n\n\nEnsai2A\n\n\nRandom forest\n\n\nCART\n\n\nUMAP\n\n\n\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\nUn bref résumé des règlementations financières\n\n\n\n\n\n\nfinance\n\n\ngdr\n\n\n\n\n\n\n\n\n\nCheryl Kouadio\n\n\n\n\n\n\n\n\n\n\n\n\npartition\n\n\n\n\n\n\n\n\n\n\n\nMar 15, 2024\n\n\n\n\n\n\nNo matching items\n\n Back to top"
  },
  {
    "objectID": "articles/ISR/ISR.html",
    "href": "articles/ISR/ISR.html",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "",
    "text": "L’Investissement Socialement Responsable (ISR) représente une approche d’investissement qui privilégie l’intégration de critères extra-financiers, notamment les critères ESG (Environnementaux, Sociaux et de Gouvernance), dans les décisions d’investissement, que ce soit pour un individu ou pour une entreprise(“Qu’est-ce que L’ISR?” n.d.).\nLes critères ESG englobent des aspects tels que le respect de l’environnement (Environnementaux), le bien-être des salariés (Sociaux) et la qualité de la gouvernance au sein des entreprises (Gouvernance)(“Critères ESG : définitions et enjeux,” n.d.). Ils servent à évaluer la durabilité et l’éthique des investissements au-delà des performances financières traditionnelles. Ce faisant, ils offrent un cadre pour mesurer l’impact des entreprises sur la société et l’environnement, soulignant l’importance croissante des enjeux écologiques, climatiques et sociaux dans le monde actuel.\nEn outre, l’adoption de ces critères est souvent liée à la mise en œuvre de stratégies de Responsabilité Sociétale des Entreprises (RSE), illustrant un engagement vers une gestion d’entreprise plus responsable et transparente. Ainsi, l’ISR incarne non seulement une démarche d’investissement éthique mais également une vision à long terme visant à concilier performance économique et impact social positif."
  },
  {
    "objectID": "articles/ISR/ISR.html#quest-ce-que-linvestissement-socialement-responsable",
    "href": "articles/ISR/ISR.html#quest-ce-que-linvestissement-socialement-responsable",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "",
    "text": "L’Investissement Socialement Responsable (ISR) représente une approche d’investissement qui privilégie l’intégration de critères extra-financiers, notamment les critères ESG (Environnementaux, Sociaux et de Gouvernance), dans les décisions d’investissement, que ce soit pour un individu ou pour une entreprise(“Qu’est-ce que L’ISR?” n.d.).\nLes critères ESG englobent des aspects tels que le respect de l’environnement (Environnementaux), le bien-être des salariés (Sociaux) et la qualité de la gouvernance au sein des entreprises (Gouvernance)(“Critères ESG : définitions et enjeux,” n.d.). Ils servent à évaluer la durabilité et l’éthique des investissements au-delà des performances financières traditionnelles. Ce faisant, ils offrent un cadre pour mesurer l’impact des entreprises sur la société et l’environnement, soulignant l’importance croissante des enjeux écologiques, climatiques et sociaux dans le monde actuel.\nEn outre, l’adoption de ces critères est souvent liée à la mise en œuvre de stratégies de Responsabilité Sociétale des Entreprises (RSE), illustrant un engagement vers une gestion d’entreprise plus responsable et transparente. Ainsi, l’ISR incarne non seulement une démarche d’investissement éthique mais également une vision à long terme visant à concilier performance économique et impact social positif."
  },
  {
    "objectID": "articles/ISR/ISR.html#pourquoi-faire-un-investissement-socialement-responsable",
    "href": "articles/ISR/ISR.html#pourquoi-faire-un-investissement-socialement-responsable",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Pourquoi faire un investissement socialement responsable ?",
    "text": "Pourquoi faire un investissement socialement responsable ?\nL’ISR présente de multiples avantages tant pour les investisseurs que pour les entreprises engagées dans cette démarche. Ces avantages reflètent l’évolution des attentes sociétales et la reconnaissance croissante de l’importance de la durabilité et de l’éthique dans le monde des affaires. J’en ai identifier certains dans les points suivants.\n\nPour les entreprises :\n\nMeilleure image : Une forte performance ESG peut significativement améliorer la réputation d’une entreprise. Elle témoigne de son engagement envers des pratiques durables et éthiques, ce qui peut renforcer la confiance des consommateurs, des partenaires et des investisseurs.\nMeilleure performance financière : De nombreuses études démontrent que les entreprises avec une notation ESG élevée tendent de meilleures performances financièrement sur le long terme. Cela s’explique par une meilleure anticipation des risques, une gestion plus efficace et une capacité à saisir les opportunités de marché liées à la durabilité.\nMeilleure gestion des risques : L’adoption de pratiques ESG solides permet aux entreprises de mieux identifier et gérer les risques, qu’ils soient climatiques, sociaux ou de marché.\nMeilleure attractivité pour les investisseurs : En démontrant un engagement clair envers la durabilité et l’éthique, les entreprises attirent davantage d’investisseurs conscients de l’importance des critères ESG. Cette attractivité accrue peut se traduire par un accès facilité au capital et à de meilleures conditions de financement.\n\n\n\nPour les investisseurs :\n\nContribution à la réduction de certains risques financiers : En investissant dans des entreprise intégrant les critères ESG dans leur processus de décision, les investisseurs contribuent indirectement à une meilleure identification et anticipation les risques liés au changement climatique, aux problématiques sociales, et aux défis de gouvernance, ce qui contribue à une meilleure protection de leur capital sur le long terme.\nImpact positif sur la société : L’ISR permet aux investisseurs de contribuer activement à une économie plus durable et équitable. En choisissant d’investir dans des entreprises qui adoptent des pratiques responsables, ils favorisent des modèles économiques respectueux de l’environnement et du bien-être social.\n\nEn somme, l’ISR offre une perspective d’investissement qui va au-delà des retours financiers immédiats pour embrasser des bénéfices à long terme, tant sur le plan économique que social et environnemental."
  },
  {
    "objectID": "articles/ISR/ISR.html#comment-investir",
    "href": "articles/ISR/ISR.html#comment-investir",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Comment investir?",
    "text": "Comment investir?\n\nComment souscrire à un fonds ISR ?\nSouscrire à un fonds1 ISR (Investissement Socialement Responsable) est une démarche accessible pour tout particulier souhaitant allier rendement financier et impact positif sur la société et l’environnement(“Comment investir ?” n.d.). En consultant son conseiller financier ou son établissement bancaire, il est possible de placer son argent dans une variété de produits financiers responsables, parmi lesquels :\n\nAssurance-vie\nPlan d’Épargne en Actions (PEA) : (sous réserver de s’assurer que le fonds ISR choisi est bien éligible au PEA) Offre la possibilité de placer son épargne en actions de sociétés européennes.\nLes compte-titres ordinaires(CTO) : permet d’investir en bourse sur les marchés financiers français et/ou étrangers et dans tout type de valeurs mobilières (OPC2, actions, obligations, monétaire, warrants, trackers…).\nL’épargne salariale ou les plans d’épargne d’entreprise (PEE) : un produit d’épargne collectif qui permet aux salariés d’une entreprise de se constituer un portefeuille de valeurs mobilières qui peuvent proposer des fonds ISR.\nEnfin, certains produits d’épargne retraite individuelle, comme le Plan d’Epargne Retraite (PER).\n\nCes véhicules d’investissement permettent aux particuliers de contribuer à une économie plus durable tout en recherchant une performance financière. Il est recommandé de se rapprocher d’un conseiller pour déterminer le produit le mieux adapté à ses objectifs financiers et à ses valeurs éthiques.\n\n\nComment choisir une entreprise ISR ?\nChoisir une entreprise ou un fonds ISR (Investissement Socialement Responsable) nécessite une approche combinant analyses personnelle, financière et extra-financière, cette dernière se concentrant sur les critères ESG (Environnementaux, Sociaux et de Gouvernance)(“Comment bien choisir son fonds ISR ?” 2021).\nPour choisir efficacement une entreprise ISR, il est crucial de réaliser une analyse à triple volet :\n\nAnalyse des motivations personnelles : Vos convictions personnelles constituent un excellent moyen de choisir le fonds ISR qui vous convient le mieux. Elles vous aideront à identifier le type de placement à privilégier et définir par exemple des fonds thématiques, d’exclusion (normatifs3 ou sectoriels), Best effort, Best in Class, Best in Universe.\nAnalyse financière : Elle permet d’évaluer la performance économique de l’entreprise, sa santé financière, sa capacité à générer des profits et à maintenir une croissance durable. Cette analyse est indispensable pour s’assurer que l’entreprise est non seulement responsable, mais aussi viable et performante à long terme.\nAnalyse extra-financière (ESG) : Cette analyse complète l’évaluation financière en examinant comment l’entreprise aborde les défis et saisit les opportunités liées aux critères environnementaux, sociaux et de gouvernance. Cela permet de choisir des fonds intégrant les critères ESG comme les fonds labellisés ISR.\n\n\n\nLabel ISR :\nLe Label ISR est une certification officielle du ministère de l’économie et des finances français qui garantit que le fonds d’investissement respecte des critères ESG stricts4 dans ses choix d’investissement. Il assure également que le fonds investit dans des entreprises qui adhèrent à ces principes, offrant ainsi une couche supplémentaire de confiance pour les investisseurs soucieux de l’impact de leurs placements.\nCe label est attribué aux fonds candidats lorsque ceux-ci sont conformes aux exigences du label,classées en 6 catégories, qui constituent les 6 piliers du référentiel (“Critères d’attribution,” n.d.)."
  },
  {
    "objectID": "articles/ISR/ISR.html#footnotes",
    "href": "articles/ISR/ISR.html#footnotes",
    "title": "Investissement Socialement Responsable (ISR): De quoi parle-t-on ?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nSouvent appelé fonds de placement. Il s’agit d’une société d’ordre public ou privé qui investit du capital pour soutenir des projets souvent innovants.↩︎\norganismes de placement collectif↩︎\nLes fonds d’investissement d’exclusion normatifs font référence aux fonds faisant l’objet de plusieurs controverses.↩︎\ncf liste des fonds labellisés (“Liste des fonds labellisés,” n.d.).↩︎"
  },
  {
    "objectID": "articles/GDR/VaR_eg.html",
    "href": "articles/GDR/VaR_eg.html",
    "title": "Application de la VaR",
    "section": "",
    "text": "Nous allons ici nous intéresser aux applications de la Value at Risk (VaR) en finance. La VaR est une mesure de risque qui permet d’estimer les pertes maximales potentielles d’un portefeuille d’actifs financiers sur un horizon de temps donné, à un certain niveau de confiance. Elle est largement utilisée par les institutions financières pour évaluer et gérer les risques de marché, de crédit et de liquidité (cf. Value at-Risk).\nNous verrons ainsi les applications des VaR analytique, historique et Monte Carlo."
  },
  {
    "objectID": "articles/GDR/VaR_eg.html#var-analytique",
    "href": "articles/GDR/VaR_eg.html#var-analytique",
    "title": "Application de la VaR",
    "section": "VaR analytique",
    "text": "VaR analytique\nPour rappel, la VaR analytique ou gaussienne est basée sur la distribution gaussienne des rendements. Nous allons utiliser la distribution normale pour calculer la VaR à horizon 1 jour. La VaR à horizon 1 jour est définie comme suit :\n\\[\nVaR = -\\mu_{PnL} + \\Phi^{-1}(\\alpha) \\times \\sigma_{PnL}\n\\] où \\(\\Phi^{-1}(\\alpha)\\) est le quantile de la distribution normale du PnL (Profit and Loss) à \\(\\alpha\\).\nPour ce faire, nous allons tester que les rendements suivent une loi normale. Nous utiliserons le test de Shapiro (shapiro dans la librairie scipy.stats) dont l’hypothèse nulle est que la population étudiée suit une distribution normale.\n\nfrom scipy import stats\nstats.shapiro(train_close[\"Return\"]).pvalue\n\n7.902342429866941e-41\n\n\nNous obtenons une pvaleur quasiment nulle donc nous rejettons l’hypothèse de la distribution normale de nos rendements. Celà est plus visible avec le QQ-plot ci dessous qui montre clairement que les queues de distribution du rendement ne suit pas une loi normale.\n\n## Analyse graphique avec le QQ-plot\nplt.figure(figsize=(12, 8))\nprobplot = stats.probplot(train_close[\"Return\"], \n                        sparams = (np.mean(train_close[\"Return\"]), np.std(train_close[\"Return\"])), \n                        dist='norm', plot=plt)\nplt.plot(probplot[0][0], probplot[0][0], color='red', linestyle='dashed', linewidth=2, label='Première bissectrice')\nplt.title('QQ-plot')\n\nText(0.5, 1.0, 'QQ-plot')\n\n\n\n\n\n\n\n\n\n\nfrom scipy import stats\ndef gaussian_var(PnL, seuil):\n    mean_PnL = np.mean(PnL)\n    sd_PnL = np.std(PnL)\n    VaR = - mean_PnL + sd_PnL * stats.norm.ppf(seuil)\n    return VaR\n\nseuil = 0.99\nVaR_gaussienne = gaussian_var(train_close[\"Return\"], seuil)\n\nprint(f\"La VaR à horizon 1 jour est de {round(VaR_gaussienne, 4)}\")\n\nLa VaR à horizon 1 jour est de 0.0324\n\n\nLa VaR à horizon 1 jour est de 0.0324, ce qui signifie que la perte maximale en terme de rendements du portefeuille est de 3.24% en un jour.\nSur 10 jours, la VaR est de \\(VaR_{1j} \\times \\sqrt{10}=\\) 10.24%. Pour le visualiser sur la distribution des rendements, nous avons le graphique ci-dessous :\n\n# Plot histogram of returns\nplt.hist(train_close[\"Return\"], bins=50, density=True, alpha=0.7,color=\"grey\")\n\n# Plot VaR line\nplt.axvline(x=-VaR_gaussienne, color=\"orange\", linestyle=\"--\", linewidth=1)\nplt.axvline(x=0, color=\"grey\",  linewidth=0.5)\n\n# Add text for Loss and Gain\nplt.text(-0.01, plt.ylim()[1] * 0.9, 'Pertes', horizontalalignment='right', color='red')\nplt.text(0.01, plt.ylim()[1] * 0.9, 'Gains', horizontalalignment='left', color='green')\n\n\n# Add labels and title\nplt.xlabel(\"Returns\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Gaussian VaR at {seuil * 100}%, Var: {VaR_gaussienne:.4f}\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nBacktesting\nPour backtester la VaR, nous allons comparer dans l’échantillon test les rendements avec la VaR à horizon 1 jour. Si le rendement est inférieur à l’opposé de la VaR gaussienne, alors la VaR est violée et celà correspond à une exception.\nCi dessous, le graphique qui permet de visualiser le nombre d’exceptions que nous comptabilisons sur nos données test.\n\nplt.plot(ts_data.index[0:train_size], train_close['Return'], label=\"historical train returns\", color = 'gray')\nplt.plot(ts_data.index[train_size:], test_close['Return'], label=\"historical test returns\", color = 'blue')\nplt.plot(ts_data.index[train_size:], [-VaR_gaussienne for i in range(test_size)], label=\"gaussian VaR\", color = 'red')\nlist_exceptions_gaus = [i for i in range(len(test_close['Return'])) if test_close['Return'][i]&lt;-VaR_gaussienne]\nplt.scatter(test_close.index[list_exceptions_gaus], test_close['Return'][list_exceptions_gaus], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNous pouvons compter le nombre d’exceptions pour la VaR à horizon 1 jour qui est égale à 30 et en déduisons que le taux d’exception est 1.38%.\n\nround((len(list_exceptions_gaus)/test_size)*100,2) \n\n1.38\n\n\nPour tester la pertinence de la VaR calculée, il faudrait idéalement que le taux d’exception soit inférieur à 1%. Pour ce faire, nous pouvons effectuer un test de proportion. Nous utiliserons la fonction stats.binomtest pour effectuer ce test.\n\ndef ptest(p0,n,k) :\n  variance=p0*(1-p0)/n\n  p=(k/n)\n  t=(p-p0)/np.sqrt(variance)\n\n  pvaleur=1-stats.norm.cdf(t)\n  return pvaleur\n\nptest(0.01,test_size,len(list_exceptions_gaus))\n\n0.03729091089082781\n\n\nLa pvaleur de ce test est 3.70%, celà est inférieur à 5% donc nous rejetons l’hypothèse nulle selon laquelle le taux d’exception est égale à 0.01 au risque 5% de se tromper. Celà nous indique que la VaR gaussienne n’est pas performante. Ceci n’est pas surprenant étant donné que nous faisons une hypothèse sur la distribution des rendements qui n’est pas vérifiée."
  },
  {
    "objectID": "articles/GDR/VaR_eg.html#var-historique",
    "href": "articles/GDR/VaR_eg.html#var-historique",
    "title": "Application de la VaR",
    "section": "VaR historique",
    "text": "VaR historique\nLa VaR historique est basée sur les rendements historiques. Elle est définie comme l’opposé du quantile de niveau \\(1-\\alpha\\) des rendements historiques.\nConsidérons les mouvements de prix quotidiens pour l’indice CAC40 au cours des 6513 jours de trading. Nous avons donc 6513 scénarios ou cas qui serviront de guide pour les performances futures de l’indice, c’est-à-dire que les 6513 derniers jours seront représentatifs de ce qui se passera demain.\nAinsi donc la VaR historique pour un horizon de 1jour à 99% correspond au 1er percentile de la distribution de probabilité des rendements quotidiens (le top 1% des pires rendements).\n\ndef historical_var(PnL, seuil):\n    return -np.percentile(PnL, (1 - seuil) * 100)\n\nVaR_historique = historical_var(train_close[\"Return\"],seuil)\nprint(f\"La VaR historique à horizon 1 jour est de {round(VaR_historique, 4)}\")\n\nLa VaR historique à horizon 1 jour est de 0.0396\n\n\nNous en déduisons que la perte maximale en terme de rendements du portefeuille est de 3.96% en un jour (soit 12.52% en 10jours)\n\n# Plot histogram of returns\nplt.hist(train_close[\"Return\"], bins=50, density=True, alpha=0.7,color=\"grey\")\n\n# Plot VaR line\nplt.axvline(x=-VaR_historique, color=\"orange\", linestyle=\"--\", linewidth=1)\nplt.axvline(x=0, color=\"grey\",  linewidth=1)\n# Add text for Loss and Gain\nplt.text(- 0.01, plt.ylim()[1] * 0.9, 'Pertes', horizontalalignment='right', color='red')\nplt.text(0.01, plt.ylim()[1] * 0.9, 'Gains', horizontalalignment='left', color='green')\n\n\n# Add labels and title\nplt.xlabel(\"Returns\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Historical VaR at {seuil * 100}% Var: {VaR_historique:.4f}\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nBacktesting\nEn ce qui concerne le backtesting, nous pouvons voir que la VaR historique est beaucoup moins violée dans l’échantillon test que la VaR gaussienne. Le taux d’exception est de 0.64%.\n\nimport matplotlib.pyplot as plt\nplt.plot(ts_data.index[0:train_size], train_close['Return'], label=\"historical train returns\", color = 'gray')\nplt.plot(ts_data.index[train_size:], test_close['Return'], label=\"historical test returns\", color = 'blue')\nplt.plot(ts_data.index[train_size:], [-VaR_historique for i in range(test_size)], label=\"historical VaR\", color = 'red')\nlist_exceptions_hist = [i for i in range(len(test_close['Return'])) if test_close['Return'][i]&lt;-VaR_historique]\nplt.scatter(test_close.index[list_exceptions_hist], test_close['Return'][list_exceptions_hist], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nNous pouvons compter le nombre d’exceptions pour la VaR à horizon 1 jour qui est égale à 14 et en déduisons que le taux d’exception est 0.64%. Ce taux d’exception est statistiquement supérieur à 1% (car la pvaleur est d’environ 0.95). Ainsi, la VaR historique est performante pour la période considérée.\n\nround((len(list_exceptions_hist)/test_size)*100,2) \nptest(0.01,test_size,len(list_exceptions_hist))\n\n0.9522032659883799"
  },
  {
    "objectID": "articles/GDR/VaR_eg.html#var-monte-carlo",
    "href": "articles/GDR/VaR_eg.html#var-monte-carlo",
    "title": "Application de la VaR",
    "section": "VaR Monte Carlo",
    "text": "VaR Monte Carlo\nLa VaR Monte Carlo est basée sur la simulation de trajectoires de rendements. Nous allons simuler jusqu’à 10000 scénarios de rendements et calculer la VaR à horizon 1 jour en posant une hypothèse de normalité sur la distribution des rendements afin de voir quand est ce que la VaR se stabilise.\n\nVaR_results = []\n\nnum_simulations_list = range(10, 10000 + 1, 1)\nmean=train_close[\"Return\"].mean()\nstd = train_close[\"Return\"].std()\n\nfor num_simulations in num_simulations_list:\n  # Generate random scenarios of future returns\n  simulated_returns = np.random.normal(mean, std, size= num_simulations)\n\n  # Calculate portfolio values for each scenario\n  portfolio_values = (train_close[\"Close\"].iloc[-1] * (1 + simulated_returns))\n\n  # Convert portfolio_values into a DataFrame\n  portfolio_values = pd.DataFrame(portfolio_values)\n\n  # Calculate portfolio returns for each scenario\n  portfolio_returns = portfolio_values.pct_change()\n  portfolio_returns=portfolio_returns.dropna()\n  portfolio_returns=portfolio_returns.mean(axis=1)\n\n\n  # Calculate VaR\n  if portfolio_returns.iloc[-1] != 0:\n      VaR_monte_carlo =  historical_var(portfolio_returns,seuil)\n  else:\n      VaR_monte_carlo = 0\n  \n  VaR_results.append(VaR_monte_carlo)\n\n\n# Plotting the results\nplt.figure(figsize=(10, 6))\nplt.xticks(np.arange(0,10000 + 1, 1000))\nplt.plot(num_simulations_list, VaR_results, linestyle='-')\nplt.xlabel('Number of Simulations')\nplt.ylabel('Value at Risk (VaR)')\nplt.title('VaR vs Number of Simulations')\nplt.grid(True)\nplt.show()\n# Customize x-axis ticks\n\n\n\n\n\n\n\n\nVisuellement, la VaR se stabilise à partir de 3000 scénarios. Nous utiliserons donc 3000 simulations de rendements. Nous en déduisons que la perte maximale en terme de rendements du portefeuille est de 4.31% en un jour (soit 13.98% en 10jours)\n\nnum_simulations = 3000\n\n# Generate random scenarios of future returns\nsimulated_returns = np.random.normal(mean, std, size= num_simulations)\n\n# Calculate portfolio values for each scenario\nportfolio_values = (train_close[\"Close\"].iloc[-1] * (1 + simulated_returns))\n\n# Convert portfolio_values into a DataFrame\nportfolio_values = pd.DataFrame(portfolio_values)\n\n# Calculate portfolio returns for each scenario\nportfolio_returns = portfolio_values.pct_change()\nportfolio_returns=portfolio_returns.dropna()\nportfolio_returns=portfolio_returns.mean(axis=1)\n\n\n# Calculate VaR\nif portfolio_returns.iloc[-1] != 0:\n    VaR_monte_carlo =  historical_var(portfolio_returns,seuil)\nelse:\n    VaR_monte_carlo = 0\n\nVaR_monte_carlo\n\n0.046418576829940564\n\n\n\n# Plot histogram of returns\nplt.hist(portfolio_returns, bins=50, density=True, alpha=0.7,color=\"grey\")\n\n# Plot VaR line\nplt.axvline(x=-VaR_monte_carlo, color=\"orange\", linestyle=\"--\", linewidth=1)\nplt.axvline(x=0, color=\"grey\",  linewidth=1)\n# Add text for Loss and Gain\nplt.text(- 0.01, plt.ylim()[1] * 0.9, 'Pertes', horizontalalignment='right', color='red')\nplt.text(0.01, plt.ylim()[1] * 0.9, 'Gains', horizontalalignment='left', color='green')\n\n\n# Add labels and title\nplt.xlabel(\"Returns\")\nplt.ylabel(\"Frequency\")\nplt.title(f\"Simulated Returns, Monte carlo VaR at {seuil * 100}% Var: {VaR_monte_carlo:.4f}\")\n\n# Show the plot\nplt.show()\n\n\n\n\n\n\n\n\n\nBacktesting\nEn ce qui concerne le backtesting, nous pouvons voir que la VaR historique est beaucoup moins violée dans l’échantillon test que les deux autres VaRs. En effet, le taux d’exception est de 0.37%.\n\nplt.plot(ts_data.index[0:train_size], train_close['Return'], label=\"historical train log returns\", color = 'gray')\nplt.plot(ts_data.index[train_size:], test_close['Return'], label=\"historical test log returns\", color = 'blue')\nplt.plot(ts_data.index[train_size:], [-VaR_monte_carlo for i in range(test_size)], label=\"Non parametric Bootstrap VaR\", color = 'red')\nlist_exceptions_np_boot = [i for i in range(len(test_close['Return'])) if test_close['Return'][i]&lt;-VaR_monte_carlo]\nplt.scatter(test_close.index[list_exceptions_np_boot], test_close['Return'][list_exceptions_np_boot], color='red', label='Exceptions')\nplt.title('CAC40')\nplt.ylabel('Values')\nplt.plot()\nplt.legend()\nplt.show()\n\n\n\n\n\n\n\n\nCe taux est statistiquement inférieur à 1% ce qui temoigne de la performance de la VaR monte carlo.\n\nround((len(list_exceptions_np_boot)/test_size)*100,2) \nptest(0.01,test_size,len(list_exceptions_np_boot))\n\n0.9984628718602482"
  },
  {
    "objectID": "articles/GDR/resume_reglementation.html",
    "href": "articles/GDR/resume_reglementation.html",
    "title": "Un bref résumé des règlementations financières",
    "section": "",
    "text": "L’importance des fonds propres dans l’activité bancaire.\nDans le bilan simplifié, inspiré du modèle d’entreprise présenté dans la section i- Le bilan d’entreprise, la structure se divise en deux principales composantes : l’actif et le passif. L’actif regroupe les immobilisations de la banque, les crédits et prêts accordés aux ménages et aux entreprises, ainsi que les portefeuilles de titres, parmi d’autres éléments. Concernant le passif, il comprend les dépôts et les épargnes des ménages, les dettes, et surtout, les fonds propres ou capitaux propres.\nLes fonds propres sont cruciaux dans le passif d’une banque et se composent principalement d’actions ordinaires, de certificats d’investissement et de réserves de la banque. Ils jouent un triple rôle : ils favorisent la croissance, garantissent les activités de la banque (marquant ainsi la solidité bancaire), et constituent les ressources les plus onéreuses pour la couverture des risques. Ces fonds sont rémunérés via le ROE (Return on Equity), la banque s’efforçant de maximiser ce retour sur capitaux propres pour ses actionnaires, en minimisant les risques et en augmentant les profits.\n\n\nAccords de Bâle I, II, III et IV : une évolution des régulations bancaires.\nLe comité de Bâle, en 1988, a instauré un ratio de solvabilité, le ratio Cooke, imposant aux banques de maintenir un minimum de 8% de fonds propres par rapport à leurs actifs pondérés en fonction des risques. Ce ratio a été remplacé en 2006 par le ratio McDonough dans le cadre des Accords de Bâle II (publiés en 2004), qui introduisaient une gestion des risques plus sophistiquée incluant les risques de crédit, de marché et opérationnels. Les principes de ces accords reposent sur trois piliers : le premier concerne les exigences minimales de fonds propres, le deuxième la surveillance prudentielle, et le troisième la transparence et la discipline de marché.\nLe premier pilier de Bâle II propose trois méthodes pour évaluer le risque de crédit : l’approche standard (Standard Approach, SA), l’approche fondée sur les notations internes (Internal Ratings-Based, IRB) et l’approche IRB avancée. L’approche standard, la plus simple, utilise des taux de pondération fixes définis par les régulateurs. L’approche IRB permet aux banques d’utiliser leurs propres évaluations des probabilités de défaillance (Probability of Default, PD), tandis que l’approche IRB avancée permet de modéliser des paramètres de risque de crédit plus détaillés tels que les taux de perte en cas de défaut (Loss Given Default, LGD) et l’exposition en cas de défaut (Exposure at Default, EAD).\nPour le risque opérationnel, trois méthodes ont été introduites : l’approche de l’indicateur de base (Basic Indicator Approach, BIA), l’approche standardisée (Standardized Approach, TSA) et l’approche de mesure avancée (Advanced Measurement Approach, AMA). La méthode BIA est basique et repose sur un indicateur de charge en capital proposé par le comité de Bâle. La méthode TSA adapte la BIA en tenant compte de la nature des activités de la banque, et la méthode AMA permet aux banques de modéliser leurs propres paramètres de risque opérationnel.\nAvec l’essor des crises financières, comme celle de 2008, les régulations telles que Bâle III et Solvabilité II ont évolué pour renforcer les exigences en fonds propres et en liquidités. Le ratio de Bâle III, introduit en 2010, visait à améliorer la gestion des risques bancaires. Il comprenait un ratio de levier simple et un ratio de liquidité à court terme pour gérer des retraits massifs de dépôts. Bâle IV, publié en 2017, a poursuivi ces efforts en cherchant à standardiser et renforcer l’évaluation des risques pour accroître la stabilité financière. Bien qu’il continue sur la lignée de Bâle III, Bâle IV impacte significativement toutes les directions de banque, en particulier les directions des risques et financières, affectant le calcul des actifs pondérés par le risque (RWA) et l’évaluation du coût du capital et de l’impact sur les fonds propres.\n\n\n\nChronologie des accords de bâle\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "articles/GDR/var.html",
    "href": "articles/GDR/var.html",
    "title": "La VaR",
    "section": "",
    "text": "{#sec-var-def} La mesure de risque réglementaire correspond à la valeur en risque ou value at-risk (VaR) qui est le quantile de perte (ou perte maximale subie). Il s’agit dans cette section de développer la notion de VaR pour des portefeuilles linéaires et non linéaires."
  },
  {
    "objectID": "articles/GDR/var.html#le-backtesting",
    "href": "articles/GDR/var.html#le-backtesting",
    "title": "La VaR",
    "section": "2.1 Le backtesting",
    "text": "2.1 Le backtesting\nLe backtesting est un contrôle de la qualité de la VaR pour un horizon de 1 jour. Il permet de vérifier si la VaR est bien calibrée. Pour cela, on compare la VaR calculée avec la perte réelle. Si la VaR est bien calibrée, la perte réelle ne doit pas dépasser la VaR dans 99,99% des cas. La commission bancaire utilise un nombre d’exception pour valider le modèle. Notons PnL le profit and loss du portefeuille et VaR la valeur à risque. Nous avons : \\[\nP(PnL&lt;-VaR)=1-\\alpha\n\\]\nConsidérons maintenant la variable de bernoulli \\(I\\) qui vaut 1 si le PnL est inférieur à l’opposé de la VaR avec probabilité \\(1-\\alpha\\) (une exception) et 0 sinon. Pour une période ouvré comptant n jours, la probabilité d’avoir \\(i\\) exceptions est donnée par la loi binomiale \\(\\mathcal{text{Bin}}(n,1-\\alpha)\\). La probabilité d’avoir plus de \\(k\\) exceptions est donnée par la loi binomiale cumulative \\(\\mathcal{B}(n,1-\\alpha)\\). La probabilité d’avoir au plus de \\(i\\) exceptions est donnée par : \\[\nP(N_{ex}\\leq i)=\\sum_{j=0}^{i} \\binom{n}{j}(1-\\alpha)^j \\alpha^{n-j}\n\\]\nPour tester que la probabilité d’exception n’excède pas \\(1-\\alpha\\), nous pouvons utiliser un test de proportion. Si la proportion d’exceptions empirique est supérieur à celui attendu, le modèle est rejeté :\n\\[\nH_0: P(PnL&lt;-VaR)=1-\\alpha=p_0 \\quad \\text{vs} \\quad H_1: P(PnL&lt;-VaR)&gt;1-\\alpha=p_0\n\\]\nLa statistique de test est donnée par (sous \\(H_0\\)) : \\[\nT= \\frac{1}{n} \\sum_{i=1}^{n} I_{Pnl&lt;-VaR} \\sim \\mathcal{N}(p_0,\\frac{p_0(1-p_0)}{n})\n\\] qui donne la proportion d’exceptions observée lorsque \\(np&gt;5\\) et \\(n(1-p)&gt;5\\).\nLa p-valeur est donnée par \\(P(T&gt;t|H_0)=1-\\phi(t)\\) où \\(t\\) est la valeur observée de la statistique de test et \\(\\phi\\) est la fonction de répartition de la loi normale. # La VaR analytique"
  },
  {
    "objectID": "articles/GDR/var.html#cas-général",
    "href": "articles/GDR/var.html#cas-général",
    "title": "La VaR",
    "section": "2.2 Cas général",
    "text": "2.2 Cas général\nDans cette approche, nous considérons que les facteurs de risque sont gaussiens \\(PnL \\sim N(\\mu_{PnL}, \\sigma_{PnL})\\) . Nous avons donc :\n\\[\\begin{align*}\nP(PnL \\leq VaR) = 1 - \\alpha\\Leftrightarrow\\Phi \\left( \\frac{VaR - \\mu_{PnL}}{\\sigma_{PnL}} \\right) = 1 - \\alpha\n\\end{align*}\\]\nNous en déduisons donc que la VaR est calculé comme suit :\n\\[\nVaR = -\\mu_{PnL} + \\Phi^{-1}(\\alpha) \\times \\sigma_{PnL}\n\\]\nLorsque \\(\\alpha=99\\%\\), \\(\\Phi^{-1}(\\alpha) = 2.33\\). Remarquons que la VaR est une fonction décroissante de l’espérance de PnL et une fonction croissante de la volatilité du PnL. En pratique, nous posons \\(\\mu_{PnL}=0\\) car il est difficle de prévoir l’espérance du PnL futur.\n\n2.2.1 Exemple\nNous considérons une position courte de 1 million de dollars sur le contrat à terme S&P 500. Nous estimons que la volatilité annualisée \\(\\sigma_{\\text{SPX}}\\) est égale à 35%.\nLa perte du portefeuille est égale à \\(L(w) = N \\times R_{\\text{SPX}}\\) où \\(N\\) est le montant de l’exposition (−1 million de dollars) et \\(R_{\\text{SPX}}\\) est le rendement (gaussien) de l’indice S&P 500. Nous déduisons que la volatilité de la perte annualisée est \\(\\sigma(L) = |N| \\times \\sigma_{\\text{SPX}}\\). La valeur à risque pour une période de détention d’un an est :\n\\[\nVaR_{1Y}(99\\%)=2.33 \\times 0.35 \\times 1 000 000 = 815 500 \\text{ euros}\n\\]\nAinsi, la perte maximale pour l’investisseur sur un 1an s’élève à 815 500€ avec un seuil de confiance de 99% (donc 1% de chance de se tromper).\nPour utiliser une autre période de détention, nous utilisons la règle de la racine carré pour convertir la volatilité pour une fréquence donné \\(f_1\\) en une autre volatilité pour une autre fréquence \\(f_2\\) : \\(\\sigma_{f_2}=\\sqrt{f_2/f_1}\\sigma_{f_1}\\)\nAinsi, nous obtenons pour les VaRs 1 mois et 1 jour les résultats suivants :\n\\[\\begin{align*}\nVaR_{1M}(99\\%) &= \\frac{815 500}{\\sqrt{12}}=235414  \\text{ euros} \\\\\nVaR_{1J}(99\\%) &= \\frac{815 500}{\\sqrt{260}}=235414  \\text{ euros}\n\\end{align*}\\]\nEn pratique, la VaR est calculé sur 1 jour, pour l’avoir sur n jours, il faut donc appliquer cette formule :\n\\[\nVaR_{nJ} =  VaR_(1J) \\times \\sqrt{n}\n\\]"
  },
  {
    "objectID": "articles/GDR/var.html#modèles-linéaires-de-facteurs",
    "href": "articles/GDR/var.html#modèles-linéaires-de-facteurs",
    "title": "La VaR",
    "section": "2.3 Modèles linéaires de facteurs",
    "text": "2.3 Modèles linéaires de facteurs\nNous considérons un portefeuille de \\(n\\) actifs et une fonction de tarification \\(g\\) qui est linéaire par rapport aux prix des actifs. Nous avons : \\[\nP(t; \\theta) = \\sum_{i=1}^n \\theta_i P_{i}(t)\n\\]\nIci, \\(P_i(t)\\) est connu tandis que \\(P_i(t+h)\\) est stochastique. La première idée est de choisir les facteurs comme étant les prix futurs.Dans cette approche, les rendements des actifs sont les facteurs de risque du marché et chaque actif possède son propre facteur de risque.\nLe problème est que les prix sont loin d’être stationnaires, ce qui nous amène à devoir affronter certains problèmes pour modéliser la distribution \\(F_t\\). Une autre idée est de récrire le prix futur comme suit : \\[\nP_i(t+h) = P_i(t) (1 + R_i(t;h))\n\\] où \\(R_i(t;h)\\) est le retour de l’actif entre \\(t\\) et \\(t+h\\).\nNous déduisons que le PnL aléatoire est :\n\\[\\begin{align*}\nPnL &= P(t+h,\\theta) - P(t,\\theta) \\\\\n&= \\sum_{i=1}^n \\theta_i P_i(t+h) - \\sum_{i=1}^n \\theta_i P_i(t) \\\\\n&= \\sum_{i=1}^n\\theta_i P_i(t) (1 - R_i(t,h))  - \\sum_{i=1}^n \\theta_i P_i(t) \\\\\n&= \\sum_{i=1}^n \\theta_i P_i(t)  R_i(t,h) \\\\\n&= \\sum_{i=1}^n W_i(t)  R_i(t,h)\n\\end{align*}\\]\noù \\(W_i = \\theta_i P_i(t)\\) est le montant investi (ou l’exposition nominale)dans l’actif \\(i\\).\n\nSoit \\(R(t,h)\\) le vecteur des retours des actifs et \\(W_t = (W_{1,t}, \\dots, W_{n,t})\\) le vecteur des montants investis. Il s’ensuit que : \\[\nPnL = \\sum_{i=1}^n W_i(t) R_i(t) = W_t^T R(t,h)\n\\]\nSi nous supposons que \\(R(t+h) \\sim N(\\mu, \\Sigma)\\), nous déduisons que \\(\\mu(PnL) = W_t^T \\mu\\) et \\(\\sigma^2(\\Pi) = W_t^T \\Sigma W_t\\). Utilisant l’Équation (2.6), l’expression de la valeur à risque est : \\[\n\\text{VaR}_\\alpha (w; h) = -W_t^T \\mu + \\phi^{-1}(\\alpha) \\sqrt{W_t^T \\Sigma W_t}\n\\]\nDans cette approche, nous avons seulement besoin d’estimer la matrice de covariance des retours des actifs pour calculer la valeur à risque. Cela explique la popularité de ce modèle, surtout lorsque le P&L du portefeuille est une fonction linéaire des retours des actifs.\n\n2.3.1 Exemple Apple & Coca-Cola\nConsidérons l’exemple des entreprises d’Apple et de Coca-Cola. Les expositions nominales sont de 1 093,3$ pour Apple et de 842,8$ pour Coca-Cola. Si nous prenons en compte les prix historiques du 7 janvier 2014 au 2 janvier 2015, l’écart type estimé des rendements quotidiens est de 1,3611 % pour Apple et de 0,9468 % pour Coca-Cola, tandis que la corrélation croisée est égale à 12,0787 %. Il s’ensuit que :\n\\[\\begin{align*}\n\\sigma^2(PnL) &= W_t^T \\Sigma W_t = 1093.3^2 \\left(\\frac{1.3611}{100}\\right)^2 + 842.8^2 \\left(\\frac{0.9468}{100}\\right)^2 \\\\\n&+ 2 \\times 12.0787 \\times \\frac{1.0933 \\times 842.8 \\times 1.3611 \\times 0.9468}{10000} = 313.80\n\\end{align*}\\]\nSi nous omettons le terme de rendement attendu \\(-W_t^T \\mu\\), nous déduisons que la valeur à risque quotidienne à 99% est de 41,21 $. Nous obtenons une figure inférieure à celle de la valeur à risque historique, qui était de 47,39 $. Nous expliquons ce résultat par le fait que la distribution gaussienne sous-estime la probabilité des événements extrêmes et n’est donc pas adaptée à des calculs précis de risque dans des situations de marché volatiles.\n\n\n2.3.2 Exemple de portefeuille linéaire d’actifs\nConsidérons un portefeuille linéaire composé de trois actifs A (2 titres positions longues donc \\(\\theta_A=2\\)), B(1 titre position courte donc \\(\\theta_B=-1\\)) et C(1 titre position longue donc \\(\\theta_C=1\\)) dont les rendements journaliers sont en moyenne égaux à : \\[\n\\mu =\n\\begin{pmatrix}\n50pb\\\\\n30pb \\\\\n20pb\n\\end{pmatrix}\n= \\begin{pmatrix}\n0.005\\\\\n0.003 \\\\\n0.002\n\\end{pmatrix}\n\\]\nLes volatilités journalières sont égales à 2%, 3% et 1%. Quant aux prix des actifs, ils sont respectivement de 244€, 135€,315€. La matrice de corrélation est donnée par : \\[\n\\mu =\n\\begin{pmatrix}\n1 &\\\\\n0.5 & 1 & \\\\\n0.25 & 0.6 & 1\n\\end{pmatrix}\n\\]\nNous obtenons que:\n\\[\nVaR(99\\%)=2.3263 \\times \\sqrt{82.1176} - 2.665 = 18.42\n\\]\nLa perte maximale de ce portefeuille en une journée est donc de 18.42€ avec un risque 1% de se tromper.\n\n2.3.2.1 Implémentation en R\n\ncalculate_VaR &lt;- function(mu,W_t,std_devs,correlation_matrix, alpha) {\n  # Dimensions de la matrice\n  n &lt;- nrow(correlation_matrix)\n  \n  # Convertir les écarts-types et les corrélations en matrice de covariance\n  cov_matrix &lt;- matrix(0, nrow = n, ncol = n)\n  for (i in 1:n) {\n    for (j in 1:n) {\n      cov_matrix[i, j] &lt;- std_devs[i] * std_devs[j] * correlation_matrix[i, j]\n    }\n  }\n  \n  q&lt;-qnorm(alpha, mean = 0, sd = 1)\n\n  VaR&lt;- -t(W_t) %*% mu + 2.3263*sqrt(t(W_t) %*% cov_matrix %*% W_t)\n\n  return(VaR)\n}\n\n\nW_t &lt;- matrix(c(2*244, -1*135, 1*315),nrow = 3)\nmu &lt;- matrix(c(50,30,20),nrow = 3)/10000\nstd_devs&lt;- c(2,3,1)/100\n\n# Matrice de corrélation\ncorrelation_matrix &lt;- matrix(c(\n1,0.5,0.25,\n0.5,1,0.6,\n0.25,0.6,1\n), nrow = 3, byrow = TRUE)\n\nVaR&lt;- calculate_VaR(mu,W_t,std_devs,correlation_matrix,0.99)\nVaR\n\n         [,1]\n[1,] 18.41564"
  },
  {
    "objectID": "articles/GDR/var.html#modèles-factoriels-de-risque",
    "href": "articles/GDR/var.html#modèles-factoriels-de-risque",
    "title": "La VaR",
    "section": "2.4 Modèles factoriels de risque",
    "text": "2.4 Modèles factoriels de risque\nNous supposons que la valeur du portefeuille dépend de \\(m\\) facteurs de risque. Nous avons que la valeur du portefeuille à \\(t+h\\) dépend des facteurs de risques :\n\\[\nP(t+h,\\theta) = g(F_1(t+h), \\dots, F_m(t+h);\\theta)\n\\]\noù g est la fonction de valorisation (pricing function)\nSupposons maintenant que le portefeuille soit linéaire par rapport aux facteurs de risque, ainsi donc le retour des actifs à l’horizon h est :\n\\[\\begin{align*}\nR(t,h)&= B F(t,h) + \\epsilon(t,h) \\\\\n\\end{align*}\\] où \\(B\\) est la matrice des sensibilités du portefeuille aux facteurs de risque (interne, commun) et \\(F(t,h)\\) est le vecteur des facteurs de risque à l’horizon h avec \\(E(F)=\\mu(F)\\) et \\(cov(F)=\\Omega\\). De plus, \\(\\epsilon(t,h)\\) est le vecteur des erreurs qui sont des variables aléatoires gaussiennes indépendantes avec \\(E(\\epsilon)=0\\) et \\(cov(\\epsilon)=D\\).\nSi le retour des actifs est gaussien, nous en deduisons que le PnL est une variable aléatoire gaussienne:\n\\[\nPnL \\sim \\mathcal{N}(W_t^TB^T\\mu(F), W_t^T (B\\Omega B^T + D) W_t)\n\\]\nAinsi donc la VaR est calculé comme suit : \\(VaR=-W_t^TB^T\\mu(F) + \\Phi^{-1}(\\alpha)\\sqrt{ W_t^T (B\\Omega B^T + D) W_t)}\\)\nCette méthode repose sur 3 hypothèses : l’indépendance temporelle des variations de la valeur du portefeuille, la normalité des facteurs et la relation linéaire entre les facteurs et la valeur du portefeuille. En général, nous ne connaissons pas \\(B, \\mu, \\Sigma\\). Dans la pratique, nous les estimons à partir des données historiques des facteurs et \\(B\\) est le vecteur des sensibilités du portefeuille aux facteurs de risque. La seuil difficulté de cette méthode est l’estimation de la matrice de variance covariance.\n\n2.4.1 Exemple d’un portefeuille obligataire sans risque de crédit\nNous considérons une exposition sur une obligation américaine à $t=$31 décembre 2014. Le nominal de l’obligation est de 100 tandis que les coupons annuels \\(C(t_m)\\) sont égaux à 5, \\(t_m&gt;t\\). La maturité résiduelle est de cinq ans et les dates de fixation sont à la fin de décembre (\\(n_C=5\\). Le nombre d’obligations détenues dans le portefeuille est de 10 000.\nNous notons \\(B_t(T)\\) le prix d’une obligation zéro coupon (montant qu’un investisseur serait prêt à payer aujourd’hui pour recevoir un paiement fixe à une date future : combien me rapport un euro à maturité \\(T\\) aujourd’hui?) au temps \\(t\\) pour l’échéance \\(T\\). Nous avons \\(B_t(T) = e^{-(T - t)R_t(T)}\\) où \\(R_t(T)\\) est le taux de rendement zéro coupon.\nLa valeur de l’obligation est donc : \\[\nP(t) =  \\sum_{m=1}^{n_C} C(t_m) B_t(t_m)\n\\]\nOn en déduit que le PnL est : \\[\\begin{align*}\nPnL &=  ( \\sum_{m=1}^{n_C} C(t_m) B_{t+h}(t_m) - \\times \\sum_{m=1}^{n_C} C(t_m) B_t(t_m) )\\\\\n&=  -\\sum_{m=1}^{n_C} C(t_m) (t_m - t) B_{t}(t_m) \\Delta R(t+h,t_m)) \\\\\n&=  \\sum_{m=1}^{n_C} W_t(tm) \\Delta R(t+h,t_m) \\\\\n\\end{align*}\\]\noù \\(W_t(t_m) = -C(t_m)(t_m-t) B_t(t_m)\\) est le montant investi dans l’obligation \\(m\\) et \\(\\Delta R(t+h,t_m)\\) est le rendement de l’obligation \\(m\\) entre \\(t\\) et \\(t+h\\).\n\n\n\n\\(t_m - t\\)\n\\(C(t_m)\\)\n\\(R_t(t_m)\\)\n\\(B_t(t_m)\\)\n\\(W_{t_m}\\)\n\n\n\n\n1\n5\n0.431%\n0.996\n-4.978\n\n\n2\n5\n0.879%\n0.983\n-9.826\n\n\n3\n5\n1.276%\n0.962\n-14.437\n\n\n4\n5\n1.569%\n0.939\n-18.783\n\n\n5\n105\n1.777%\n0.915\n-480.356\n\n\n\nNous en déduisons que le prix de l’obligation est de \\(P(t)=115,47 \\$\\) et l’exposition totale est de 1 154 706 $. En utilisant la période historique de l’année 2014, nous estimons la matrice de covariance entre les variations quotidiennes des cinq taux d’intérêt à coupon zéro sachant que l’écart-type est respectivement de 0,746 pb pour \\(\\Delta_h R_t(t + 1)\\), 2,170 pb pour \\(\\Delta_h R_t(t + 2)\\), 3,264 pb pour \\(\\Delta_h R_t(t + 3)\\), 3,901 pb pour \\(\\Delta_h R_t(t + 4)\\) et 4,155 pb pour \\(\\Delta_h R_t(t + 5)\\), où \\(h\\) correspond à un jour de bourse. Pour la matrice de corrélation en pb (points de base), nous obtenons :\n\\[\n\\rho =\n\\begin{pmatrix}\n100.000 &  \\\\\n87.205 & 100.000 \\\\\n79.809 & 97.845 & 100.000 \\\\\n75.584 & 95.270 & 98.895 & 100.000  \\\\\n71.944 & 92.110 & 96.556 & 99.219 & 100.000 \\\\\n\\end{pmatrix}\n\\]\nNous en déduisons que la valeur à risque à 99% est (en supposant que la moyenne du PnL est nulle): \\[\nVaR= 2.33 * \\sqrt{W_t^T \\Sigma W_t}\n\\] Nous obtenons une valeur à risque de 4970$ pour une période de détention d’un jour.\n\n2.4.1.1 Implémentation en R\n\n# Définition des écarts-types en points de base convertis en pourcentage\nstd_devs &lt;- c(0.746, 2.170, 3.264, 3.901, 4.155)/10000\n\n# Matrice de corrélation\ncorrelation_matrix &lt;- matrix(c(\n  100, 87.205, 79.809, 75.584, 71.944,\n  87.205, 100, 97.845, 95.270, 92.110,\n  79.809, 97.845, 100, 98.895, 96.556,\n  75.584, 95.270, 98.895, 100, 99.219,\n  71.944, 92.110, 96.556, 99.219, 100\n), nrow = 5, byrow = TRUE)/100\n\nW_tm &lt;- matrix(c(-4.978, -9.826, -14.437, -18.783, -480.356),nrow = 5)*10000\nmu&lt;-rep(0,5)\n\nVaR&lt;-calculate_VaR(mu,W_tm,std_devs,correlation_matrix,0.99)\nVaR\n\n         [,1]\n[1,] 4970.384"
  },
  {
    "objectID": "projets/LAL/lal.html",
    "href": "projets/LAL/lal.html",
    "title": "Projet statistique : Temps de survie ajusté sur la qualité de vie- Estimation de l’effet d’un traitement en vue d’une médecine personnalisée",
    "section": "",
    "text": "La leucémie aiguë lymphoblastique est le cancer le plus fréquent chez les enfants en représentant environ 75-80% des leucémies des enfants de moins de 15 ans aux US et en Europe, et 20% des leucémies des adultes (“Leucémie lymphoïde chronique (LLC) - Hématologie et oncologie,” n.d.). C’est un cancer dont l’origine a lieu dans les cellules souches du sang. Ces dernières se transforment pour se spécifier en une des trois types de cellules sanguines (les globules rouges, les globules blancs et les plaquettes). Cette différenciation a lieu dans la moelle osseuse. Or, lors d’une leucémie lymphoblastique, il y a une prolifération excessive de globules blancs ou de leurs précurseurs (cellules souches lymphoïques) qui se différencient de manière anormale en ne devenant pas matures, on les appelle les cellules blastiques. Elle est qualifiée d’aiguë lorsque la maladie débute de manière soudaine et se développe rapidement, en quelques semaines voire quelques jours.\nPour lutter contre ce cancer, dont le taux de survie pour les enfants vivant dans les pays développés est de 90%, il y a une nécessité de mettre en place des traitements intensifs. C’est le cas des chimiothérapies ou encore de la greffe de moelle osseuse, dont le but est de modifier les cellules souches des malades. Les greffes s’effectuent sous deux formes : la greffe autologue quand les cellules proviennent du malade et la greffe allogénique quand les cellules proviennent d’un donneur.\nPour mesurer l’effet du traitement sur l’évolution de celle-ci, le critère reconnu comme le plus important bénéfice est la survie globale des malades dans les essais randomisés. Toutefois, cet indicateur ne reflète pas pleinement la qualité de vie des patients. En hématologie, il s’agit le plus souvent de mesurer le risque de survenue d’une progression, ou d’un décès, tout en considérant également les évènements indésirables en lien avec la toxicité des traitements.\nNous proposons donc d’utiliser une mesure résumée du temps passé sans toxicité ni maladie (« time without symptoms and treatment toxicity », TwIST) comme critère de jugement d’un essai, possiblement pondérée par des coefficients d’utilité (« quality-adjusted TwIST », q-TwIST), (Solem et al. 2020).\nCette approche nous permettra ainsi d’examiner la possibilité de réduire l’intensité du traitement habituel des patients, traditionnellement associée à une forte toxicité, au profit d’un traitement plus ciblé et moins toxique, sans pour autant augmenter le risque de rechute."
  },
  {
    "objectID": "projets/LAL/lal.html#caractéristiques-des-patients",
    "href": "projets/LAL/lal.html#caractéristiques-des-patients",
    "title": "Projet statistique : Temps de survie ajusté sur la qualité de vie- Estimation de l’effet d’un traitement en vue d’une médecine personnalisée",
    "section": "Caractéristiques des patients",
    "text": "Caractéristiques des patients\nLes 155 patients inclus dans notre étude participaient également à l’essai précédent, parmi lesquels 76 ont bénéficié du traitement standard de nature intensif (bras A) et 79 ont reçu une thérapie allégée (bras B). La répartition de notre échantillon se compose de 48 % de femmes, avec un âge moyen de 46 ans, et de 52 % d’hommes, âgés en moyenne de 50 ans .\nL’inclusion des patients s’est échelonnée de 2016 à 2019, avec un suivi moyen durant 4 ans, jusqu’en 2023.\n\n\n\n\n\n\n\n\n\nDates d'enregistrement de l'étude\n\n\nVariables\nPremière date enregistrée\nDernière date enregistrée\n\n\n\n\nDate d'inclusion\n2016-03-08\n2019-02-01\n\n\nDate de pré-phase\n2016-03-01\n2019-01-29\n\n\nDate de chimiothérapie(Cycle 1)\n2016-03-11\n2019-02-05\n\n\nDate de fin d'étude\n2016-04-16\n2023-01-23\n\n\n\n\n\n\n\n\n[1] \"2019-05-16\"\n\n\nAvant l’initiation du traitement constitué 4 cycles de chimiothérapie suivies ou non d’une greffe, 85,6 % des patients ne montraient aucun signe de blastes leucémiques, alors que 14,4 % en avaient, ce dernier groupe étant caractérisé par une inhibition de la production de cellules sanguines normales.\nParmi les patients ayant des blastes dans le liquide cephalorachidien (LCR), 45 % présentaient également un taux de globules blancs inférieur à la normale (&lt; 5/ml), tandis que 55 % avaient un taux de globules blancs supérieur à la normale (&gt; 5/ml). En ce qui concerne l’indice de masse corporelle, 50 % des patients affichaient un IMC supérieur à 24,9, ce qui correspond à une situation de surpoids (Organisation mondiale de la Santé, n.d.)."
  },
  {
    "objectID": "projets/LAL/lal.html#diagnostic-des-patients-avant-le-traitement",
    "href": "projets/LAL/lal.html#diagnostic-des-patients-avant-le-traitement",
    "title": "Projet statistique : Temps de survie ajusté sur la qualité de vie- Estimation de l’effet d’un traitement en vue d’une médecine personnalisée",
    "section": "Diagnostic des patients avant le traitement",
    "text": "Diagnostic des patients avant le traitement\nAvant de subir des traitements liées à la lutte contre la leucémie, les patients ont été soumis à des examens cliniques et biologiques. Les résultats de ces examens sont résumés dans le tableau 2. Nous pouvons constater que dans les deux bras de l’essai, les patients présentaient des taux de globules blancs (GB) et de blastes circulants au diagnostic, respectivement de 20,5 Giga/L et 0,1 Giga/L. Les patients présentaient également des taux d’hémoglobine (Hb) de 10,5 g/dL, et un taux de blastes dans la moelle osseuse de 0,1 %. En ce qui concerne les atteintes de la rate, du foie et du médiastin, respectivement 10 %, 20 % et 30 % des patients présentaient des atteintes de ces organes.\nEn ce qui concerne les examens biologiques, nous constatons que parmi les individus pour lesquels la recherche de délétion1 du gène IKZF1 a été réalisée, une majorité présente une mutation de ce gène. Deux délétions majeures ont été observées :\n\nLa première est caractérisée par la perte des exons 4 à 7 ;\nLa deuxième est caractérisée par la perte des exons 2 à 7 ;\n\nCes délétions sont souvent associées à une grande incidence cumulative de rechute et à une survie globale réduite (Martinelli et al. 2009).\nDes mesures ont été relevées sur les patients avant le début du traitement, lors de la période dite de pré-phase, qui, pour la moitié des individus, a duré entre 7 et 8 jours. Les résultats de ces prélèvements se trouvent en annexe. Parmi les interventions subies par les patients, les ponctions lombaires ont été pratiquées sur 92 % d’entre eux, entraînant des traumatismes chez 38 patients (dont 29 avec une ponction lombaire traumatisante négative [TLP-] et 9 avec une ponction lombaire traumatisante positive [TLP+]). Par ailleurs, 17 patients ont bénéficié d’un diagnostic du système nerveux central, parmi lesquels seulement 4 présentaient des signes cliniques et 15 présentaient une atteinte biologique.\nNous avons également enregistré 7 cas d’atteinte tardive du système nerveux central, passant de CNS 2 à CNS 1, et un seul cas diagnostiqué CNS 2 devenu CNS 3 tardivement. De même, au premier jour de la pré-phase, 50 % des patients présentaient un taux de globules blancs situé entre 3 et 12 Giga/L, sans présence de blastes.\nEn outre, 93 patients ont montré une sensibilité aux corticoïdes."
  },
  {
    "objectID": "projets/LAL/lal.html#bilan-de-fin-de-traitement",
    "href": "projets/LAL/lal.html#bilan-de-fin-de-traitement",
    "title": "Projet statistique : Temps de survie ajusté sur la qualité de vie- Estimation de l’effet d’un traitement en vue d’une médecine personnalisée",
    "section": "Bilan de fin de traitement",
    "text": "Bilan de fin de traitement\nLors de l’examen des données, il est intéressant de noter que la moitié des patients ont étendu leur engagement dans l’étude au-delà de 46 mois, révélant un niveau de rétention significatif. De manière plus spécifique, l’analyse de la distribution de la durée de suivi des participants révèle la présence de deux pics dominants, ou modes, situés respectivement aux alentours de 18 mois et 46 mois. Cette observation suggère une tendance bimodale dans l’implication des participants, avec un groupe manifestant une adhésion à moyen terme autour de 18 mois et un autre démontrant un engagement à plus long terme, prolongeant leur participation jusqu’à 46 mois ou plus.\n\n\n\n\n\n\n\n\n\nNéanmoins, 105 patients ont interrompu leur participation de manière anticipée, répartis équitablement entre le bras A (52 patients) et le bras B (53 patients). Les motifs d’interruption prématurée sont variés, incluant le décès, la rechute, le retrait par consentement (CST), des évènements de ré-évoluation moléculaire, des effets secondaires sévères, entre autres raisons.\n\n\n\n\n\n\n\n\n\nParmi ces interruptions, la rechute représente la cause principale, concernant 41,3% des arrêts prématurés. Elle est suivie de près par le décès des patients(26.1%).\nAu total, 36 patients sont décédés. Parmi eux, 17 n’avaient pas rechuté (8 dans le bras A et 9 dans le bras B), 19 avaient rechuté. Les causes de décès sont diverses, la toxicité liée au traitement, les complications de la greffe, et la leucémie, entre autres. La cause principale des décès était la leucémie, représentant 36,1% des cas, suivie par les complications liées à la greffe, avec 25,0%.\nIl est également important de noter que 35 patients ont connu une rechute (11 dans le bras A et 24 dans le bras B). Par ailleurs, 133 patients ont reçu une greffe durant leur suivi, comprenant 93 greffes allogéniques (46 dans le bras A et 47 dans le bras B) et 40 greffes autologues (19 dans le bras A et 21 dans le bras B). Parmi ces patients greffés, 39 ont développé une maladie du greffon contre l’hôte (GVHD) aiguë (22 dans le bras A et 17 dans le bras B), et 11 ont souffert d’une GVHD chronique (6 dans le bras A et 5 dans le bras B), qui sont des complications graves de l’allogreffe de cellules souches (“Rejet de greffe” 2023)."
  },
  {
    "objectID": "projets/LAL/lal.html#estimation-de-la-moyenne-du-temps-de-survie-sans-maladie-rechute-ni-toxicité-twist",
    "href": "projets/LAL/lal.html#estimation-de-la-moyenne-du-temps-de-survie-sans-maladie-rechute-ni-toxicité-twist",
    "title": "Projet statistique : Temps de survie ajusté sur la qualité de vie- Estimation de l’effet d’un traitement en vue d’une médecine personnalisée",
    "section": "Estimation de la moyenne du temps de survie sans maladie (rechute) ni toxicité (TWiST)",
    "text": "Estimation de la moyenne du temps de survie sans maladie (rechute) ni toxicité (TWiST)\nNous avons décidé de nous intéresser à quatre évènements ponctuels qui sont susceptibles d’arriver au cours de notre étude. Il s’agit du début du traitement, de la fin du traitement, de la rechute et du décès. La période entre le début du traitement et sa fin correspond au nombre de jours des quatre cycles, tandis que celle entre la fin du traitement et la rechute correspond à l’estimation du temps écoulé avant l’apparition de nouveau des symptômes de la maladie. De plus, après le choix de ces quatre évènements, nous avons défini trois états d’intérêt :\n\nTOX qui correspond à la période de toxicité du traitement du à la chimiothérapie et/ou les greffes qui sont sont associées à des effets indésirables.\nTWiST :Time without symptoms of disease and toxicity, qui correspond à la période où les symptômes d’aggravation de la maladie n’existent plus. C’est la période de temps où le patient est en rémission et sans effets secondaires du traitement.\nREL : la période suivant la rechute jusqu’au décès du patient.\n\n\n\n\n\n\nNotre étude de survie s’appuiera sur le partitionnement des données en ces trois états spécifiques, ce qui nous permettra d’examiner le bénéfice d’un traitement par rapport à un autre en fonction des évènements ciblés. Faire varier ce partitionnement nous permettra, par ailleurs, d’enrichir nos analyses.\nNous prévoyons d’évaluer les durées moyennes de survie associées à chaque état, en utilisant des méthodes bien établies dans le domaine de l’analyse de survie, notamment la méthode de Kaplan-Meier (Kaplan and Meier 1958). Cette approche est reconnue pour sa capacité à fournir une estimation fiable de la fonction de survie, facilitant ainsi la comparaison des probabilités de survie entre différents groupes de traitement au cours du temps.\nDe plus, nous accorderons une attention particulière à la troncature pour avoir un temps moyen de survie restreint (RMST). Cette procédure est essentielle pour prévenir la surestimation des durées moyennes de survie dans chaque état, mais aussi pour mesurer les différences entre groupes de traitement(Han and Jung 2022).\nEn adoptant cette stratégie, nous nous assurons que notre analyse produit des estimations de survie précises et représentatives, améliorant ainsi la robustesse et la fiabilité de nos conclusions concernant l’impact des traitements étudiés. Le point temporel choisi pour obtenir une RMST afin de refléter l’horizon temporel cliniquement pertinent est de ??? ans."
  },
  {
    "objectID": "projets/LAL/lal.html#pondération-de-chaque-durée-par-un-coefficient-dutilité-q-twist",
    "href": "projets/LAL/lal.html#pondération-de-chaque-durée-par-un-coefficient-dutilité-q-twist",
    "title": "Projet statistique : Temps de survie ajusté sur la qualité de vie- Estimation de l’effet d’un traitement en vue d’une médecine personnalisée",
    "section": "Pondération de chaque durée par un coefficient « d’utilité » (Q-TWiST)",
    "text": "Pondération de chaque durée par un coefficient « d’utilité » (Q-TWiST)\nLe Q-TWiST permet de calculer le temps moyen passé dans chaque état en tenant compte de la qualité de vie du patient dans chacun de ces états. Le temps moyen est estimé pour chaque groupe de traitement grâce aux aires sous les courbes de survie. Une troncature est utilisée qui est égale à la médiane du temps de survie. La qualité de vie est estimée par une utilité associée à chaque état. En somme, le Q-TWiST peut s’écrire sous la forme suivante : \\[Q-TWiST = U\\_TOX \\times TOX + TWiST + U\\_REL \\times REL\\] Avec \\(U \\in [0, 1]\\), 0 = mauvais comme la mort et 1 le meilleur état possible, c’est le poids qu’on accorde aux différents états. Par définition, U_TWiST = 1 puisque le patient n’a plus de symptôme de la maladie. Les variables de périodes sont en unités de jours donc le Q-TWiST est calculé en jours.\nCette étude nous permettra de determiner pour quelle(s) combinaison(s) de coefficients le bénéfice du traitement est-il modifié.\nAttention : les estimations des utilités sont fondamentales car celles-ci influent sur les critères d’évaluation d’un traitement. Ainsi changer les utilités peut amener à changer de traitement pour un même traitement. Ce sont des estimations car nous ne possédons pas de table dans notre base de données avec les utilités de chaque patient, ni de leur ressenti Pour avoir une meilleure fiabilité des estimations des écarts-types, nous pouvons utiliser des méthodes de : bootstrapping non-paramétrique ou encore de ré-échantillonnage numérique."
  },
  {
    "objectID": "projets/LAL/lal.html#footnotes",
    "href": "projets/LAL/lal.html#footnotes",
    "title": "Projet statistique : Temps de survie ajusté sur la qualité de vie- Estimation de l’effet d’un traitement en vue d’une médecine personnalisée",
    "section": "Footnotes",
    "text": "Footnotes\n\n\nLa délétion est une mutation génétique caractérisée par la perte de matériel génétique sur un chromosome. (“Délétion” 2023)↩︎"
  },
  {
    "objectID": "projets/micro/micro_notes.html",
    "href": "projets/micro/micro_notes.html",
    "title": "Cours de microéconométrie",
    "section": "",
    "text": "Introduction\nLe document suivant est un résumé des cours de microéconométrie. Il regroupe toutes les méthodes enseignés au cours de Mr. Sun Yutec à l’Ensai pour la promotion 2024-2025 dont je fais parti.\n\n\n\n\n\n\n\n\n\n Back to top"
  },
  {
    "objectID": "projets/regression_lin/regression_lineaire.html",
    "href": "projets/regression_lin/regression_lineaire.html",
    "title": "La régression linéaire",
    "section": "",
    "text": "La régression linéaire est une méthode d’apprentissage supervisé qui vise à évaluer, lorsqu’il existe, la relation linéaire entre une variable d’intérêt et des variables explicatives."
  },
  {
    "objectID": "projets/regression_lin/regression_lineaire.html#hypothèses",
    "href": "projets/regression_lin/regression_lineaire.html#hypothèses",
    "title": "La régression linéaire",
    "section": "Hypothèses",
    "text": "Hypothèses\n\nRelation Linéaire\nL’hypothèse fondamentale de la régression linéaire est l’existence d’une relation linéaire entre la variable cible et les variables explicatives. Pour s’assurer de la pertinence de cette hypothèse avant de procéder à la régression linéaire, il est recommandé de :\n\nVisualisation des données : Utiliser un nuage de points pour chaque paire de variables explicatives et la variable cible. Cette visualisation aide à détecter la présence et la forme de relations potentielles, guidant ainsi la décision d’utiliser ou non un modèle linéaire.\nTests de corrélation :\n\nCorrélation de Pearson : Ce test mesure la corrélation linéaire entre deux variables, fournissant un coefficient dont la valeur absolue proche de 1 indique une forte corrélation linéaire.\nCorrélation de Spearman : Ce test de rang est utilisé pour évaluer la corrélation monotone entre deux variables, utile notamment lorsque les données ne respectent pas les conditions de normalité requises pour le test de Pearson.\n\n\n\n\nPropriétés des Erreurs\nPour que les estimations des paramètres du modèle linéaire soient fiables, tles erreurs du modèle, représentées par \\(\\xi_i\\), doivent répondre à plusieurs critères :\n\nErreurs centrées : La moyenne attendue des erreurs doit être nulle, soit \\(E[\\xi_i] = 0\\). Cela signifie que le modèle ne présente pas de biais systématique dans les prédictions.\nHomoscédasticité : La variance des erreurs doit être constante pour toutes les observations, exprimée par \\(V[\\xi_i] = \\sigma^2\\). Cette propriété garantit que la précision des estimations est uniforme à travers la gamme des valeurs prédites.\nDécorrélation des erreurs : Les erreurs doivent être mutuellement indépendantes, c’est-à-dire que la covariance entre toute paire d’erreurs est nulle, \\(Cov(\\xi_i, \\xi_j) = 0\\) pour \\(1\\leq i \\neq j \\leq n\\). Cette condition est essentielle pour éviter les biais dans les estimations des paramètres et pour que les tests statistiques sur les coefficients soient valides."
  },
  {
    "objectID": "projets/TS/ts1.html",
    "href": "projets/TS/ts1.html",
    "title": "TD - Séries temporelles 1",
    "section": "",
    "text": "Dans cette section, il s’agit d’une proposition de réponse au TD de Séries temporelles 1 dispensé par Mr José GÓMEZ GARCÍA à l’ENSAI en 2023-2024 au deuxième année.\nVous trouverez l’énoncé du td via le lien ci-joint ."
  },
  {
    "objectID": "projets/TS/ts1.html#exercice-1",
    "href": "projets/TS/ts1.html#exercice-1",
    "title": "TD - Séries temporelles 1",
    "section": "Exercice 1",
    "text": "Exercice 1\nSoit \\(\\epsilon_t\\) un bruit blanc. Pour chacun des processus suivants, dire s’il s’agit d’un processus ARMA stationnaire. Si oui, déterminer les ordres p et q et préciser si le processus admet une représentation AR(\\(\\infty\\)) et/ou MA(\\(\\infty\\)) et si la représentation ARMA est minimale.\n\n1.\nNous savons que : \\[\\begin{equation*}\n\\begin{aligned}\nX_t &= \\frac{5}{6} X_{t-1} - \\frac{1}{6} X_{t-2} + \\epsilon_t + \\epsilon_{t-1} \\\\\n&\\Leftrightarrow X_t - \\frac{5}{6} X_{t-1} + \\frac{1}{6} X_{t-2} = \\epsilon_t + \\epsilon_{t-1} \\\\\n&\\Leftrightarrow (I - \\frac{5}{6}B + \\frac{1}{6}B^2) X_t = (1 + B) \\epsilon_t \\\\\n&\\Leftrightarrow \\Phi(B)X_t = \\Psi(B) \\epsilon_t\n\\end{aligned}\n\\end{equation*}\\]\nDéterminons les racines de \\(\\Phi(B)\\) : \\[\\begin{equation*}\n\\begin{aligned}\n\\Phi(B) &= I-\\frac{5}{6}B + \\frac{1}{6}B^2  \\\\\n&= \\frac{1}{6}(B^2 - 5B + 6I) \\\\\n&= \\frac{1}{6} (B - 2I)(B - 3I)  \\\\\n&= (I - \\frac{1}{2}B)(I - \\frac{1}{3}B)\n\\end{aligned}\n\\end{equation*}\\]\nLes racines de \\(\\Phi\\) sont de modules 2 et 3, de modules supérieurs à 1 donc, \\(X_t\\) est un processus ARMA(2,1) stationnaire. De plus, il admet une représentation MA(\\(\\infty\\)).\n\n\\(\\Psi(B) = (I+B)\\) admet comme racine (évidente) 1. Donc \\(X_t\\) n’admet aucune représentation AR(\\(\\infty\\)).\n\nComme \\(\\Phi(B)\\) et \\(\\Psi(B)\\) n’admettent aucune racine commune, on ne peut réduire la représentation. le processus ARMA est donc minimale.\nD’accord, je vais tenter de transcrire les équations et commentaires de votre image dans un format similaire à celui que vous avez fourni précédemment.\n\n\n2.2\nNous pouvons voir que \\(X_t\\) suit une \\(AR(\\infty)\\): \\[\\begin{equation*}\n\\begin{aligned}\n\\sum_{k=0}^{\\infty} \\frac{1}{2^k} X_{t-k} &= \\epsilon_t \\\\  \n&\\Leftrightarrow (\\sum_{k=0}^{\\infty} (\\frac{B}{2})^k ) X_t = \\epsilon_t \\\\\n&\\Leftrightarrow \\frac{1}{I-\\frac{B}{2}}X_t = \\epsilon_t\\\\\n&\\Leftrightarrow X_t=(I-\\frac{B}{2})\\epsilon_t\n\\end{aligned}\n\\end{equation*}\\] Ainsi, \\(X_t\\) est un processus MA(1). De plus, la racine de \\((I-\\frac{B}{2})\\) est 2 \\(\\geq 1\\), donc ce processus ARMA(0,1) est stationnaire et admet uniquement une représentation AR(\\(\\infty\\)).\n\n\n2.3\n\\[\\begin{equation*}\n\\begin{aligned}\nX_t &= \\frac{5}{4} X_{t-1} - \\frac{1}{4} X_{t-2} + \\epsilon_t + 2 \\epsilon_{t-1} \\\\\n&\\Leftrightarrow X_t - \\frac{5}{4} X_{t-1} + \\frac{1}{4} X_{t-2} &= \\epsilon_t + 2 \\epsilon_{t-1} \\\\\n&\\Leftrightarrow (1 - \\frac{5}{4}B + \\frac{1}{4}B^2)X_t &= (1 + 2B) \\epsilon_t \\\\\n&\\Leftrightarrow (1 - B)(1-\\frac{1}{4}B)X_t &= (1 + 2B) \\epsilon_t \\\\\n&\\Leftrightarrow \\Phi(B)X_t = \\Psi(B) \\epsilon_t\n\\end{aligned}\n\\end{equation*}\\] \\(\\Phi(B)\\) admet pour racine de 1 et 4. Comme l’une des racines est de module égal à 1, \\(X_t\\) n’est pas stationnaire.\n\n\n2.4\n\\[\\begin{equation*}\n\\begin{aligned}\nX_t &= 2X_{t-1} - X_{t-2} + \\epsilon_t + \\frac{1}{4}\\epsilon_{t-1} \\\\\n&\\Leftrightarrow X_t - 2X_{t-1} + X_{t-2} &= \\epsilon_t + \\frac{1}{4}\\epsilon_{t-1} \\\\\n&\\Leftrightarrow (1 - 2B + B^2)X_t &= (1 + \\frac{B}{4}) \\epsilon_t \\\\\n&\\Leftrightarrow \\Phi(B)X_t = \\Psi(B) \\epsilon_t\n\\end{aligned}\n\\end{equation*}\\] \\(\\Phi(B)\\) admet comme racine (évidente) 1 donc \\(X_t\\) n’est pas stationnaire."
  },
  {
    "objectID": "projets/TS/ts1.html#exercice-2",
    "href": "projets/TS/ts1.html#exercice-2",
    "title": "TD - Séries temporelles 1",
    "section": "Exercice 2",
    "text": "Exercice 2\n\n2.1\n\\((X_t) \\sim ARIMA(2,1,1)\\) \\(\\Rightarrow Y_t \\sim ARMA(1,1)\\) avec \\(Y_t = (I-B)X_t\\).\n\\(Y_t \\sim ARMA(1,1) \\Leftrightarrow Y_t = c + \\phi_1 Y_{t-1} + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1}\\)\nOr :\n\\[\\begin{equation*}\n\\begin{aligned}\nY_t = X_t - X_{t-1} &= c + \\phi_1 Y_{t-1} + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1}\\\\\n&\\Leftrightarrow X_t - X_{t-1} = c + \\phi_1 (X_{t-1} - X_{t-2}) + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} \\\\\n&\\Leftrightarrow X_t - X_{t-1} = c + \\phi_1 X_{t-1} - \\phi_1 X_{t-2} + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} \\\\\n&\\Leftrightarrow X_t  = c + (1-\\phi_1) X_{t-1} - \\phi_1 X_{t-2} + \\varepsilon_t + \\theta_1 \\varepsilon_{t-1} \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\n\n2.2\n\\((X_t) \\sim ARI(2,1)\\) \\(\\Rightarrow Y_t \\sim AR(1)\\) avec \\(Y_t = (I-B)X_t\\).\n\\(Y_t \\sim AR(1) \\Leftrightarrow Y_t = c + \\phi_1 Y_{t-1} + \\phi_2 Y_{t-2}\\)\nOr :\n\\[\\begin{equation*}\n\\begin{aligned}\nY_t = X_t - X_{t-1} &= c +\\phi_1 Y_{t-1} + \\phi_2 Y_{t-2}\\\\\n&\\Leftrightarrow X_t - X_{t-1} = c + \\phi_1 (X_{t-1} - X_{t-2}) + \\phi_2 (X_{t-2} - X_{t-3}) \\\\\n&\\Leftrightarrow X_t = c + (\\phi_1+1) (X_{t-1} + (\\phi_2-\\phi_1)X_{t-2} - \\phi_2 X_{t-3}) \\\\\n\\end{aligned}\n\\end{equation*}\\]\n\n\n2.3\n\\((X_t) \\sim I(2)\\) \\(\\Rightarrow Y_t = (I-B)X_t\\) est stationnaire.\nSelon le théorème de Wold, on peut écrire donc:\n\\[\nY_t = c + \\sum_{j=0}^{\\infty} \\psi_j \\varepsilon_{t-j} + V_t\n\\]\nOù \\(V_t\\) est connu tel que \\(cov(\\varepsilon_t,V_s) = 0\\) pour \\(s \\neq t\\).\nAinsi, \\[\\begin{equation*}\n\\begin{aligned}\nY_t = X_t - X_{t-1} &= c + \\sum_{j=0}^{\\infty} \\psi_j \\varepsilon_{t-j} + V_t\\\\\n&\\Leftrightarrow X_t  = c + X_{t-1}+ \\sum_{j=0}^{\\infty} \\psi_j \\varepsilon_{t-j} + V_t\\\\\n\\end{aligned}\n\\end{equation*}\\]\n\n\n2.4\n\\((X_t)\\) est \\(SARIMA(0,2,1)*(1,1,0)_7\\)\nPosons:\n\\[ Y_t = (1-B^7)^1(1- B)^0X_t =(1-B^7)X_t=X_t - X_{t-7} \\]\nNous savons que cette équation est une ARMA de forme :\n\\[\n\\Phi_7^1(B^7)\\Phi^2(B)Y_t = \\Theta_7^0(B^7)\\Theta^1(B)\\varepsilon_t\n\\]\nOù \\(\\Phi\\) et \\(\\Theta\\) sont les polynômes AR et MA, respectivement.\nNous avons donc : \\[\n(1 - \\phi B^7)(1 -\\alpha_1 B - \\alpha_2 B^2)Y_t = 1*(1 - \\theta_1 B)\\varepsilon_t\n\\] \\[\n(1 - \\phi B^7)(1 -\\alpha_1 B - \\alpha_2 B^2)(X_t - X_{t-7} ) = 1*(1 - \\theta_1 B)\\varepsilon_t\n\\]\n\n\n2.5\n\\((X_t)\\) est \\(SARMA(2,0)*(1,2)_{12}\\)\nPosons:\n\\[ Y_t = (1-B^{12})^0(1- B)^0X_t =X_t \\]\nNous savons que cette équation est une ARMA de forme :\n\\[\n\\Phi_{12}^1(B^{12})\\Phi^2(B)Y_t = \\Theta_{12}^2(B^7)\\Theta^0(B)\\varepsilon_t\n\\]\nOù \\(\\Phi\\) et \\(\\Theta\\) sont les polynômes AR et MA, respectivement.\nNous avons donc : \\[\n(1 - \\phi B^{12})(1 -\\alpha_1 B - \\alpha_2 B^2)Y_t = (1 - \\theta_1 B^{12}-\\theta_2 B^{24})\\varepsilon_t\n\\] \\[\n(1 - \\phi B^{12})(1 -\\alpha_1 B - \\alpha_2 B^2)X_t = (1 - \\theta_1 B^{12}-\\theta_2 B^{24})\\varepsilon_t\n\\]"
  },
  {
    "objectID": "projets/TS/ts1.html#exercice-3",
    "href": "projets/TS/ts1.html#exercice-3",
    "title": "TD - Séries temporelles 1",
    "section": "Exercice 3",
    "text": "Exercice 3\n\n3.1\nSoit \\(M = (I - B)(I - B^2)\\) Ainsi, \\[M(a + bt + ct^2) = (I - B)(I - B^2)(a + bt + ct^2) = (I - B-B^2+B^3)(a + bt + ct^2) \\]\nAppliquons M à chaque composantes de \\(a + bt + ct^2\\): 1. M appliqué à \\(a\\):\n\\[M(a)=a-a-a+a=0\\]\n\nM appliqué à \\(bt\\)\n\n\\[ M(bt) = bt - b(t-1) - b(t-2) + b(t-3)\\] \\[ M(bt) = bt - bt + b - bt + 2b + bt - 3b \\] \\[ M(bt) = 0\\]\n\nM appliqué à \\(ct^2\\) \\[ M(ct^2) = ct^2 - c(t-1)^2 - c(t-2)^2 + c(t-3)^2\\] \\[ M(bt) =  ct^2 - c(t^2 - 2t + 1) - c(t^2 - 4t + 4) + c(t^2 - 6t + 9)  \\] \\[ M(bt) = -c-4c+9c  \\] \\[ M(bt) = 4c  \\]\n\nEn combinant les résultats de M appliqué à \\(a, bt\\) et \\(ct^2\\), nous trouvons :\n\\[ M(a + bt + ct^2) = 0 + 0 + 4c = 4c \\]\nCela confirme que \\(M\\) transforme le polynôme de degré 2 en une constante \\(4c\\).\n\n\n3.2\n\\[ M(\\cos(\\pi t)) = \\cos(\\pi t) - \\cos(\\pi(t-2)) - \\cos(\\pi(t-1)) +  \\cos(\\pi(t-3))  \\] \\[ M(\\cos(\\pi t)) = \\cos(\\pi t) - \\cos(\\pi-2\\pi) - \\cos(\\pi t- \\pi)) +  \\cos(\\pi t-3\\pi)  \\] \\[ M(\\cos(\\pi t)) = \\cos(\\pi t) - \\cos(\\pi t) +  \\cos(\\pi t)) -  \\cos(\\pi )  \\] \\[ M(\\cos(\\pi t)) = 0 \\]\nDOnc M absorbe les saisonnalités \\(cos(\\pi t)\\)\n\n\n3.3\n\\[ E(X_t)=E(a+bt+ct^2+cos(\\pi t) + \\varepsilon_t)\\] \\[ E(X_t)=a+bt+ct^2+cos(\\pi t) + E(\\varepsilon_t))\\] \\[ E(X_t)=a+bt+ct^2+cos(\\pi t) \\] car \\(\\varepsilon_t\\) est un bruit blanc.\nIci, l’espérance dépend du temps donc \\(X_t\\) n’est pas stationnaire\n\n\n3.4\nSoit \\(Y_t=M(X_t)\\) On a donc : \\[Y_t=M(a+bt+ct^2)+M(cos(\\pi t))+M(\\varepsilon_t)\\] \\[Y_t=4c+0+\\varepsilon_t-\\varepsilon_{t-1}-\\varepsilon_{t-2}+\\varepsilon_{t-3}\\] \\[Y_t=4c-\\sum_{i=0}^{3} \\theta_i \\varepsilon_{t-i})\\], avec \\(\\theta_0=-1\\), \\(\\theta_1=1\\),\\(\\theta_2=1\\) et \\(\\theta_3=-1\\)\nDonc \\(Y_t\\) est bien un processus MA(3).\n\n\ni\n\n\\[E(Y_t)=4c\\]\nComme \\(Y_t\\) est un processus MA(3), sa fonction d’autocovariance s’écrit : \\[Cov(Y_t,Y_{t+h}=\\sigma^2(\\sum_{j=0}^{3-h} \\theta_j \\theta_{j+h})\\]\n\nAinsi, \\[\\gamma(h)=\n\\begin{cases}\n\\sigma^2 & \\text{si } h = 0 \\\\\n\\sigma^2(-1+1-1) & \\text{si } |h| = 1 \\\\\n\\sigma^2(-1-1) & \\text{si } |h| = 2 \\\\\n\\sigma^2(1) & \\text{si } |h| = 3 \\\\\n0& \\text{si } |h| &gt;3\n\\end{cases}\n\\]\n\\[\\gamma(h)=\n\\begin{cases}\n\\sigma^2 & \\text{si } h = 0 \\\\\n-\\sigma^2 & \\text{si } |h| = 1 \\\\\n-2\\sigma^2 & \\text{si } |h| = 2 \\\\\n\\sigma^2 & \\text{si } |h| = 3 \\\\\n0& \\text{si } |h| &gt;3\n\\end{cases}\n\\]\n\n\nii\n\\(Y_t=(I-B)(I-B^2)X_t\\) avec \\(Y_t \\sim MA(3)\\)\nAinsi, on identifie facilement un processus SARIMA : \\[Y_t=(I-B^{s=2})^{D=1}(I-B)^{d=1}X_t \\sim SARIMA(0,1,3)*(0,1,0)_2\\]"
  },
  {
    "objectID": "projets/TS/ts1.html#exercice-4",
    "href": "projets/TS/ts1.html#exercice-4",
    "title": "TD - Séries temporelles 1",
    "section": "Exercice 4",
    "text": "Exercice 4\n\n4.1\nNous avons que : \\[\\begin{equation*}\n\\begin{aligned}\nX_t &= \\frac{1}{2} X_{t-1} + \\varepsilon_t - \\frac{1}{4}\\varepsilon_{t-1} \\\\\n&\\Leftrightarrow X_t - \\frac{1}{2} X_{t-1} = \\varepsilon_t - \\frac{1}{4}\\varepsilon_{t-1} \\\\\n&\\Leftrightarrow (I - \\frac{1}{2}B ) X_t = (1 - \\frac{1}{4} B) \\epsilon_t \\\\\n&\\Leftrightarrow \\Phi(B)X_t = \\Psi(B) \\epsilon_t\n\\end{aligned}\n\\end{equation*}\\]\nLa racine de \\(\\Phi\\) est 2 et la racine de \\(\\Psi\\) est 4. Les deux polynômes n’admettent aucune racine commune, donc le processus est minimale.\n\n\n4.2\nDéterminons d’abord l’inverse de \\(\\Phi(B)=(I - \\frac{1}{2}B )\\). \\(\\Phi(B)\\) est inversible car c’est un polynôme de racine de module 2 &gt; 1. Ainsi, \\(\\Phi(B)^{-1}=\\sum_{i=0}^{\\infty} (\\frac{B}{2})^i\\). De fait, nous avons : \\[X_t=\\Phi(B)^{-1}(\\Psi(B)\\varepsilon_t)\\] \\[X_t=\\sum_{i=0}^{\\infty} (\\frac{B}{2})^i( \\varepsilon_t - \\frac{1}{4}\\varepsilon_{t-1})\\] \\[X_t=\\sum_{i=0}^{\\infty} (\\frac{1}{2})^i( \\varepsilon_{t-i} - \\frac{1}{4}\\varepsilon_{t-1-i})\\] \\[X_t=\\sum_{i=0}^{\\infty} \\frac{1}{2^i} \\varepsilon_{t-i} - \\sum_{i=0}^{\\infty} \\frac{1}{2^{i+2}}\\varepsilon_{t-1-i})\\] À l’aide d’un changement de variable j=j-1, on a : \\[X_t=\\sum_{i=0}^{\\infty} \\frac{1}{2^i} \\varepsilon_{t-i} - \\sum_{i=1}^{\\infty} \\frac{1}{2^{i+1}}\\varepsilon_{t-i})\\] \\[X_t=\\varepsilon_t + \\sum_{i=1}^{\\infty} \\frac{1}{2^i} \\varepsilon_{t-i} - \\sum_{i=1}^{\\infty} \\frac{1}{2^{i+1}}\\varepsilon_{t-i})\\] \\[X_t=\\varepsilon_t + \\sum_{i=1}^{\\infty} \\frac{1}{2^{i+1}}\\varepsilon_{t-i})\\]\n\n\n4.3\n\\[Cov(X_t,X_{t+h})=Cov(\\varepsilon_t + \\sum_{j=1}^{\\infty} \\frac{1}{2^{j+1}}\\varepsilon_{t-j}\\ , \\ \\varepsilon_{t+h} + \\sum_{i=1}^{\\infty} \\frac{1}{2^{i+1}}\\varepsilon_{t-i+h})\\] \\[Cov(X_t,X_{t+h})=Cov(\\varepsilon_t,\\varepsilon_{t+h})+\\sum_{i=1}^{\\infty} \\frac{1}{2^{i+1}}Cov(\\varepsilon_t,\\varepsilon_{t+h-i})+\\sum_{j=1}^{\\infty} \\frac{1}{2^{j+1}}Cov(\\varepsilon_{t-j},\\varepsilon_{t+h})+\\sum_{j=1}^{\\infty}\\sum_{i=1}^{\\infty} \\frac{1}{2^{j+1}}\\frac{1}{2^{i+1}}Cov(\\varepsilon_{t-j},\\varepsilon_{t+h-i})\\] \\[Cov(X_t,X_{t+h})=Cov(\\varepsilon_t,\\varepsilon_{t+h})+\\sum_{i=1}^{\\infty} \\frac{1}{2^{i+1}}Cov(\\varepsilon_t,\\varepsilon_{t+h-i})+\\sum_{j=1}^{\\infty} \\frac{1}{2^{j+1}}Cov(\\varepsilon_{t-j},\\varepsilon_{t+h})+\\sum_{j=1}^{\\infty}\\sum_{i=1}^{\\infty} \\frac{1}{2^{i+j+1}}Cov(\\varepsilon_{t-j},\\varepsilon_{t+h-i})\\]\n\nPour h=0, l’équation devient : \\[Cov(X_t,X_t)=Cov(\\varepsilon_t,\\varepsilon_t)+\\sum_{j=1}^{\\infty}\\sum_{i=1}^{\\infty} \\frac{1}{2^{i+j+1}}Cov(\\varepsilon_{t-j},\\varepsilon_{t-i})\\]\n\nLorsque i=j, l’équation devient donc : \\[\\begin{equation*}\n\\begin{aligned}\nCov(X_t,X_t)&=Cov(\\varepsilon_t,\\varepsilon_t)+\\sum_{i=1}^{\\infty} \\frac{1}{2^{2i+1}}Cov(\\varepsilon_{t-i},\\varepsilon_{t-i})\\\\\n&=\\sigma^2+\\sigma^2\\sum_{i=1}^{\\infty} \\frac{1}{2^{2i+1}}\\\\\n&=\\sigma^2+\\frac{\\sigma^2}{4}\\sum_{i=1}^{\\infty} (\\frac{1}{4})^2\\\\\n&=\\sigma^2+\\frac{\\sigma^2}{4}\\times \\frac{1}{3}\\\\\n&=\\frac{13\\sigma^2}{12}\n\\end{aligned}\n\\end{equation*}\\]\n\nPour h&gt;0, l’équation devient : \\[Cov(X_t,X_t+h)=\\sum_{i=1}^{\\infty} \\frac{1}{2^{i+1}}Cov(\\varepsilon_t,\\varepsilon_{t+h-i})+\\sum_{j=1}^{\\infty} \\frac{1}{2^{j+1}}Cov(\\varepsilon_{t-j},\\varepsilon_{t+h})+\\sum_{j=1}^{\\infty}\\sum_{i=1}^{\\infty} \\frac{1}{2^{i+j+1}}Cov(\\varepsilon_{t-j},\\varepsilon_{t+h-i})\\]\n\n\\[\\begin{equation*}\n\\begin{aligned}\nCov(X_t,X_t+h)&=\\frac{\\sigma^2}{2^{h+1}}+0+\\frac{\\sigma^2}{2^{h+2}} \\times \\frac{1}{3}\\\\\n&=\\frac{\\sigma^2}{2^{h+1}}+0+\\frac{\\sigma^2}{2^{h+2}} \\times \\frac{1}{3}\\\\\n&=\\frac{\\sigma^2}{2^{h+1}}+0+\\frac{\\sigma^2}{2^{h+2}} \\times \\frac{1}{3}\\\\\n\\end{aligned}\n\\end{equation*}\\]\nAinsi donc : \\[\\gamma(h)=\n\\begin{cases}\n\\frac{13\\sigma^2}{12} & \\text{si } h = 0 \\\\\n\\frac{7 \\sigma^2}{2^{h+2} \\times 3}& \\text{si } |h| &gt;0 \\\\\n\\end{cases}\n\\]\n\\[\\Rightarrow \\rho(h)=\n\\begin{cases}\n\\frac{13\\sigma^2}{12} & \\text{si } h = 0 \\\\\n\\frac{7 \\sigma^2}{2^{h+2} \\times 3}& \\text{si } |h| &gt;0 \\\\\n\\end{cases}\n\\]\n\n\n4.4\nPosons \\(F=Vect(X_{t-1}, X_{t-2}, \\dots)\\). \\((\\varepsilon_t)_t\\) est le processus d’innovation de \\(X_t\\), car \\(X_t\\) est de représentation minimale. Ainsi, \\(\\varepsilon_t=X_t-P_F(X_t) \\Leftrightarrow P_F(X_t) = X_t - \\varepsilon_t\\) De fait, \\[\\begin{equation*}\n\\begin{aligned}\nP_F(X_t)&= \\frac{1}{2}X_{t-1}+ \\varepsilon_t - \\frac{1}{4}\\varepsilon_{t-1}-\\varepsilon_t \\\\\n&= \\frac{1}{2}X_{t-1}- \\frac{1}{4}\\varepsilon_{t-1} \\\\\n\\text{Or   } &\\varepsilon_t=\\sum_{j=0}^{\\infty}(\\frac{1}{4})^j(X_t-\\frac{1}{2}X_{t-1}) \\Rightarrow \\varepsilon_t=X_t - \\sum_{j=1}^{\\infty}\\frac{1}{4^j}X_{t-j}\\\\\n\\text{Donc   } &P_F(X_t)= \\frac{1}{2}X_{t-1}- \\frac{1}{4}(X_{t-1}-\\sum_{j=1}^{\\infty}\\frac{1}{4^j}X_{t-1-j})\n\\end{aligned}\n\\end{equation*}\\]"
  },
  {
    "objectID": "projets/TS/ts1.html#exercice-5",
    "href": "projets/TS/ts1.html#exercice-5",
    "title": "TD - Séries temporelles 1",
    "section": "Exercice 5",
    "text": "Exercice 5\n\n5.1\n\\[\\begin{equation*}\n\\begin{aligned}\n\\gamma_\\omega(h) &= Cov(\\omega_t,\\omega_{t+h}) \\\\\n&= Cov(v_t+\\eta_t-2\\eta_{t-1}, v_{t+h}+\\eta_{t+h}-2\\eta_{t+h-1}) \\\\\n&= Cov(v_t, v_{t+h})+Cov(\\eta_{t},\\eta_{t+h})-2Cov(\\eta_{t},\\eta_{t+h-1})-2Cov(\\eta_{t-1}+2\\eta_{t+h})+4Cov(\\eta_{t-1},\\eta_{t+h-1})\n\\end{aligned}\n\\end{equation*}\\]\n\\[\\begin{equation}\n\\begin{aligned}\n\\text{Donc   } &\\gamma_\\omega(h)=\n\\begin{cases}\n\\frac{10}{9} & \\text{si } h = 0 \\\\\n\\frac{-1}{3}& \\text{si } |h| =1 \\\\\n\\text{0 sinon}\n\\end{cases}\n\\end{aligned}\n\\end{equation}\\]\nComme \\(\\gamma_\\omega(h)=0 \\ \\forall |h|&gt;1\\) alors \\(\\omega_t\\) est un MA(1). Il peut donc s’écrire :\n\\(\\omega_t=\\varepsilon_t-\\phi_1\\varepsilon_{t-1}\\)\nLa fonction d’autocovariance d’un tel processus est : \\[\\begin{equation}\n\\begin{aligned}\n\\Rightarrow Cov(\\omega_t,\\omega_{t+h})=\n\\begin{cases}\n\\sigma^2 + \\phi_1^2\\sigma^2 & \\text{si } h = 0 \\\\\n-\\phi_1\\sigma^2 & \\text{si } |h| = 1 \\\\\n0\n\\end{cases}\n\\end{aligned}\n\\end{equation}\\]\nAinsi (1) et (2) nous permettent de determiner \\(\\rho_w(1)=\\frac{-\\phi_1\\sigma^2 }{\\sigma^2 + \\phi_1^2\\sigma^2 } \\Rightarrow (\\phi_1-\\frac{1}{3})(\\phi_1-3)=0\\) . Donc, \\(\\omega_t\\) admet une représentation MA inversible pour \\(\\phi_1=\\frac{1}{3}&lt;1\\).\n\n\n5.2\n\\[\\begin{equation*}\n\\begin{aligned}\nX_t&=Y_t+\\eta_t\\\\\n&=Y_t + \\eta_t - 2BY_t + 2BY_t\\\\\n&= (I-2B)Y_t+\\eta_t+2BY_t\\\\\n&= v_t+ \\eta_t + 2BY_t\\\\\n&= v_t + \\eta_t + 2 (X_{t-1}-\\eta_{t-1})\\\\\n&=\\omega_t+2X_{t-1}\\\\\n\\text{Donc  }&\\omega_t=X_t-2X_{t-1} \\sim ARMA(1,1)\n\\end{aligned}\n\\end{equation*}\\]\n\\((I-2B)X_t=(I-\\frac{1}{3}B)\\varepsilon_t\\) est minimale car (I-2B) et \\((I-\\frac{1}{3}B)\\) n’ont pas de racine commune et \\(\\varepsilon_t\\) est son rocessus d’innovation de variance \\(\\sigma^2=1\\).\n\n\n5.3\nOn sait que \\((I-2B)X_t=(I-\\frac{1}{3}B)\\varepsilon_t\\), cela implique : \\[\\begin{equation*}\n\\begin{aligned}\n\\varepsilon_t&=(\\sum_{i=0}^{\\infty} \\frac{B}{3}^i)(I-2B)X_t\\\\\n&=(\\sum_{i=0}^{\\infty} \\frac{B}{3}^i)(X_t-2X_{t-1})\\\\\n&=(\\sum_{i=0}^{\\infty} \\frac{1}{3}^i)X_{t-1} -2X_{t-1-I}) \\sim AR(\\infty)\n\\end{aligned}\n\\end{equation*}\\]"
  },
  {
    "objectID": "projets/TS/ts1.html#exercice-6",
    "href": "projets/TS/ts1.html#exercice-6",
    "title": "TD - Séries temporelles 1",
    "section": "Exercice 6",
    "text": "Exercice 6\n\n6.1\n\\(\\hat{X}_{t+1}=\\alpha X_t + (1-\\alpha)X_t\\) Montrons par récurrence que \\(\\hat{X}_{t+1}=\\sum_{j=0}^{t-1}\\alpha(1-\\alpha)^jX_{t-j}\\)\n Pour t=1, nous devons vérifier que:\n\\[ \\hat{X}_2 = \\alpha X_1 + (1-\\alpha) \\hat{X}_1 \\]\nÀ \\(t=1\\), nous n’avons pas de terme antérieur, donc \\(\\hat{X}_1 = X_1\\), et notre formule de prédiction devient:\n\\[ \\hat{X}_2 = \\alpha X_1 + (1-\\alpha) X_1 = X_1 \\]\nCela montre que pour \\(t=1\\), la formule de prédiction se réduit simplement à \\(\\hat{X}_2 = X_1\\), qui est équivalent à une somme qui n’a qu’un seul terme :\n\\[ \\hat{X}_2 = \\sum_{j=0}^{0}\\alpha(1-\\alpha)^jX_{1-j} = \\alpha(1-\\alpha)^0X_1 = X_1 \\]\nC’est notre équation de base pour \\(t=1\\), et c’est vrai par définition.\n Supposons que la formule soit vraie pour un certain \\(t\\), c’est-à-dire que:\n\\(\\hat{X}_{t+1} = \\sum_{j=0}^{t-1}\\alpha(1-\\alpha)^jX_{t-j}\\)\nIl faut maintenant prouver que la formule est vraie pour \\(t+1\\), c’est-à-dire que:\n\\(\\hat{X}_{t+2} = \\sum_{j=0}^{t}\\alpha(1-\\alpha)^jX_{t-j+1}\\)\nEn remplaçant \\(\\hat{X}_{t+1}\\) par l’expression de l’hypothèse de récurrence, on obtient: \\[\\begin{equation*}\n\\begin{aligned}\n\\hat{X}_{t+2} &= \\alpha X_{t+1} + (1-\\alpha) \\sum_{j=0}^{t-1}\\alpha(1-\\alpha)^jX_{t-j}\\\\\n&= \\alpha X_{t+1} + \\sum_{j=0}^{t-1}\\alpha(1-\\alpha)^{j+1}X_{t-j}\\\\\n&= \\alpha X_{t+1}  + \\sum_{j=1}^{t}\\alpha(1-\\alpha)^{j}X_{t-j+1}\\\\\n&= \\sum_{j=0}^{t}\\alpha(1-\\alpha)^j X_{t+1-j}\n\\end{aligned}\n\\end{equation*}\\]\n\nNous avons prouvé par récurrence que \\(\\hat{X}_{t+1}=\\sum_{j=0}^{\\infty}\\alpha(1-\\alpha)^jX_{t-j}\\).\n\n\n6.2\n\nLorsque \\(\\alpha \\approx 0\\), nous avons \\(\\hat{X}_{t+1}\\) qui dépend de \\(\\hat{X}_t\\), or \\(\\hat{X}_{t}=\\sum_{j=0}^{t-2}\\alpha(1-\\alpha)^jX_{t-j}=\\dots=\\alpha X_{t-1}\\). Ainsi donc, \\(\\alpha \\approx 0\\) \\(\\hat{X}_{t+1}\\) dépend d’un passé lointain.\nLorsque \\(\\alpha \\approx 1\\), nous avons \\(\\hat{X}_{t+1}\\) qui dépend de \\(X_t\\), donc d’un passé plus proche.\n\n\n\n6.3\n\\[\\begin{equation*}\n\\begin{aligned}\n\\hat{X}_{t+1}&=\\alpha X_t+(1-\\alpha)\\hat{X}_t\n&=\\alpha X_t + \\hat{X}_t + \\alpha \\hat{X}_t\n&= \\hat{X}_t + \\alpha(X_t-\\hat{X}_t)\n\\end{aligned}\n\\end{equation*}\\]\n\n\n6.4\nPosons\\(f(a) = \\sum_{j=0}^{t-1} (1 - \\alpha)^j (X_{t-j} - a)^2\\) Ainsi donc :\n\\[\\begin{equation*}\n\\begin{aligned}\nf'(a) &= -2 \\sum_{j=0}^{t-1} (1 - \\alpha)^j (X_{t-j} - a) = 0 \\\\\n&\\Leftrightarrow \\sum_{j=0}^{t-1} (1 - \\alpha)^j X_{t-j} = a \\sum_{j=0}^{t-1} (1 - \\alpha)^j \\\\\n&\\Leftrightarrow a = \\frac{\\sum_{j=0}^{t-1} (1 - \\alpha)^j X_{t-j}}{\\sum_{j=0}^{t-1} (1 - \\alpha)^j}\n\\end{aligned}\n\\end{equation*}\\]\nÀ mesure que \\(t\\) devient grand, la série géométrique converge et l’expression ci-dessus pour \\(a\\) se simplifie en \\(\\hat{X}_{t+1}\\).\nDe plus, \\(f''(a)=2 \\sum_{j=0}^{t-1} (1 - \\alpha)^j &gt;0\\) donc \\(\\hat{X}_{t+1} \\approx a\\).\n\n\n6.5\nPour estimer \\(\\alpha\\), on peut utiliser la méthode des moindres carrés.\n\n\n6.6\nNous pouvons écrire la fonction de coût \\(H_t(l, b)\\) comme suit : \\[ H_t(l, b) = \\sum_{j=0}^{t-1} (1 - \\alpha)^j (X_{t-j} - (l - bj))^2 \\]\nPour minimiser cette fonction par rapport à \\(l\\) et \\(b\\), nous dérivons \\(H_t(l, b)\\) par rapport à \\(l\\) et \\(b\\) et égalons les dérivées à zéro pour trouver les points critiques.\nEn différentiant \\(H_t(l, b)\\) par rapport à \\(l\\) et \\(b\\) et en égalant à zéro, nous obtenons un système de deux équations (les conditions du premier ordre pour un minimum) :\n\\[ \\frac{\\partial H_t(l, b)}{\\partial l} = -2\\sum_{j=0}^{t-1} (1 - \\alpha)^j (X_{t-j} - (l - bj)) = 0 \\] \\[ \\frac{\\partial H_t(l, b)}{\\partial b} = -2\\sum_{j=0}^{t-1} j(1 - \\alpha)^j (X_{t-j} - (l - bj)) = 0 \\]\nLe minimum se réalisant où les dérivées s’annulent (fonction convexe) et en remarquant que \\[\n\\sum_{i=0}^{\\infty}(1-\\alpha)^i = \\frac{1}{\\alpha},\n\\] \\[\n\\sum_{i=0}^{\\infty} i(1 - \\alpha)^i = \\frac{1-\\alpha}{\\alpha^2},\n\\] \\[\n\\sum_{i=0}^{\\infty} i^2(1 - \\alpha)^i = \\frac{(1-\\alpha)(2-\\alpha)}{\\alpha^3},\n\\] on a: \\[\n\\left\\{\n\\begin{array}{l}\n\\sum_{i=0}^{\\infty}(1 - \\alpha)^i y_{t-i} - \\frac{l}{\\alpha} + b\\frac{(1-\\alpha)}{\\alpha^2} =0 \\\\\n\\sum_{i=0}^{\\infty} i(1 - \\alpha)^i y_{t-i} - l\\frac{1-\\alpha}{\\alpha^2} + b \\frac{(1-\\alpha)(2-\\alpha)}{\\alpha^3} =0\n\\end{array}\n\\right.\n\\] Soit \\[\n\\left\\{\n\\begin{array}{l}\n\\alpha \\sum_{i=0}^{\\infty}(1 - \\alpha)^i y_{t-i} - l + b\\frac{(1-\\alpha)}{\\alpha} =0 \\\\\n\\alpha^2 \\sum_{i=0}^{\\infty} i(1 - \\alpha)^i y_{t-i} - l(1 - \\alpha) + b \\frac{(1-\\alpha)(2-\\alpha)}{\\alpha} =0\n\\end{array}\n\\right.\n\\] On note: \\(L_1(t) = \\alpha \\sum_{i=0}^{\\infty}(1 - \\alpha)^i y_{t-i}\\) et \\(L_2(t) = \\alpha \\sum_{i=0}^{\\infty}(1 - \\alpha)^i L_1(t - i)\\) Remarquons que \\(L_2(t) - \\alpha L_1(t) = \\alpha^2 \\sum_{i=1}^{\\infty} i(1 - \\alpha)^i y_{t-i}\\) car: \\[\nL_2(t) - \\alpha L_1(t) = \\alpha \\sum_{i=1}^{\\infty}(1 - \\alpha)^i L_1(t - i)\n\\] \\[\n= \\alpha^2 \\sum_{i=1}^{\\infty}\\sum_{j=0}^{\\infty}(1 - \\alpha)^{i+j} y_{t-(i+j)}\n\\] \\[\n= \\alpha^2 \\sum_{i=1}^{\\infty}\\sum_{k=i}^{\\infty}(1 - \\alpha)^k y_{t-k} = \\alpha^2 \\sum_{i=1}^{\\infty} i(1 - \\alpha)^i y_{t-i}\n\\] Alors: \\[\n\\left\\{\n\\begin{array}{l}\nL_1(t) - l + b\\frac{(1-\\alpha)}{\\alpha} =0 \\\\\nL_2(t) - \\alpha L_1(t) - l(1 - \\alpha) + b \\frac{(1-\\alpha)(2-\\alpha)}{\\alpha} =0\n\\end{array}\n\\right.\n\\] Et \\[\n\\left\\{\n\\begin{array}{l}\nL_1(t)= l - b\\frac{(1-\\alpha)}{\\alpha} \\\\\nL_2(t)=l - b\\frac{2(1-\\alpha)}{\\alpha}\n\\end{array}\n\\right.\n\\] Ainsi \\[\n\\left\\{\n\\begin{array}{l}\nl = 2L_1(t) - L_2(t) \\\\\nb= \\frac{\\alpha}{1-\\alpha} (L_1(t) - L_2(t))\n\\end{array}\n\\right.\n\\] Nous pouvons ensuite en déduire les formules de récurrences. \\[\nL_1(t) = \\alpha y_t + (1 - \\alpha)L_1(t - 1)\n\\] \\[\n\\left\\{\n\\begin{array}{l}\nL_2(t)=\\alpha L_1(t) + (1 - \\alpha)L_2(t - 1) \\\\\n=\\alpha^2 y_t + \\alpha(1 - \\alpha)L_1(t - 1) + (1 - \\alpha)L_2(t - 1)\n\\end{array}\n\\right.\n\\] Ainsi \\[\nl = (1 - (1 - \\alpha)^2)y_t + (1 - \\alpha)(2 - \\alpha)L_1(t - 1) - (1 - \\alpha)L_2(t - 1)\n\\] En incorporant \\(L_1(t) = l_{t - 1} - \\frac{1-\\alpha}{\\alpha} b_t\\) et \\(L_2(t) = l_{t} - b_t \\frac{2(1-\\alpha)}{\\alpha}\\), on en déduit après simplification: \\[\nl_t = (1 - (1 - \\alpha)^2)y_t + (1 - \\alpha)^2(l_{t-1} + b_{t-1})\n\\] De même, \\[\nb_t = \\alpha^2 y_t + \\alpha(1 - \\alpha)L_1(t - 1) - \\alpha L_2(t - 1)\n\\] \\[\nb_t = \\alpha^2 y_t - \\alpha^2 l_{t-1} + (1 - \\alpha)^2 b_{t-1}\n\\]\n\\[\n\\begin{cases}\nl_t=l_{t-1}+b_{t-1}+(1-(1-\\alpha)^2)(X_t-\\hat{X}_t)\\\\\nb_t=b_{t-1}+\\alpha^2 (X_t-\\hat{X}_t)\n\\end{cases}\n\\] avec comme initialisation \\(l_0=X_1\\) et \\(b_0=X_2-X_1\\)."
  },
  {
    "objectID": "projets/np_stat/np_stat_notes.html",
    "href": "projets/np_stat/np_stat_notes.html",
    "title": "Notes de statistique non paramétrique",
    "section": "",
    "text": "Back to top"
  },
  {
    "objectID": "projets/app_supervise/Tp_Cheryl_Kouadio.html",
    "href": "projets/app_supervise/Tp_Cheryl_Kouadio.html",
    "title": "Tp noté - Apprentissage supervisé 2A",
    "section": "",
    "text": "set.seed(2023)\nlibrary(data.table)\nlibrary(ggplot2)\nlibrary(dplyr)\nlibrary(tidyr)\nlibrary(rpart)\nlibrary(rpart.plot)\nlibrary(caret)\nlibrary(ranger)\nlibrary(reticulate)"
  },
  {
    "objectID": "projets/app_supervise/Tp_Cheryl_Kouadio.html#analyse-descriptive-univariée-des-variables",
    "href": "projets/app_supervise/Tp_Cheryl_Kouadio.html#analyse-descriptive-univariée-des-variables",
    "title": "Tp noté - Apprentissage supervisé 2A",
    "section": "1. Analyse descriptive univariée des variables",
    "text": "1. Analyse descriptive univariée des variables\nAvant toute analyse descriptive de nos variables, il est important de vérifier que nos données ne contiennent aucune valeur manquante.\n\ncolSums(is.na(wine_quality))\n\n       fixed.acidity     volatile.acidity          citric.acid \n                   0                    0                    0 \n      residual.sugar            chlorides  free.sulfur.dioxide \n                   0                    0                    0 \ntotal.sulfur.dioxide              density                   pH \n                   0                    0                    0 \n           sulphates              alcohol              quality \n                   0                    0                    0 \n\n\nNous constatons que nos données ne contiennent aucune valeur manquante. Cette constatation est prometteuse car elle signifie que nos données sont complètes, ce qui est essentiel pour la construction d’un modèle de régression fiable.\n\n1.1 Qualité du vin (quality)\nLa variable quality constitue ici notre variable d’intérêt pour la régression à effectuer. Cette variable est discrète et prend des valeurs entières entre 3 (pour un vin de mauvaise qualité) et 8 (pour un vin de meilleur qualité).\nEn moyenne, les vins ont une note de \\(5,636\\). De fait, nous constatons qu’au moins 50% des vins ont une note supérieur à la moyenne.\n\nggplot(wine_quality,aes(x=quality))+\n  geom_bar()+\n  theme_bw()+\n  labs(title =\"Répartition de la qualité des vins\",x=\"qualité des vins\", y=\"nombre de vins\")+theme(aspect.ratio = 1.5)\n\n\n\n\n\n\n\nround(prop.table(table(wine_quality$quality)),2)\n\n\n   3    4    5    6    7    8 \n0.01 0.03 0.43 0.40 0.12 0.01 \n\n\nPar ailleurs,la majorité de nos vins obtient une note de 5 (43% des vins) ou 6 (40% des vins).\n\nNous pourrons considérer cette variable comme étant catégorielle étant donnée le faible nombre de valeurs qu’elle prend. Le problème posé ici peut être donc vu comme un problème de classification tout comme un problème de régression.\n\n\n\n1.2 Acidité fixe (fixed.acidity)\nL’acidité fixe dans un vin correspond à la quantité d’acides fixes présents naturellement dans le vin. L’ensemble des vins de notre jeu de données présentent une acidité fixe moyenne de \\(8,32\\). Cependant, il est intéressant de noter que la médiane de l’acidité fixe se situe à 7,90, ce qui indique qu’un peu plus de la moitié des vins sont plus acides que d’autres.\n\nggplot(wine_quality,aes(y=fixed.acidity))+\n  geom_boxplot()+\n  theme_bw()+\n  labs(title =\"Niveau d'acidité fixe des vins\")+\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\nDe plus, l’acidité fixe des vins est compris entre \\(4,60\\) à \\(15,90\\) avec une dispersion de \\(\\sigma^2=3.03\\). Cette variation signifie que certains vins affichent une acidité plus élevée que d’autres, ce qui contribue à une grande diversité au sein de l’échantillon.\n\n\n1.3 Acidité Volatile (volatile.acidity)\nL’acidité volatile dans un vin correspond à la quantité d’acides volatils qu’il contient, c’est à dire des acides qui sont capables de s’évaporer après la vinification.\nDans notre jeu de données, nous constatons que l’acidité volatile s’étend de 0,1200 à 1,5800, avec une moyenne d’environ 0.5278. Ainsi, certains vins présentent une acidité volatile plus élevée que d’autres.\n\nggplot(wine_quality,aes(y=volatile.acidity))+\n  geom_boxplot()+\n  theme_bw()+\n  labs(title =\"Niveau d'acidité volatile des vins\")+\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\n\n\n1.4 Acide Citrique (citric.acid)\nLa teneur en acide citrique varie de 0 à 1, avec une moyenne d’environ 0,271. Tout comme l’aciditité fixe, 50% des vins ont un niveau d’acide citrique inférieur à la moyenne.\n\nsummary(wine_quality$citric.acid)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.000   0.090   0.260   0.271   0.420   1.000 \n\n\nOn observe également une certaine variabilité des teneurs en acide citrique dans cet échantillon. En effet, le premier quart des observations a une acidité inférieure à 0.09 , tandis que le dernier quart a une acidité supérieure à 0.42. De fait, certains vins sont quasi dépourvus en acide citrique tandis que d’autres sont riches en cet acide.\n\n\n1.5 Sucre Résiduel (residual.sugar)\nLe sucre résiduel fait référence à la quantité de sucre restant dans un vin après la fermentation alcoolique.\n\nsummary(wine_quality$residual.sugar)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n  0.900   1.900   2.200   2.539   2.600  15.500 \n\n\nOn constate une certaine variabilité des teneurs en sucre résiduel de nos vins. En effet, le premier quartile à 1,9 montre que 25% des vins ont un sucre résiduel inférieur à cette valeur. A l’opposé, 25% dépassent 2,6 d’après le troisième quartile.\nBien que la majorité se concentre autour de la moyenne aux alentours de 2-3 , certains vins sont très faiblement dotés en sucre résiduel (minimum de 0,9) tandis que d’autres en contiennent des doses importantes, pouvant aller jusqu’à 15,5 (max).\n\n\n1.6 Chlores (chlorides)\nLe chlore (chloride) est un élément chimique présent naturellement dans le vin. Il s’agit de l’ion chlorure \\(Cl-\\).\n\nsummary(wine_quality$chlorides)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n0.01200 0.07000 0.07900 0.08747 0.09000 0.61100 \n\n\nDans notre jeu de données, nous constatons que 25% des vins ont une teneur inférieure à 0.07 L. De plus, la moitié des vins ont moins de 0,079 de teneur en chlore, soit moins de la moyenne qui est de 0,08.\nLa majorité des vins(50%) ont une concentration en chlorure située entre 0,07 et 0,09 g/L, avec quelques valeurs extrêmes plus faibles et plus élevées.\n\n\n1.7 Dioxyde de Soufre Libre (free.sulfur.dioxide)\nLe dioxyde de soufre libre (SO2 libre) est une forme de dioxyde de soufre (SO2) que l’on trouve naturellement ou que l’on ajoute au vin. C’est la forme active du dioxyde de soufre dans le vin, essentielle pour sa conservation.\n\nggplot(wine_quality,aes(x=free.sulfur.dioxide))+\n  geom_histogram(bins=30)+\n  theme_bw()+\n  labs(title =\"Teneur en dioxide de soufre libre\")+\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\nOn constate une variabilité importante des teneurs en dioxyde de soufre libre dans nos vins. Le premier quartile à 1 indique que 25% des vins ont une très faible teneur en dioxyde de soufre, (inférieure à 1) tandis que le troisième quartile à 21 révèle que 25% des vins ont une concentration supérieure à 21 en dioxyde de soufre libre. De fait, certains vins possèdent une teneur très élevée en ce dioxyde.\n\n\n1.8 Dioxyde de Soufre Total (total.sulfur.dioxide)\nLe dioxyde de soufre total (SO2 total) est l’autre forme de dioxyde de soufre dans le vin. Il est un composé chimique utilisé couramment comme additif dans le processus de vinification.\n\nsummary(wine_quality$total.sulfur.dioxide)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n   6.00   22.00   38.00   46.47   62.00  289.00 \n\n\nIl y a une grande variabilité des teneurs en dioxyde de soufres totaux pour les vins de cet échantillon. En moyenne, les vins de notre échantillon ont une teneur de 46,47 ce qui est supérieur à la médiane qui s’élève à 38.\n\n\n1.9 Densité (density)\nLa densité du vin est généralement mesurée en gramme par centimètre cube (g/cm³).\n\nsummary(wine_quality$density)\n\n   Min. 1st Qu.  Median    Mean 3rd Qu.    Max. \n 0.9901  0.9956  0.9968  0.9967  0.9978  1.0037 \n\n\nLa valeur minimale de 0.9901 indique la présence de vins avec une densité très faible. Le premier quartile à 0.9956 montre que 25% des vins ont une densité inférieure à ce seuil. La médiane à 0.9968 signifie que la moitié des observations sont en dessous de cette valeur. Le troisième quartile à 0.9978 révèle que 25% des vins ont une densité qui dépasse 0.9978. Enfin, la valeur maximale atteint 1.0037, démontrant l’existence de vins avec des densités très élevées.\n\n\n1.10 pH\nOn remarque d’emblée que les vins de cet échantillon présentent des pH relativement homogènes et resserrés. En effet, 50% des observations (l’intervalle entre le 1er et le 3e quartile) sont comprises entre 3,21 et 3,4. De plus, la moyenne de 3,311 et la médiane à 3,31 indiquent une distribution centrée.\n\nggplot(wine_quality,aes(x=pH))+\n  geom_histogram(bins = 30)+\n  theme_bw()+\n  labs(title =\"Répartition du pH des vins\",x=\"pH\")+\n  theme(aspect.ratio = 1)\n\n\n\n\n\n\n\n\nCette faible dispersion des pH autour de la moyenne suggère une certaine cohérence des vins analysés en termes d’acidité. Quelques vins se démarquent avec un pH plus acide (minimum) ou plus basique (maximum), mais l’essentiel des échantillons affiche un pH entre 3,2 et 3,4.\n\n\n1.11 Sulfates (sulphates)\nLes sulfates, ou plus spécifiquement le dioxyde de soufre (SO2), sont couramment utilisés dans l’industrie vinicole comme additif.\n\nggplot(wine_quality,aes(x=sulphates))+\n  geom_histogram(bins = 30)+\n  theme_bw()+\n  labs(title =\"Répartition du niveau de sulfate des vins\")+\n  theme(aspect.ratio = 1.5)\n\n\n\n\n\n\n\n\nOn observe pour les sulfates une distribution plus dispersée que pour le pH précédemment étudié. Bien que l’intervalle interquartile soit resserré entre 0.55 et 0.73, on observe une valeur minimal de sulphates de 0.33 à 2.00 au maximum.\nCette amplitude plus large induit une moyenne de 0.6581 plus éloignée de la médiane à 0.62. Cette variabilité plus marquée des sulfates pourrait supposer que les vins analysés présentent des concentrations diverses en cet élément. Si la majorité affiche une teneur groupée entre 0.55 et 0.73, certains vins s’en distinguent avec des sulfates très faibles ou au contraire particulièrement élevés.\n\n\n1.12 Alcool (alcohol)\nLa teneur en alcool est l’une des caractéristiques les plus importantes du vin.\nOn constate que la concentration en alcool présente une distribution relativement étalée au sein de l’échantillon. La moyenne de 10.42, légèrement supérieure à la médiane de 10.2, confirme une asymétrie positive de la distribution. De fait, certains vins se démarquent donc avec des degrés d’alcool marqués, au-delà de 14°, quand d’autres restent sur une teneur plus modérée.\n\nggplot(wine_quality,aes(x=alcohol))+\n  geom_density() +\n    geom_histogram(bins = 30)+\n  theme_bw()+\n  labs(title =\"Concentration en alcool\")+\n    theme(aspect.ratio = 1.5)"
  },
  {
    "objectID": "projets/app_supervise/Tp_Cheryl_Kouadio.html#analyse-visuelle-bivariée-entre-les-variables",
    "href": "projets/app_supervise/Tp_Cheryl_Kouadio.html#analyse-visuelle-bivariée-entre-les-variables",
    "title": "Tp noté - Apprentissage supervisé 2A",
    "section": "2. Analyse visuelle bivariée entre les variables",
    "text": "2. Analyse visuelle bivariée entre les variables\n\n2.1 Les relations entre les variables explicatives :\nL’analyse graphique des relations entre les variables explicatives montre que la majorité des couples de variables ne présentent pas de relation linéaire visible. Cependant, quelques exceptions apparaissent avec des corrélations linéaires visuelles notables :\n\nUne relation positive entre fixed.acidity (acidité fixe) et citric.acid (acidité citrique) : quand l’acidité fixe augmente, l’acidité citrique a tendance à augmenter aussi.\nUne relation linéaire positive également entre fixed.acidity et density (densité) : plus l’acidité fixe est élevée, plus la densité du vin a tendance à être importante.\nA l’inverse, une relation linéaire négative semble se dégager entre fixed.acidity et le pH des vins : le pH diminue lorsque l’acidité fixe croît.\n\n\npairs(wine_quality[,-12], pch = \".\",upper.panel = NULL, \n      main=\"Visualisation des relations entre les variables explicatives\")\n\n\n\n\n\n\n\n\n\nCette analyse graphique préliminaire met en évidence quelques relations linéaires visuelles intéressantes entre certains couples de variables explicatives. Des analyses complémentaires comme des tests devraient être effectuées afin de confirmer et quantifier précisément ces relations.\n\n\n\n2.2 Les variables qui influent sur la qualité du vin\nPuisque la variable à prédire (la note du vin) est discrète avec de une étendue de valeurs prises basse, il est pertinent de la considérer comme une variable catégorielle pour l’analyse graphique. L’objectif étant d’évaluer visuellement l’influence des variables explicatives sur cette note qualitative, la représentation adaptée n’est pas un nuage de points.\nEn effet, un nuage de points convient pour analyser des relations continues entre variables quantitatives continues. La visualisation la plus appropriée consistera donc à représenter la distribution de chaque variable explicative en fonction des classes de la variable à prédire, à savoir les différentes notes de qualité des vins.\nOn utilisera pour cela des boîtes à moustaches (boxplots) pour chaque variable explicative, en utilisant les note de qualité comme modalité.\n\nwine_quality %&gt;%\n  mutate(quality = as.factor(quality)) %&gt;%\n  melt(id_vars='quality') %&gt;%\n  ggplot(mapping=aes(x=quality, y=value)) +\n  geom_boxplot(outlier.size = 0.5)+\n  facet_wrap(~variable, scales=\"free\") +\n  labs(title='Boxplot selon la qualité des vins')\n\n\n\n\n\n\n\n\nL’analyse visuelle préliminaire semble mettre en évidence plusieurs variables potentiellement prédictives de la note de qualité des vins :\n\nLa teneur en acide volatile (volatile.acidity) semble négativement corrélée aux bonnes notes de qualité de vins.\nL’acidité citrique (citric.acid) est quant à elle positivement corrélée à la note.\nLe degré d’alcool (alcohol) semble être positivement lié à la note de qualité.\nLa densité (density) semble elle aussi négativement corrélée à la qualité du vin.\nEnfin, la concentration en sulfates (sulphates) est positivement corrélée à la note de qualité.\n\n\nBien que nécessitant confirmation, ces relations visuelles fournissent de premières pistes intéressantes sur les facteurs qui influent potentiellement la qualité du vin. Par la suite, nous allons effectuer un modèle d’arbre cart de régression. Cela nous permettra d’avoir un indicateur sur les variables qui influent bel et bien sur la qualité du vin."
  },
  {
    "objectID": "projets/app_supervise/Tp_Cheryl_Kouadio.html#construction-des-échantillons",
    "href": "projets/app_supervise/Tp_Cheryl_Kouadio.html#construction-des-échantillons",
    "title": "Tp noté - Apprentissage supervisé 2A",
    "section": "1. Construction des échantillons",
    "text": "1. Construction des échantillons\nComme observé au 1.1, les notes de qualité des vins dans notre jeu de données présentent une distribution déséquilibrée, avec une forte majorité de vins ayant une note entre 5 et 6. Afin d’obtenir des échantillons représentatifs pour l’entraînement et les tests, nous devons stratifier les données en fonction de cette variable cible.\nNous allons ainsi constituer nos jeux de données de sorte à conserver la même proportion des différentes notes de qualité dans l’échantillon d’apprentissage et de test avec la fonction createDataPartition de la library caret. Cette stratification selon la variable cible permettra d’obtenir des échantillons représentatifs.\nPour le découpage, nous avons arbitrairement choisi d’allouer 70% des données à l’ensemble d’entraînement, qui servira à l’apprentissage du modèle prédictif. Les 30% restants formeront l’échantillon test, qui permettra d’évaluer la performance de nos différents modèles sur des données inconnues.\n\nset.seed(2023)\nsplitIndex &lt;- createDataPartition(wine_quality$quality, p = 0.7, \n                                  list = FALSE)\n\n# Création des ensembles d'apprentissage et de test en maintenant la proportion de chaque niveau de qualité\ntrain &lt;- wine_quality[splitIndex, ]\ntest &lt;- wine_quality[-splitIndex, ]"
  },
  {
    "objectID": "projets/app_supervise/Tp_Cheryl_Kouadio.html#algorithme-cart-classification-and-regression-trees",
    "href": "projets/app_supervise/Tp_Cheryl_Kouadio.html#algorithme-cart-classification-and-regression-trees",
    "title": "Tp noté - Apprentissage supervisé 2A",
    "section": "2. Algorithme CART (Classification And Regression Trees)",
    "text": "2. Algorithme CART (Classification And Regression Trees)\n\n2.1 Entrainement de l’arbre CART\nL’acronyme CART signifie Classification And Regression Trees. Il désigne une méthode statistique qui construit des prédicteurs par arbre aussi bien en régression qu’en classification. Ici, il sera utilisé pour de la régression.\n\n#Entrainement d'un arbre CART\narbre_cart&lt;-rpart(quality~.,data=train,method=\"anova\")\n\nEn modèle CART, l’hyper paramètre à optimiser est la complexité qui permet l’élagage des arbres. En effet, lorsque l’arbre CART est maximal, notre prédicteur a une très grande variance et un biais faible. A contrario, lorsque l’arbre est constitué uniquement de la racine (qui engendre alors un prédicteur constant), on a a une très petite variance mais un biais élevé.\nL’élagage est une procédure de sélection de modèles, où les modèles sont les sous-arbres élagués de l’arbre maximal. Cette procédure minimise un critère pénalisé où la pénalité est proportionnelle au nombre de feuilles de l’arbre.\nSelon le graphique ci-dessous, la complexité permettant de faire un bon compromis biais-variance est une complexité \\(0,01\\). Nous allons donc re-entrainé notre arbre CART avec cet hyper paramètre.\n\nplotcp(arbre_cart)\n\n\n\n\n\n\n\n\n\n#Re-entrainement de l'arbre CART\narbre_cart&lt;-rpart(quality~.,data=train,method=\"anova\",cp=0.01)\n\n\n\n2.2 Divisions équireductrices et equidivisantes\nL’entrainement de l’arbre CART sur notre échantillon d’apprentissage nous permet de prédire des nouvelles valeurs en se basant sur la connaissance de certaines valeurs de certaines variables.\nLe premier noeud formé par l’entrainement de l’arbre CART est le noeud racine qui contient toutes les 1120 observations. Cette première division est basée sur la teneur en alcool des vins de notre échantillon d’apprentissage.\nAinsi, si la teneur en alcool est inférieure à 10.525, alors l’observation va à gauche. C’est la meilleure division car elle offre la plus grande amélioration. Il existe également d’autres variables qui lorsqu’elles sont divisées à un certain seuil, fournissent la meilleure amélioration (diminution) de l’erreur (ou de l’impureté). Ce sont les divisions équiréductrices(Primary splits) comme sulphates, volatile.acidity etc.\nS’il manque des valeurs pour cette variable, l’arbre CART se basera sur d’autres variables comme des divisions équidivisantes (Surrogate splits) comme density, chlorides, etc., pour décider de la division et donc prédire un nouveau vin.\nPour prédire un nouveau vin pour lequel le taux d’alcool et la densité n’aurait pas été mesurée et serait donc manquants, l’arbre CART se basera donc sur la concentration en chlore pour effectuer la première coupure.\n\n# Afficher le résumé de l'apprentissage de notre modèle\n#summary(arbre_cart) \n\n### Extrait de la sortie de summary(arbre_rpart) ###\nNode number 1: 1120 observations,    complexity param=0.1738621\n  mean=5.632143, MSE=0.6486097 \n  left son=2 (684 obs) right son=3 (436 obs)\n  Primary splits:\n      alcohol          &lt; 10.525   to the left,  improve=0.17386210, (0 missing)\n      sulphates        &lt; 0.645    to the left,  improve=0.12939930, (0 missing)\n      volatile.acidity &lt; 0.405    to the right, improve=0.12619650, (0 missing)\n      citric.acid      &lt; 0.295    to the left,  improve=0.07021357, (0 missing)\n      density          &lt; 0.995565 to the right, improve=0.06232750, (0 missing)\n  Surrogate splits:\n      density          &lt; 0.995575 to the right, agree=0.763, adj=0.392, (0 split)\n      chlorides        &lt; 0.0685   to the right, agree=0.686, adj=0.193, (0 split)\n      volatile.acidity &lt; 0.375    to the right, agree=0.666, adj=0.142, (0 split)\n      fixed.acidity    &lt; 6.75     to the right, agree=0.651, adj=0.103, (0 split)\n      sulphates        &lt; 0.705    to the left,  agree=0.640, adj=0.076, (0 split)\n\n\n2.3 Visualisation du résultat\nL’arbre CART nous permet de d’avoir la prédiction de la qualité du vin avec de nouvelle valeur. L’arbre démarre avec la valeur de 5.6, qui est la note moyenne de tous les vins et se lit assez intuitivement. Selon l’arbre, les variables les plus informatives sur la qualité du vin sont la concentration en alcool, en sulfates, l’acidité volatile ainsi que le pH.\nPar exemple, si nous avons un nouveau vin avec les caractéristiques suivantes alcohol = 11.5, pH = 2, free.sulfur.dioxide = 7.6, sulphates =0.42, volatile.acidity = 0.98, nous aurions obtenu la note de \\(4\\).\n\nrpart.plot(arbre_cart)\n\n\n\n\n\n\n\n\n\n\n2.4 Erreur de généralisation et comparer les vraies valeurs aux prédictions\nL’Erreur Quadratique Moyenne est la métrique de performance que nous utiliserons pour évaluer la qualité des modèles de régression. Elle mesure la moyenne des carrés des erreurs, c’est-à-dire la moyenne des carrés des différences entre les valeurs prédites et les valeurs réelles. Plus celle-ci est faible, plus le modèle est bien ajusté à nos données et donc plus précis.\n\n#Erreur de prédictions\npredictions_cart &lt;- predict(arbre_cart, test)\nmse &lt;- mean((predictions_cart - test$quality )^2) \nprint(paste(\"Erreur de généralisation (RMSE) : \", sqrt(mse)))\n\n[1] \"Erreur de généralisation (RMSE) :  0.678171784937612\"\n\n\n\ndata &lt;- data.frame(TrueValues = test$quality, PredictedValues = predictions_cart)\n\nggplot(data, aes(x = TrueValues, y = PredictedValues)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"vraies valeurs\", y = \"prédictions\", title = \"Predictions de la qualité du vin\")\n\n\n\n\n\n\n\n\n\n\n2.5 Robustesse\nLes arbres CART, bien que puissants pour la modélisation et la visualisation, présentent une sensibilité notable à de petits changements dans les données. Cela peut se traduire par une grande variabilité dans la structure de l’arbre pour de légères modifications du jeu de données.\nEn observant les quatre arbres ci dessous, nous pouvons constater que chaque arbre diffère dans sa structure, ses nœuds de division et ses critères de division, malgré le fait qu’ils soient issus de jeux de données très similaires ou de sous-ensembles de données. Cette variabilité souligne le manque de robustesse des arbres CART.\n\nnum_trees &lt;- 4  # Nombre d'arbres à générer\n\n# Créez une liste pour stocker les arbres\narbres &lt;- list()\n\n# Générez et stockez les arbres dans la liste\nfor (i in 1:num_trees) {\n  index &lt;- createDataPartition(wine_quality$quality, p = 0.5, list = FALSE)\n  training_sample &lt;- wine_quality[index, ]\n  arbres[[i]] &lt;- rpart(quality ~ ., data = training_sample)\n}\n\npar(mfrow=c(2, 2))  # Crée une grille pour afficher plusieurs arbres\n\n# Affichez chaque arbre dans la grille\nfor (i in 1:num_trees) {\n  rpart.plot(arbres[[i]], main = paste(\"Arbre\", i))\n}\n\n\n\n\n\n\n\n\n\nEn pratique, pour obtenir des modèles plus robustes et moins sensibles aux variations, on utilise souvent les forêts aléatoires. Les forêts aléatoires, en combinant plusieurs arbres CART (où chaque arbre est formé à partir d’un sous-ensemble de données et d’un sous-ensemble de variables), permettent de “moyenner” les prédictions et d’atténuer les effets des instabilités individuelles des arbres."
  },
  {
    "objectID": "projets/app_supervise/Tp_Cheryl_Kouadio.html#forêt-aléatoire",
    "href": "projets/app_supervise/Tp_Cheryl_Kouadio.html#forêt-aléatoire",
    "title": "Tp noté - Apprentissage supervisé 2A",
    "section": "3. Forêt aléatoire",
    "text": "3. Forêt aléatoire\n\n3.1 Validation croisée\nLe package ranger est utilisé pour entraîner des forêts aléatoires. Voici quelques hyperparamètres importants de ranger que vous pouvez ajuster :\n\nmtry: Nombre d’arbre à insérer dans la forêt aléatoire\nmin.node.size: Taille minimale du nœud (critère de changement d’un noeud en feuille)\nsplitrule: le critère de pureté (“gini”, “extratrees”, “variance” ,etc.).\n\n\nmodelLookup(\"ranger\")\n\n\n  \n\n\n\nAfin de les optimiser, nous allons réaliser une validation croisée.\n\n# Configuration de la validation croisée\ncontrol &lt;- trainControl(method = \"cv\", number = 5)\n\n# Nouvelle grille d'hyperparamètres\ngrid &lt;- expand.grid(\n  mtry = 1:10,\n  splitrule = c(\"variance\", \"extratrees\"),\n  min.node.size =1:5\n)\n\n# modèle via validation croisée\nforet_aleatoire &lt;- train(\n  quality ~ .,\n  data = train,\n  method = \"ranger\",\n  trControl = control,\n  tuneGrid = grid\n)\n\nforet_aleatoire$bestTune\n\n\n  \n\n\n\nNous avons utilisé l’erreur quadratique moyenne (RMSE) pour choisir le meilleur modèle par validation croisée. Les hyperparamètres optimaux sont mtry = 6, splitrule = extratrees, et min.node.size = 2, ce qui assure un bon équilibre entre performance et risque de surajustement.\n\n\n3.2 Re-entrainement du modèle\n\n# Re-entrainement du modele avec valeurs optimales\nforet_aleatoire_final &lt;- ranger(quality ~ ., \n                        data = train, \n                        mtry = foret_aleatoire$bestTune$mtry, \n                        splitrule = foret_aleatoire$bestTune$splitrule,\n                        min.node.size = foret_aleatoire$bestTune$min.node.size, \n                        importance = 'permutation')\n\nSelon notre modèle de forêt aléatoire, l’alcool est de loin la variable la plus influente pour prédire la qualité du vin, suivi de l’acidité volatile et du sulfate etc. Cela correspond plus ou moins aux variables que nous avons identifié dans l’analyse visuelle préliminaire 2.2 .\nLes autres variables comme le chlorures et le sucre résiduel ont une moindre influence sur la qualité du vin. Ces résultats peuvent orienter nos prochains tests sur les facteurs clés affectant la qualité du vin.\n\n# Visualisation de l'importance des variables\nsort(foret_aleatoire_final$variable.importance, decreasing=T)\n\n             alcohol     volatile.acidity            sulphates \n          0.29968914           0.13309690           0.11818299 \ntotal.sulfur.dioxide              density          citric.acid \n          0.06834531           0.06113796           0.05774871 \n       fixed.acidity                   pH  free.sulfur.dioxide \n          0.04428113           0.03737898           0.03737879 \n           chlorides       residual.sugar \n          0.03435503           0.02516916 \n\n\n\n\n3.3 Erreur de généralisation et erreur Out-Of-Bag\nL’erreur OOB est une estimation de l’erreur de généralisation obtenue en utilisant chaque observation pour évaluer un modèle qui n’a pas été formé avec cette observation. C’est l’un des avantages des forêts aléatoires; elles fournissent une estimation interne de l’erreur de généralisation.\n\npredictions_rf &lt;- predict(foret_aleatoire_final, data = test)$predictions\nmse &lt;- mean((predictions_rf - test$quality)^2)\nprint(paste(\"Erreur de généralisation (MSE): \", mse))\n\n[1] \"Erreur de généralisation (MSE):  0.336074897703549\"\n\nprint(paste(\"Erreur Out-Of-Bag (OOB): \", foret_aleatoire_final$prediction.error))\n\n[1] \"Erreur Out-Of-Bag (OOB):  0.342473480056595\"\n\n\nNous constatons qu’il y a une faible différence entre l’erreur de généralisation et l’erreur Out-Of-Bag (environ \\(0,01\\)).\nLa proximité de ces deux erreurs suggère que notre modèle de forêt aléatoire est robuste et généralise bien aux nouvelles données, sans signe apparent de sur ajustement.\nEn bref, notre modèle semble performant et fiable pour la prédiction de la qualité du vin.\n\ndata &lt;- data.frame(TrueValues = test$quality, PredictedValues = predictions_rf)\n\nggplot(data, aes(x = TrueValues, y = PredictedValues)) +\n  geom_point() +\n  theme_minimal() +\n  labs(x = \"vraies valeurs\", y = \"prédictions\", title = \"Predictions de la qualité du vin\")\n\n\n\n\n\n\n\n\n\n\n3.4 MDA importance (Mean decreasing accuracy)\nL’idée principale de la MDA repose sur le fait que si une variable explicative n’est pas importante pour prédire la cible, qu’on la prenne en compte ou non dans la construction de la forêt ne devrait pas changer l’erreur de généralisation.\nElle se calcule de la manière suivante :\n\\[MDA(X_j) = \\frac{1}{n_{\\text{test}}} \\sum_{i=1}^{n_{\\text{test}}} l(y^*_i, \\frac{1}{B} \\sum_{b=1}^{B} g^{(-j)}_b(x^*_i)) - l(y^*_i, \\frac{1}{B} \\sum_{b=1}^{B} g_b(x^*_i)) \\] où \\(g^{-j}_b\\) désigne la forêt aléatoire construite sans la j-ième variable explicative.\n\nset.seed(123)\nB &lt;- foret_aleatoire_final$num.trees\nn_test &lt;- nrow(test)\n  \noriginal_preds &lt;- predict(foret_aleatoire_final, data = test)$predictions\noriginal_loss &lt;- mean((original_preds - test[[\"quality\"]])^2) # MSE\n  \nfeature_names &lt;- names(test)\nfeature_names &lt;- feature_names[feature_names != \"quality\"]\nMDA_values &lt;- numeric(length(feature_names))\n  \n  \nfor(j in seq_along(feature_names)) {\n  temp_data &lt;- train[, -j, with = FALSE]\n    \n  temp_forest &lt;- ranger(quality ~., data = temp_data, num.trees = B)\n\n  permuted_preds &lt;- predict(temp_forest, data = test)$predictions\n  permuted_loss &lt;- mean((permuted_preds - test$quality)^2) # MSE après suppression de variable j\n\n  MDA_values[feature_names[j]] &lt;- (permuted_loss - original_loss) \n}\n  \nsort(MDA_values[MDA_values != 0],decreasing = T)\n\n           sulphates              alcohol     volatile.acidity \n        0.0366279588         0.0264460487         0.0144637845 \ntotal.sulfur.dioxide              density        fixed.acidity \n        0.0075565892         0.0073128653         0.0051167717 \n                  pH            chlorides       residual.sugar \n        0.0050919154         0.0034444456         0.0034389022 \n         citric.acid  free.sulfur.dioxide \n        0.0021225963         0.0009107246 \n\n\nLa sortie MDA montre l’importance des variables dans un modèle de forêt aléatoire. Les “sulphates” et “alcohol” ont les valeurs MDA les plus élevées, ce qui signifie qu’ils sont les plus importants pour la précision du modèle. Les variables comme “total.sulfur.dioxide” et “density” ont une importance moindre mais intéressante à savoir.\nA l’inverse, des variables comme “fixed.acidity” et “citric.acid” etc. ont des valeurs MDA négatives, cela signifirait que les supprimer du modèle ferait baisser notre erreur de généralisation ce qui pourrait indiquer qu’elles ne sont pas très utiles dans ce modèle.\n\nsort(foret_aleatoire_final$variable.importance,decreasing = T)\n\n             alcohol     volatile.acidity            sulphates \n          0.29968914           0.13309690           0.11818299 \ntotal.sulfur.dioxide              density          citric.acid \n          0.06834531           0.06113796           0.05774871 \n       fixed.acidity                   pH  free.sulfur.dioxide \n          0.04428113           0.03737898           0.03737879 \n           chlorides       residual.sugar \n          0.03435503           0.02516916 \n\n\nDans la sortie d’importance de notre modèle constitué de toutes les variables explicatives, “Alcohol” prend la première place, suivi de “sulphates” et “volatile.acidity”.\nDans les deux cas, “alcohol”, “sulphates”, et “volatile.acidity” se démarquent comme les variables les plus influentes, mais leur ordre d’importance change. “Alcohol” est devenu plus dominant dans la sortie donné directement par le modèle."
  },
  {
    "objectID": "projets/LAL/partition.html",
    "href": "projets/LAL/partition.html",
    "title": "partition",
    "section": "",
    "text": "#setwd(\"~/Repositories/cherry-stats/projets/LAL\")\nload(\"qaly.RData\")\nlibrary(dplyr)\n\n\nAttachement du package : 'dplyr'\n\n\nLes objets suivants sont masqués depuis 'package:stats':\n\n    filter, lag\n\n\nLes objets suivants sont masqués depuis 'package:base':\n\n    intersect, setdiff, setequal, union\n\nlibrary(boot)\nlibrary(survRM2)\n\nLe chargement a nécessité le package : survival\n\n\n\nAttachement du package : 'survival'\n\n\nL'objet suivant est masqué depuis 'package:boot':\n\n    aml\n\n\n\nqaly$finctdt   &lt;- apply(qaly[,c(\"C1dt\",\"C2dt\" , \"C3dt\" , \"C4dt\", \"Inter1dt\" )],1,max,na.rm=T)\nqaly$finct.dt &lt;- as.Date(as.character(qaly$finctdt),format=\"%Y-%m-%d\")\n\n\nqaly$suivi  &lt;- as.numeric(qaly$Datemax-qaly$randodt)/(365.25/12)\nqaly$survie &lt;- as.numeric(qaly$deathdt-qaly$randodt)/(365.25/12)\nqaly$delfinct &lt;- as.numeric(qaly$finct.dt-qaly$randodt)/(365.25/12)\nqaly$delarr &lt;- as.numeric(qaly$arrpremadt-qaly$randodt)/(365.25/12)\nqaly$delfinct[which(qaly$delfinct==0)]&lt;- qaly$delarr[which(qaly$delfinct==0)]\nqaly$finct&lt;- rep(1,length(qaly$delfinct))\nqaly$delrec&lt;- as.numeric(qaly$relapsdt-qaly$randodt)/(365.25/12)\nqaly$delpfs&lt;- qaly$suivi\nqaly$delpfs[!is.na(qaly$delrec)]&lt;-qaly$delrec[!is.na(qaly$delrec)]\nqaly$delbmt &lt;- as.numeric(qaly$bmtdt-qaly$randodt)/(365.25/12)\nqaly$delbmt1 &lt;- as.numeric(qaly$cgvhdt-qaly$randodt)/(365.25/12)\nqaly$delbmt2 &lt;- as.numeric(qaly$agvhdt-qaly$randodt)/(365.25/12)\n\nqaly$delfinct2 &lt;- qaly$delfinct\nqaly$delfinct2[!is.na(qaly$delbmt)]&lt;-pmax(qaly$delfinct[!is.na(qaly$delbmt)],qaly$delbmt[!is.na(qaly$delbmt)]) #add ,qaly$delbmt1[!is.na(qaly$delbmt1)],qaly$delbmt2[!is.na(qaly$delbmt2)]\n#It takes in account the complication ecountered after the transplant\n#finct\nqaly$finct2 &lt;- qaly$finct\nqaly$finct2[!is.na(qaly$delbmt)]&lt;-1\n\n\n\n\nlibrary(survival)\n\n# Arm A\nct_A  &lt;- survfit(Surv(delfinct,finct)~1,data=qaly,subset = (R1 == \"Intensive arm (A)\")) # ct : Durée du traitement\nct2_A  &lt;- survfit(Surv(delfinct2,finct2)~1,data=qaly,subset = (R1 == \"Intensive arm (A)\")) #Durée du traitement avec complications\nsv_A  &lt;- survfit(Surv(suivi,as.numeric(as.character(dc)))~1,data=qaly,subset = (R1 == \"Intensive arm (A)\")) # Durée de survie : Temps avant décès\nefs_A &lt;- survfit(Surv(delpfs,as.numeric(as.character(pfs)))~1, data=qaly,subset = (R1 == \"Intensive arm (A)\")) # Durée de survie sans progression : Temps avant rechute \n\n# Arm B\nct_B  &lt;- survfit(Surv(delfinct,finct)~1,data=qaly,subset = (R1 == \"Light arm (B)\"))\nct2_B  &lt;- survfit(Surv(delfinct2,finct2)~1,data=qaly,subset = (R1 == \"Light arm (B)\"))\nsv_B  &lt;- survfit(Surv(suivi,as.numeric(as.character(dc)))~1,data=qaly,subset = (R1 == \"Light arm (B)\"))\nefs_B &lt;- survfit(Surv(delpfs,as.numeric(as.character(pfs)))~1, data=qaly,subset = (R1 == \"Light arm (B)\"))\n\n\n\n\n\n\n\n#fig.width=6, fig.height=5.5 =&gt; use later for website\n\nfit1&lt;-list(CT = ct_A, CT2=ct2_A,SV=sv_A, EFS=efs_A)\nlibrary(survminer)\n\nLe chargement a nécessité le package : ggplot2\n\n\nLe chargement a nécessité le package : ggpubr\n\n\n\nAttachement du package : 'survminer'\n\n\nL'objet suivant est masqué depuis 'package:survival':\n\n    myeloma\n\nlibrary(gridExtra)\n\n\nAttachement du package : 'gridExtra'\n\n\nL'objet suivant est masqué depuis 'package:dplyr':\n\n    combine\n\ncolors &lt;- c(\"#A31621\", \"#053C5E\", \"#F3A712\", \"#1F7A8C\", \"#AFB3F7\")\n\nt&lt;-ggsurvplot_combine(fit1,\n          risk.table = TRUE,                  # Add risk table\n   xlab = \"Time in days\",   # customize X axis label.\n   ggtheme = theme_light(), # customize plot and risk table with a therme.\n risk.table.y.text.col = T, # colour risk table text annotations.\n  risk.table.y.text = FALSE,\n  legend.labs = c(\"CT\",\"CT2\",\"SV\",\"EFS\"))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `survtable = purrr::map2(...)`.\nCaused by warning:\n! `select_()` was deprecated in dplyr 0.7.0.\nℹ Please use `select()` instead.\nℹ The deprecated feature was likely used in the survminer package.\n  Please report the issue at &lt;https://github.com/kassambara/survminer/issues&gt;.\n\nsv_plot_A &lt;- ggsurvplot(sv_A, risk.table = TRUE,\n                      ggtheme = theme_light(),\n                      # tables.theme = theme(\n                      #   axis.text.x = element_blank(),  # Hide x-axis text\n                      #   axis.ticks.x = element_blank(), # Hide x-axis ticks\n                      #   axis.title.x = element_blank()  # Optionally, hide the x-axis title as well\n                      # ),\n                      risk.table.y.text.col = TRUE,\n                      risk.table.y.text = FALSE,\n                      palette = colors[2])  \n\nlibrary(dplyr)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(ks)\nlibrary(tidyr)\nlibrary(survival)\nlibrary(ggplot2)\nlibrary(ggsurvfit)\n\n\ndf&lt;-t$plot$data\nsurv_A&lt;-df %&gt;% mutate(strata = sub(\"x=\", \"\", strata)) %&gt;%\n  group_by(strata) %&gt;%\n  summarise(time = c(time - 0.001, time),\n            surv = c(1, head(surv, -1), surv),\n            .groups = \"drop\") %&gt;%\n  arrange(time) %&gt;%\n  pivot_wider(names_from = \"strata\", values_from = \"surv\") %&gt;%\n  fill(1:5, .direction = \"downup\") %&gt;%\n  ggplot(aes(time, CT)) +\n  geom_step(aes(linetype = \"CT\")) +\n  geom_ribbon(aes(ymin = 0, ymax = CT, fill = \"TOX1\"),alpha=0.5)+\n  geom_step(aes(y = CT2, linetype = \"CT2\"),linewidth=0.9, colour=\"#0077b6\") +\n  geom_ribbon(aes(ymin = CT, ymax = CT2, fill = \"TOX2\"),alpha=0.5) +\n  geom_step(aes(y = EFS,linetype = \"EFS\"),linewidth=0.9, colour=\"#48cae4\") +\n  geom_ribbon(aes(ymin = CT2, ymax = EFS, fill = \"TWiST\"),alpha=0.5)+\n  geom_step(aes(y = SV,linetype = \"SV\"),linewidth=0.9, colour=\"#caf0f8\")+\n  geom_ribbon(aes(ymin = EFS, ymax = SV, fill = \"REL\"),alpha=0.5)+\n  scale_fill_manual(values = c(\"TOX1\" = \"#03045e\", \"TOX2\" = \"#0077b6\", \"TWiST\" = \"#48cae4\", \"REL\" = \"#caf0f8\")) +\n  labs(x = \"Time\", y = \"Survival Probability\", linetype = \"Duration\", fill = \"Events\") +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\ncombined_plot &lt;- grid.arrange(surv_A, sv_plot_A$table, ncol = 1,heights = c(6, 2))\n\n\n\n\n\n\n\n\n\n\n\nlibrary(survminer)\nlibrary(survival)\nlibrary(gridExtra) # or library(patchwork)\n\n# Assuming ct, sv, and efs are survival objects\nfit1 &lt;- list(CT = ct_A, SV = sv_A, EFS = efs_A)\n\n# custom color palette for CT, SV, EFS\ncolors &lt;- c(\"#A31621\", \"#053C5E\", \"#F3A712\", \"#1F7A8C\", \"#AFB3F7\")\n\n# Step 1: Generate the Combined Survival Plot without a risk table\nsurv1_A &lt;- ggsurvplot_combine(fit1,\n                        risk.table = FALSE,  # Disable risk table here\n                        xlab = \"Time in days\",\n                        ggtheme = theme_light(),\n                        legend.labs = c(\"CT\",\"SV\",\"EFS\"),\n                        palette=colors,\n                        title=\"Bras A\")\n\n# Step 2: Generate a Separate Risk Table for \"SV\"\n# Note: This assumes 'sv' is a fit object from survival analysis.\n# If 'sv' is not directly usable, you might need to recreate the survival analysis for SV.\nsv_plot_A &lt;- ggsurvplot(sv_A, risk.table = TRUE,\n                      ggtheme = theme_light(),\n                      # tables.theme = theme(\n                      #   axis.text.x = element_blank(),  # Hide x-axis text\n                      #   axis.ticks.x = element_blank(), # Hide x-axis ticks\n                      #   axis.title.x = element_blank()  # Optionally, hide the x-axis title as well\n                      # ),\n                      risk.table.y.text.col = TRUE,\n                      risk.table.y.text = FALSE,\n                      palette = colors[2])  # Adjust 'colors[2]' as per your color setup\n\n\n# Step 3: Combine the Plot and the Risk Table Manually\n# Option 1: Using gridExtra\ncombined_plot &lt;- grid.arrange(surv1_A$plot, sv_plot_A$table, ncol = 1,heights = c(6, 2))\n\n\n\n\n\n\n\n# Or Option 2: Using patchwork (Uncomment to use)\n# combined_plot &lt;- g$plot / sv_plot$table\n\n# Print or save the combined plot\nprint(combined_plot)\n\nTableGrob (2 x 1) \"arrange\": 2 grobs\n  z     cells    name           grob\n1 1 (1-1,1-1) arrange gtable[layout]\n2 2 (2-2,1-1) arrange gtable[layout]\n\n\n\n\n\n\nlibrary(survminer)\nlibrary(survival)\nlibrary(gridExtra) # or library(patchwork)\n\n# Assuming ct, sv, and efs are survival objects\nfit2 &lt;- list(CT = ct_A, SV = sv_A, EFS = efs_A, CT2 = ct2_A)\n\n# custom color palette for CT, SV, EFS\ncolors &lt;- c(\"#A31621\", \"#053C5E\", \"#F3A712\", \"#1F7A8C\", \"#AFB3F7\")\n\n# Step 1: Generate the Combined Survival Plot without a risk table\nsurv2_A &lt;- ggsurvplot_combine(fit2,\n                        risk.table = FALSE,  # Disable risk table here\n                        xlab = \"Time in days\",\n                        ggtheme = theme_light(),\n                        legend.labs = c(\"CT\",\"SV\",\"EFS\",\"CT2\"),\n                        palette=colors,\n                        title=\"Bras A\")\n\n\n\n# Step 3: Combine the Plot and the Risk Table Manually\n# Option 1: Using gridExtra\ncombined_plot &lt;- grid.arrange(surv2_A$plot, sv_plot_A$table, ncol = 1,heights = c(6, 2))\n\n\n\n\n\n\n\n# Print or save the combined plot\nprint(combined_plot)\n\nTableGrob (2 x 1) \"arrange\": 2 grobs\n  z     cells    name           grob\n1 1 (1-1,1-1) arrange gtable[layout]\n2 2 (2-2,1-1) arrange gtable[layout]\n\n\n\n# fit2&lt;-list(CT = ct, CT2 = ct2, SV=sv, EFS=efs)\n# library(survminer)\n# t&lt;-ggsurvplot_combine(fit,\n#           risk.table = TRUE,                  # Add risk table\n#    xlab = \"Time in days\",   # customize X axis label.\n#    ggtheme = theme_light(), # customize plot and risk table with a therme.\n#  risk.table.y.text.col = T, # colour risk table text annotations.\n#   risk.table.y.text = FALSE,\n#   legend.labs = c(\"CT\", \"CT2\",\"SV\",\"EFS\"))\n# t\n\n\n\n\n\n\n\n\nlibrary(survminer)\nlibrary(survival)\nlibrary(gridExtra) # or library(patchwork)\n\n# Assuming ct, sv, and efs are survival objects\nfit1 &lt;- list(CT = ct_B, SV = sv_B, EFS = efs_B)\n\n# custom color palette for CT, SV, EFS\ncolors &lt;- c(\"#A31621\", \"#053C5E\", \"#F3A712\", \"#1F7A8C\", \"#AFB3F7\")\n\n# Step 1: Generate the Combined Survival Plot without a risk table\nsurv1_B &lt;- ggsurvplot_combine(fit1,\n                        risk.table = FALSE,  # Disable risk table here\n                        xlab = \"Time in days\",\n                        ggtheme = theme_light(),\n                        legend.labs = c(\"CT\",\"SV\",\"EFS\"),\n                        palette=colors,\n                        title=\"Bras B\")\n\n# Step 2: Generate a Separate Risk Table for \"SV\"\n# Note: This assumes 'sv' is a fit object from survival analysis.\n# If 'sv' is not directly usable, you might need to recreate the survival analysis for SV.\nsv_plot_B &lt;- ggsurvplot(sv_B, risk.table = TRUE,\n                      ggtheme = theme_light(),\n                      # tables.theme = theme(\n                      #   axis.text.x = element_blank(),  # Hide x-axis text\n                      #   axis.ticks.x = element_blank(), # Hide x-axis ticks\n                      #   axis.title.x = element_blank()  # Optionally, hide the x-axis title as well\n                      # ),\n                      risk.table.y.text.col = TRUE,\n                      risk.table.y.text = FALSE,\n                      palette = colors[2])  # Adjust 'colors[2]' as per your color setup\n\n\n# Step 3: Combine the Plot and the Risk Table Manually\n# Option 1: Using gridExtra\ncombined_plot &lt;- grid.arrange(surv1_B$plot, sv_plot_B$table, ncol = 1,heights = c(6, 2))\n\n\n\n\n\n\n\n# Or Option 2: Using patchwork (Uncomment to use)\n# combined_plot &lt;- g$plot / sv_plot$table\n\n# Print or save the combined plot\nprint(combined_plot)\n\nTableGrob (2 x 1) \"arrange\": 2 grobs\n  z     cells    name           grob\n1 1 (1-1,1-1) arrange gtable[layout]\n2 2 (2-2,1-1) arrange gtable[layout]\n\n\n\n\n\n\nlibrary(survminer)\nlibrary(survival)\nlibrary(gridExtra) # or library(patchwork)\n\n# Assuming ct, sv, and efs are survival objects\nfit2 &lt;- list(CT = ct_B, SV = sv_B, EFS = efs_B, CT2 = ct2_B)\n\n# custom color palette for CT, SV, EFS\ncolors &lt;- c(\"#A31621\", \"#053C5E\", \"#F3A712\", \"#1F7A8C\", \"#AFB3F7\")\n\n# Step 1: Generate the Combined Survival Plot without a risk table\nsurv2_B &lt;- ggsurvplot_combine(fit2,\n                        risk.table = FALSE,  # Disable risk table here\n                        xlab = \"Time in days\",\n                        ggtheme = theme_light(),\n                        legend.labs = c(\"CT\",\"SV\",\"EFS\",\"CT2\"),\n                        palette=colors,\n                        title=\"Bras B\")\n\n\n\n# Step 3: Combine the Plot and the Risk Table Manually\n# Option 1: Using gridExtra\ncombined_plot &lt;- grid.arrange(surv2_B$plot, sv_plot_B$table, ncol = 1,heights = c(6, 2))\n\n\n\n\n\n\n\n# Print or save the combined plot\nprint(combined_plot)\n\nTableGrob (2 x 1) \"arrange\": 2 grobs\n  z     cells    name           grob\n1 1 (1-1,1-1) arrange gtable[layout]\n2 2 (2-2,1-1) arrange gtable[layout]"
  },
  {
    "objectID": "projets/LAL/partition.html#plot-the-survival-curves-for-each-arm",
    "href": "projets/LAL/partition.html#plot-the-survival-curves-for-each-arm",
    "title": "partition",
    "section": "",
    "text": "#fig.width=6, fig.height=5.5 =&gt; use later for website\n\nfit1&lt;-list(CT = ct_A, CT2=ct2_A,SV=sv_A, EFS=efs_A)\nlibrary(survminer)\n\nLe chargement a nécessité le package : ggplot2\n\n\nLe chargement a nécessité le package : ggpubr\n\n\n\nAttachement du package : 'survminer'\n\n\nL'objet suivant est masqué depuis 'package:survival':\n\n    myeloma\n\nlibrary(gridExtra)\n\n\nAttachement du package : 'gridExtra'\n\n\nL'objet suivant est masqué depuis 'package:dplyr':\n\n    combine\n\ncolors &lt;- c(\"#A31621\", \"#053C5E\", \"#F3A712\", \"#1F7A8C\", \"#AFB3F7\")\n\nt&lt;-ggsurvplot_combine(fit1,\n          risk.table = TRUE,                  # Add risk table\n   xlab = \"Time in days\",   # customize X axis label.\n   ggtheme = theme_light(), # customize plot and risk table with a therme.\n risk.table.y.text.col = T, # colour risk table text annotations.\n  risk.table.y.text = FALSE,\n  legend.labs = c(\"CT\",\"CT2\",\"SV\",\"EFS\"))\n\nWarning: There was 1 warning in `mutate()`.\nℹ In argument: `survtable = purrr::map2(...)`.\nCaused by warning:\n! `select_()` was deprecated in dplyr 0.7.0.\nℹ Please use `select()` instead.\nℹ The deprecated feature was likely used in the survminer package.\n  Please report the issue at &lt;https://github.com/kassambara/survminer/issues&gt;.\n\nsv_plot_A &lt;- ggsurvplot(sv_A, risk.table = TRUE,\n                      ggtheme = theme_light(),\n                      # tables.theme = theme(\n                      #   axis.text.x = element_blank(),  # Hide x-axis text\n                      #   axis.ticks.x = element_blank(), # Hide x-axis ticks\n                      #   axis.title.x = element_blank()  # Optionally, hide the x-axis title as well\n                      # ),\n                      risk.table.y.text.col = TRUE,\n                      risk.table.y.text = FALSE,\n                      palette = colors[2])  \n\nlibrary(dplyr)\nlibrary(gt)\nlibrary(gtsummary)\nlibrary(ks)\nlibrary(tidyr)\nlibrary(survival)\nlibrary(ggplot2)\nlibrary(ggsurvfit)\n\n\ndf&lt;-t$plot$data\nsurv_A&lt;-df %&gt;% mutate(strata = sub(\"x=\", \"\", strata)) %&gt;%\n  group_by(strata) %&gt;%\n  summarise(time = c(time - 0.001, time),\n            surv = c(1, head(surv, -1), surv),\n            .groups = \"drop\") %&gt;%\n  arrange(time) %&gt;%\n  pivot_wider(names_from = \"strata\", values_from = \"surv\") %&gt;%\n  fill(1:5, .direction = \"downup\") %&gt;%\n  ggplot(aes(time, CT)) +\n  geom_step(aes(linetype = \"CT\")) +\n  geom_ribbon(aes(ymin = 0, ymax = CT, fill = \"TOX1\"),alpha=0.5)+\n  geom_step(aes(y = CT2, linetype = \"CT2\"),linewidth=0.9, colour=\"#0077b6\") +\n  geom_ribbon(aes(ymin = CT, ymax = CT2, fill = \"TOX2\"),alpha=0.5) +\n  geom_step(aes(y = EFS,linetype = \"EFS\"),linewidth=0.9, colour=\"#48cae4\") +\n  geom_ribbon(aes(ymin = CT2, ymax = EFS, fill = \"TWiST\"),alpha=0.5)+\n  geom_step(aes(y = SV,linetype = \"SV\"),linewidth=0.9, colour=\"#caf0f8\")+\n  geom_ribbon(aes(ymin = EFS, ymax = SV, fill = \"REL\"),alpha=0.5)+\n  scale_fill_manual(values = c(\"TOX1\" = \"#03045e\", \"TOX2\" = \"#0077b6\", \"TWiST\" = \"#48cae4\", \"REL\" = \"#caf0f8\")) +\n  labs(x = \"Time\", y = \"Survival Probability\", linetype = \"Duration\", fill = \"Events\") +\n  theme_minimal() +\n  theme(legend.position = \"right\")\n\nWarning: Returning more (or less) than 1 row per `summarise()` group was deprecated in\ndplyr 1.1.0.\nℹ Please use `reframe()` instead.\nℹ When switching from `summarise()` to `reframe()`, remember that `reframe()`\n  always returns an ungrouped data frame and adjust accordingly.\n\ncombined_plot &lt;- grid.arrange(surv_A, sv_plot_A$table, ncol = 1,heights = c(6, 2))\n\n\n\n\n\n\n\n\n\n\n\nlibrary(survminer)\nlibrary(survival)\nlibrary(gridExtra) # or library(patchwork)\n\n# Assuming ct, sv, and efs are survival objects\nfit1 &lt;- list(CT = ct_A, SV = sv_A, EFS = efs_A)\n\n# custom color palette for CT, SV, EFS\ncolors &lt;- c(\"#A31621\", \"#053C5E\", \"#F3A712\", \"#1F7A8C\", \"#AFB3F7\")\n\n# Step 1: Generate the Combined Survival Plot without a risk table\nsurv1_A &lt;- ggsurvplot_combine(fit1,\n                        risk.table = FALSE,  # Disable risk table here\n                        xlab = \"Time in days\",\n                        ggtheme = theme_light(),\n                        legend.labs = c(\"CT\",\"SV\",\"EFS\"),\n                        palette=colors,\n                        title=\"Bras A\")\n\n# Step 2: Generate a Separate Risk Table for \"SV\"\n# Note: This assumes 'sv' is a fit object from survival analysis.\n# If 'sv' is not directly usable, you might need to recreate the survival analysis for SV.\nsv_plot_A &lt;- ggsurvplot(sv_A, risk.table = TRUE,\n                      ggtheme = theme_light(),\n                      # tables.theme = theme(\n                      #   axis.text.x = element_blank(),  # Hide x-axis text\n                      #   axis.ticks.x = element_blank(), # Hide x-axis ticks\n                      #   axis.title.x = element_blank()  # Optionally, hide the x-axis title as well\n                      # ),\n                      risk.table.y.text.col = TRUE,\n                      risk.table.y.text = FALSE,\n                      palette = colors[2])  # Adjust 'colors[2]' as per your color setup\n\n\n# Step 3: Combine the Plot and the Risk Table Manually\n# Option 1: Using gridExtra\ncombined_plot &lt;- grid.arrange(surv1_A$plot, sv_plot_A$table, ncol = 1,heights = c(6, 2))\n\n\n\n\n\n\n\n# Or Option 2: Using patchwork (Uncomment to use)\n# combined_plot &lt;- g$plot / sv_plot$table\n\n# Print or save the combined plot\nprint(combined_plot)\n\nTableGrob (2 x 1) \"arrange\": 2 grobs\n  z     cells    name           grob\n1 1 (1-1,1-1) arrange gtable[layout]\n2 2 (2-2,1-1) arrange gtable[layout]\n\n\n\n\n\n\nlibrary(survminer)\nlibrary(survival)\nlibrary(gridExtra) # or library(patchwork)\n\n# Assuming ct, sv, and efs are survival objects\nfit2 &lt;- list(CT = ct_A, SV = sv_A, EFS = efs_A, CT2 = ct2_A)\n\n# custom color palette for CT, SV, EFS\ncolors &lt;- c(\"#A31621\", \"#053C5E\", \"#F3A712\", \"#1F7A8C\", \"#AFB3F7\")\n\n# Step 1: Generate the Combined Survival Plot without a risk table\nsurv2_A &lt;- ggsurvplot_combine(fit2,\n                        risk.table = FALSE,  # Disable risk table here\n                        xlab = \"Time in days\",\n                        ggtheme = theme_light(),\n                        legend.labs = c(\"CT\",\"SV\",\"EFS\",\"CT2\"),\n                        palette=colors,\n                        title=\"Bras A\")\n\n\n\n# Step 3: Combine the Plot and the Risk Table Manually\n# Option 1: Using gridExtra\ncombined_plot &lt;- grid.arrange(surv2_A$plot, sv_plot_A$table, ncol = 1,heights = c(6, 2))\n\n\n\n\n\n\n\n# Print or save the combined plot\nprint(combined_plot)\n\nTableGrob (2 x 1) \"arrange\": 2 grobs\n  z     cells    name           grob\n1 1 (1-1,1-1) arrange gtable[layout]\n2 2 (2-2,1-1) arrange gtable[layout]\n\n\n\n# fit2&lt;-list(CT = ct, CT2 = ct2, SV=sv, EFS=efs)\n# library(survminer)\n# t&lt;-ggsurvplot_combine(fit,\n#           risk.table = TRUE,                  # Add risk table\n#    xlab = \"Time in days\",   # customize X axis label.\n#    ggtheme = theme_light(), # customize plot and risk table with a therme.\n#  risk.table.y.text.col = T, # colour risk table text annotations.\n#   risk.table.y.text = FALSE,\n#   legend.labs = c(\"CT\", \"CT2\",\"SV\",\"EFS\"))\n# t\n\n\n\n\n\n\n\n\nlibrary(survminer)\nlibrary(survival)\nlibrary(gridExtra) # or library(patchwork)\n\n# Assuming ct, sv, and efs are survival objects\nfit1 &lt;- list(CT = ct_B, SV = sv_B, EFS = efs_B)\n\n# custom color palette for CT, SV, EFS\ncolors &lt;- c(\"#A31621\", \"#053C5E\", \"#F3A712\", \"#1F7A8C\", \"#AFB3F7\")\n\n# Step 1: Generate the Combined Survival Plot without a risk table\nsurv1_B &lt;- ggsurvplot_combine(fit1,\n                        risk.table = FALSE,  # Disable risk table here\n                        xlab = \"Time in days\",\n                        ggtheme = theme_light(),\n                        legend.labs = c(\"CT\",\"SV\",\"EFS\"),\n                        palette=colors,\n                        title=\"Bras B\")\n\n# Step 2: Generate a Separate Risk Table for \"SV\"\n# Note: This assumes 'sv' is a fit object from survival analysis.\n# If 'sv' is not directly usable, you might need to recreate the survival analysis for SV.\nsv_plot_B &lt;- ggsurvplot(sv_B, risk.table = TRUE,\n                      ggtheme = theme_light(),\n                      # tables.theme = theme(\n                      #   axis.text.x = element_blank(),  # Hide x-axis text\n                      #   axis.ticks.x = element_blank(), # Hide x-axis ticks\n                      #   axis.title.x = element_blank()  # Optionally, hide the x-axis title as well\n                      # ),\n                      risk.table.y.text.col = TRUE,\n                      risk.table.y.text = FALSE,\n                      palette = colors[2])  # Adjust 'colors[2]' as per your color setup\n\n\n# Step 3: Combine the Plot and the Risk Table Manually\n# Option 1: Using gridExtra\ncombined_plot &lt;- grid.arrange(surv1_B$plot, sv_plot_B$table, ncol = 1,heights = c(6, 2))\n\n\n\n\n\n\n\n# Or Option 2: Using patchwork (Uncomment to use)\n# combined_plot &lt;- g$plot / sv_plot$table\n\n# Print or save the combined plot\nprint(combined_plot)\n\nTableGrob (2 x 1) \"arrange\": 2 grobs\n  z     cells    name           grob\n1 1 (1-1,1-1) arrange gtable[layout]\n2 2 (2-2,1-1) arrange gtable[layout]\n\n\n\n\n\n\nlibrary(survminer)\nlibrary(survival)\nlibrary(gridExtra) # or library(patchwork)\n\n# Assuming ct, sv, and efs are survival objects\nfit2 &lt;- list(CT = ct_B, SV = sv_B, EFS = efs_B, CT2 = ct2_B)\n\n# custom color palette for CT, SV, EFS\ncolors &lt;- c(\"#A31621\", \"#053C5E\", \"#F3A712\", \"#1F7A8C\", \"#AFB3F7\")\n\n# Step 1: Generate the Combined Survival Plot without a risk table\nsurv2_B &lt;- ggsurvplot_combine(fit2,\n                        risk.table = FALSE,  # Disable risk table here\n                        xlab = \"Time in days\",\n                        ggtheme = theme_light(),\n                        legend.labs = c(\"CT\",\"SV\",\"EFS\",\"CT2\"),\n                        palette=colors,\n                        title=\"Bras B\")\n\n\n\n# Step 3: Combine the Plot and the Risk Table Manually\n# Option 1: Using gridExtra\ncombined_plot &lt;- grid.arrange(surv2_B$plot, sv_plot_B$table, ncol = 1,heights = c(6, 2))\n\n\n\n\n\n\n\n# Print or save the combined plot\nprint(combined_plot)\n\nTableGrob (2 x 1) \"arrange\": 2 grobs\n  z     cells    name           grob\n1 1 (1-1,1-1) arrange gtable[layout]\n2 2 (2-2,1-1) arrange gtable[layout]"
  },
  {
    "objectID": "projets/LAL/partition.html#computing-rmst-for-each-arm",
    "href": "projets/LAL/partition.html#computing-rmst-for-each-arm",
    "title": "partition",
    "section": "Computing rmst for each arm",
    "text": "Computing rmst for each arm\n\n# Charger les packages nécessaires\nlibrary(survival)\nlibrary(survRM2) # Assurez-vous que le package rmst2 est installé pour accéder à cette fonction\nlibrary(boot)\n\n### 0 = Int arm A, 1 = Light arm B\nt_censure &lt;- 60#min(64.3285421,60.9774127) # 20% de censure\n\n# For ct_rmst\nct_rmst&lt;-rmst2(qaly$delfinct, qaly$finct, as.factor(as.numeric(qaly$R1)-1), covariates = NULL, alpha = 0.05)\n\nct_rmstB&lt;-ct_rmst$RMST.arm1$rmst[1]\nct_rmstA&lt;-ct_rmst$RMST.arm0$rmst[1]\n\n# For ct2_rmst\nct2_rmst&lt;-rmst2(qaly$delfinct2, qaly$finct2, as.factor(as.numeric(qaly$R1)-1), covariates = NULL, alpha = 0.05)\nct2_rmstB &lt;- ct2_rmst$RMST.arm1$rmst[1]  # Assuming arm1 corresponds to arm B\nct2_rmstA &lt;- ct2_rmst$RMST.arm0$rmst[1]  # Assuming arm0 corresponds to arm A\n\n# For sv_rmst\nsv_rmst&lt;-rmst2(qaly$suivi, as.numeric(as.character(qaly$dc)), as.factor(as.numeric(qaly$R1)-1), tau = t_censure, covariates = NULL, alpha = 0.05)\nsv_rmstB &lt;- sv_rmst$RMST.arm1$rmst[1]\nsv_rmstA &lt;- sv_rmst$RMST.arm0$rmst[1]\n\n# For efs_rmst\nefs_rmst&lt;-rmst2(qaly$delpfs, as.numeric(as.character(qaly$pfs)), as.factor(as.numeric(qaly$R1)-1), tau = t_censure, covariates = NULL, alpha = 0.05)\nefs_rmstB &lt;- efs_rmst$RMST.arm1$rmst[1]\nefs_rmstA &lt;- efs_rmst$RMST.arm0$rmst[1]\n\n# Calculer TOX, TWiST, et REL en utilisant les RMST calculés\ntox1_A = ct_rmstA\ntox2_A = ct2_rmstA\ntwist1_A = efs_rmstA - ct_rmstA\ntwist2_A= efs_rmstA - ct2_rmstA\nrel_A = sv_rmstA - efs_rmstA\n\n\n# Calculer TOX, TWiST, et REL en utilisant les RMST calculés\ntox1_B = ct_rmstB\ntox2_B = ct2_rmstB\ntwist1_B = efs_rmstB - ct_rmstB\ntwist2_B= efs_rmstB - ct2_rmstB\nrel_B = sv_rmstB - efs_rmstB\n\n\nBoostrapping\n\nTOX/TWISt/REL & their differences between group\n\n# Fonction pour calculer les RMST sur un échantillon bootstrapé\n\ncalculate_rmst &lt;- function(data, indices, uTWiST, uTOX, uREL, t_censure) {\n  sampled_data &lt;- data[indices, ]\n  \n  # Calculer les RMST pour chaque bras et chaque étape\n  ct_rmst &lt;- rmst2(sampled_data$delfinct, sampled_data$finct, as.factor(as.numeric(sampled_data$R1)-1), covariates = NULL, alpha = 0.05)\n  ct_rmstB &lt;- ct_rmst$RMST.arm1$rmst[1]\n  ct_rmstA &lt;- ct_rmst$RMST.arm0$rmst[1]\n  \n  ct2_rmst &lt;- rmst2(sampled_data$delfinct2, sampled_data$finct2, as.factor(as.numeric(sampled_data$R1)-1), covariates = NULL, alpha = 0.05)\n  ct2_rmstB &lt;- ct2_rmst$RMST.arm1$rmst[1]\n  ct2_rmstA &lt;- ct2_rmst$RMST.arm0$rmst[1]\n  \n  sv_rmst &lt;- rmst2(sampled_data$suivi, as.numeric(as.character(sampled_data$dc)), as.factor(as.numeric(sampled_data$R1)-1), tau = t_censure, covariates = NULL, alpha = 0.05)\n  sv_rmstB &lt;- sv_rmst$RMST.arm1$rmst[1]\n  sv_rmstA &lt;- sv_rmst$RMST.arm0$rmst[1]\n  \n  efs_rmst &lt;- rmst2(sampled_data$delpfs, as.numeric(as.character(sampled_data$pfs)), as.factor(as.numeric(sampled_data$R1)-1), tau = t_censure, covariates = NULL, alpha = 0.05)\n  efs_rmstB &lt;- efs_rmst$RMST.arm1$rmst[1]\n  efs_rmstA &lt;- efs_rmst$RMST.arm0$rmst[1]\n  \n  # Calculer les tox, twists et rels pour chaque bras et chaque étape\n  tox1_A &lt;- ct_rmstA\n  tox2_A &lt;- ct2_rmstA\n  twist1_A &lt;- efs_rmstA - ct_rmstA\n  twist2_A &lt;- efs_rmstA - ct2_rmstA\n  rel_A &lt;- sv_rmstA - efs_rmstA\n  \n  tox1_B &lt;- ct_rmstB\n  tox2_B &lt;- ct2_rmstB\n  twist1_B &lt;- efs_rmstB - ct_rmstB\n  twist2_B &lt;- efs_rmstB - ct2_rmstB\n  rel_B &lt;- sv_rmstB - efs_rmstB\n  \n  # Calculer les différences entre les bras\n  tox1_diff &lt;- tox1_A - tox1_B\n  tox2_diff &lt;- tox2_A - tox2_B\n  twist1_diff &lt;- twist1_A - twist1_B\n  twist2_diff &lt;- twist2_A - twist2_B\n  rel_diff &lt;- rel_A - rel_B\n  \n  # Calculer QTWiST pour chaque partition\n  q_twist1_A &lt;- (uTOX * tox1_A) + (uTWiST * twist1_A) + (uREL * rel_A)\n  q_twist1_B &lt;- (uTOX * tox1_B) + (uTWiST * twist1_B) + (uREL * rel_B)\n  q_twist1_diff &lt;- q_twist1_A - q_twist1_B\n  \n  q_twist2_A &lt;- (uTOX * (tox1_A + tox2_A)) + (uTWiST * (twist1_A + twist2_A)) + (uREL * rel_A)\n  q_twist2_B &lt;- (uTOX * (tox1_B + tox2_B)) + (uTWiST * (twist1_B + twist2_B)) + (uREL * rel_A)\n  q_twist2_diff &lt;- q_twist2_A - q_twist2_B\n  \n  # Retourner les résultats\n  return(c(\"TOX1 A\" = tox1_A, \"TOX2 A\" = tox2_A, \"TWiST1 A\" = twist1_A, \"TWiST2 A\" = twist2_A, \"REL A\" = rel_A, \n            \"TOX1 B\" = tox1_B, \"TOX2 B\" = tox2_B, \"TWiST1 B\" = twist1_B, \"TWiST2 B\" = twist2_B, \"REL B\" = rel_B, \n            \"TOX1diff\" = tox1_diff, \"TOX2 diff\" = tox2_diff, \"TWiST1 diff\" = twist1_diff, \"twist2_diff\" = twist2_diff, \n            \"REL diff\" = rel_diff,\"q_TWiST1 A\" = q_twist1_A, \"q_TWiST1 B\" = q_twist1_B, \"q_TWiST1 diff\" = q_twist1_diff, \n            \"q_TWiST2 A\" = q_twist2_A, \"q_TWiST2 B\" = q_twist2_B, \"q_TWiST2 diff\" = q_twist2_diff))}\n\n# Appliquer le bootstrap sur le dataset 'qaly'\nt_censure &lt;- min(64.3285421, 60.9774127) # 20% de censure\nqaly_cens&lt;-qaly %&gt;% select(delfinct, finct, delfinct2, finct2, suivi, dc, delpfs, pfs, R1)\nbootstrap_results1 &lt;- censboot(qaly_cens, calculate_rmst, R = 1000, uTWiST = 1, uTOX = 0.5, uREL = 0.5, t_censure = t_censure)\n\nboot_summary&lt;-summary(bootstrap_results1) %&gt;% as.data.frame() \nrownames(boot_summary) &lt;- names(bootstrap_results1$t0)\n\n\n\n\nThreshold analysis for Q-TWIST diff with computed var-cov\n\nresultsboot&lt;-as.data.frame(bootstrap_results1$t) %&gt;%\n  select(\"V1\", \"V3\", \"V5\", \"V6\", \"V8\", \"V10\",\"V11\",\"V13\",\"V15\") %&gt;%\n  rename(toxA = \"V1\", twistA = \"V3\", relA = \"V5\", toxB = \"V6\", twistB = \"V8\", relB = \"V10\",toxDiff=\"V11\", twistDiff=\"V13\", relDiff=\"V15\")\n\n# Attention, nous ne trouvons pas les mêmes var covariances pour les différences de qtwist\nvar(resultsboot$toxDiff)\n\n[1] 0.03132193\n\nvar(resultsboot$twistDiff)\n\n[1] 13.78092\n\nvar(resultsboot$relDiff)\n\n[1] 4.682029\n\ncov(resultsboot$toxDiff, resultsboot$twistDiff)\n\n[1] 0.1347315\n\ncov(resultsboot$toxDiff, resultsboot$relDiff)\n\n[1] 0.03890453\n\ncov(resultsboot$twistDiff, resultsboot$relDiff)\n\n[1] -4.404759\n\n\n\nvariance_QTwist &lt;- function(U_TOX, U_REL, Var_TOX, Var_TWIST, Var_REL, Cov_TOX_TWIST, Cov_TOX_REL, Cov_TWIST_REL) {\n  variance &lt;- (U_TOX^2) * Var_TOX + Var_TWIST + (U_REL^2) * Var_REL\n  variance &lt;- variance + U_TOX * Cov_TOX_TWIST + U_TOX * U_REL * Cov_TOX_REL + U_REL * Cov_TWIST_REL\n  return(variance)\n}\n\n# Matrice de variance-covariance des estimateurs bootstrappés\n\n# Var_TOX &lt;- var(resultsdiff$toxDiff)\n# Var_TWIST &lt;- var(resultsdiff$twistDiff)\n# Var_REL &lt;- var(resultsdiff$relDiff)\n# Cov_TOX_TWIST &lt;- cov(resultsdiff$toxDiff, resultsdiff$twistDiff)\n# Cov_TOX_REL &lt;- cov(resultsdiff$toxDiff, resultsdiff$relDiff)\n# Cov_TWIST_REL &lt;- cov(resultsdiff$twistDiff, resultsdiff$relDiff)\n\nVar_TOX &lt;- 0.175**2\nVar_TWIST &lt;- 3.73**2\nVar_REL &lt;- 2.06**2\nCov_TOX_TWIST &lt;- 3.85e-3\nCov_TOX_REL &lt;- 2.12e-3\nCov_TWIST_REL &lt;- -0.48\n\n\n# QTWiST\n\nresults &lt;- data.frame(uTOX=numeric(), uREL=numeric(), QTWiST1_Diff=numeric(), Lower=numeric(), Upper=numeric())\nuTWiST=1\nfor (uTOX in seq(0, 1, by=0.25)) {\n  for (uREL in seq(0, 1, by=0.25)) {\n    \n  # Calculer les RMST pour chaque bras et chaque étape\n  ct_rmst &lt;- rmst2(qaly$delfinct, qaly$finct, as.factor(as.numeric(qaly$R1)-1), covariates = NULL, alpha = 0.05)\n  ct_rmstB &lt;- ct_rmst$RMST.arm1$rmst[1]\n  ct_rmstA &lt;- ct_rmst$RMST.arm0$rmst[1]\n  \n  # ct2_rmst &lt;- rmst2(qaly$delfinct2, qaly$finct2, as.factor(as.numeric(qaly$R1)-1), covariates = NULL, alpha = 0.05)\n  # ct2_rmstB &lt;- ct2_rmst$RMST.arm1$rmst[1]\n  # ct2_rmstA &lt;- ct2_rmst$RMST.arm0$rmst[1]\n  \n  sv_rmst &lt;- rmst2(qaly$suivi, as.numeric(as.character(qaly$dc)), as.factor(as.numeric(qaly$R1)-1), tau = t_censure, covariates = NULL, alpha = 0.05)\n  sv_rmstB &lt;- sv_rmst$RMST.arm1$rmst[1]\n  sv_rmstA &lt;- sv_rmst$RMST.arm0$rmst[1]\n  \n  efs_rmst &lt;- rmst2(qaly$delpfs, as.numeric(as.character(qaly$pfs)), as.factor(as.numeric(qaly$R1)-1), tau = t_censure, covariates = NULL, alpha = 0.05)\n  efs_rmstB &lt;- efs_rmst$RMST.arm1$rmst[1]\n  efs_rmstA &lt;- efs_rmst$RMST.arm0$rmst[1]\n  \n  # Calculer les tox, twists et rels pour chaque bras et chaque étape\n  tox1_A &lt;- ct_rmstA\n  twist1_A &lt;- efs_rmstA - ct_rmstA\n  rel_A &lt;- sv_rmstA - efs_rmstA\n  \n  tox1_B &lt;- ct_rmstB\n  twist1_B &lt;- efs_rmstB - ct_rmstB\n  rel_B &lt;- sv_rmstB - efs_rmstB\n  \n  # Calculer les différences entre les bras\n  tox1_diff &lt;- tox1_A - tox1_B\n  twist1_diff &lt;- twist1_A - twist1_B\n  rel_diff &lt;- rel_A - rel_B\n  \n  # Calculer QTWiST pour chaque partition\n  q_twist1_A &lt;- (uTOX * tox1_A) + (uTWiST * twist1_A) + (uREL * rel_A)\n  q_twist1_B &lt;- (uTOX * tox1_B) + (uTWiST * twist1_B) + (uREL * rel_B)\n  q_twist1_diff &lt;- q_twist1_A - q_twist1_B\n  \n  variance_qTwist &lt;- variance_QTwist(uTOX, uREL, Var_TOX, Var_TWIST, Var_REL, Cov_TOX_TWIST, Cov_TOX_REL, Cov_TWIST_REL)\n  ic_qTwist_inf &lt;- q_twist1_diff - 1.96*sqrt(variance_qTwist)\n  ic_qTwist_sup&lt;-q_twist1_diff + 1.96*sqrt(variance_qTwist)\n  \n      # Ajouter les résultats dans le dataframe\n  results &lt;- rbind(results, data.frame(\n      uTOX=uTOX, \n      uREL=uREL, \n      QTWiST1_Diff=q_twist1_diff, \n      Lower=ic_qTwist_inf, # Lower CI\n      Upper=ic_qTwist_sup  # Upper CI\n    ))\n  }\n}\n\n\n# Filtrer les résultats où l'intervalle de confiance contient 0\ndiff_null &lt;- results %&gt;% filter(Lower &lt;= 0, Upper &gt;= 0) \n\n# Tracer le graphique en utilisant uTOX sur l'axe x et QTWiST1_Diff sur l'axe y, et uREL en tant que couleur\nplot(diff_null$uTOX, diff_null$QTWiST1_Diff, pch = 19, col = \"red\", xlab = \"uTOX\", ylab = \"QTWiST1_Diff\", main = \"Confidence interval contains 0\")\n# Ajouter uREL comme troisième axe\npoints(diff_null$QTWiST1_Diff, diff_null$uTOX, pch = 19, col = \"blue\")\n# Légende\nlegend(\"bottomright\", legend = c(\"QTWiST1_Diff\", \"uREL\"), col = c(\"red\", \"blue\"), pch = 19)\n\n\n\n\n\n\n\n# library(ggplot2)\n# # Créer le graphique\n# ggplot(diff_null, aes(x = uTOX, y = uREL)) +\n#   geom_point() + \n#   scale_x_continuous(name = \"Utility coefficient after relapse (μ_REL)\", limits = c(0, 1)) +\n#   scale_y_continuous(name = \"Utility coefficient for toxicity (μ_TOX)\", limits = c(0, 1)) +\n#   geom_abline(diffQTWiST1_Diff, linetype = \"dashed\")\n\n\n\nThreshold analysis for Q-TWIST diff with bootstrapp var-cov\n\ndelta_qtwist &lt;- function(data, indices, uTWiST, uTOX, uREL, t_censure) {\n  sampled_data &lt;- data[indices, ]\n  \n  # Calculer les RMST pour chaque bras et chaque étape\n  ct_rmst &lt;- rmst2(sampled_data$delfinct, sampled_data$finct, as.factor(as.numeric(sampled_data$R1)-1), covariates = NULL, alpha = 0.05)\n  ct_rmstB &lt;- ct_rmst$RMST.arm1$rmst[1]\n  ct_rmstA &lt;- ct_rmst$RMST.arm0$rmst[1]\n  \n  # ct2_rmst &lt;- rmst2(sampled_data$delfinct2, sampled_data$finct2, as.factor(as.numeric(sampled_data$R1)-1), covariates = NULL, alpha = 0.05)\n  # ct2_rmstB &lt;- ct2_rmst$RMST.arm1$rmst[1]\n  # ct2_rmstA &lt;- ct2_rmst$RMST.arm0$rmst[1]\n  \n  sv_rmst &lt;- rmst2(sampled_data$suivi, as.numeric(as.character(sampled_data$dc)), as.factor(as.numeric(sampled_data$R1)-1), tau = t_censure, covariates = NULL, alpha = 0.05)\n  sv_rmstB &lt;- sv_rmst$RMST.arm1$rmst[1]\n  sv_rmstA &lt;- sv_rmst$RMST.arm0$rmst[1]\n  \n  efs_rmst &lt;- rmst2(sampled_data$delpfs, as.numeric(as.character(sampled_data$pfs)), as.factor(as.numeric(sampled_data$R1)-1), tau = t_censure, covariates = NULL, alpha = 0.05)\n  efs_rmstB &lt;- efs_rmst$RMST.arm1$rmst[1]\n  efs_rmstA &lt;- efs_rmst$RMST.arm0$rmst[1]\n  \n  # Calculer les tox, twists et rels pour chaque bras et chaque étape\n  tox1_A &lt;- ct_rmstA\n  twist1_A &lt;- efs_rmstA - ct_rmstA\n  rel_A &lt;- sv_rmstA - efs_rmstA\n  \n  tox1_B &lt;- ct_rmstB\n  twist1_B &lt;- efs_rmstB - ct_rmstB\n  rel_B &lt;- sv_rmstB - efs_rmstB\n  \n  # Calculer les différences entre les bras\n  tox1_diff &lt;- tox1_A - tox1_B\n  twist1_diff &lt;- twist1_A - twist1_B\n  rel_diff &lt;- rel_A - rel_B\n  \n  # Calculer QTWiST pour chaque partition\n  q_twist1_A &lt;- (uTOX * tox1_A) + (uTWiST * twist1_A) + (uREL * rel_A)\n  q_twist1_B &lt;- (uTOX * tox1_B) + (uTWiST * twist1_B) + (uREL * rel_B)\n  q_twist1_diff &lt;- q_twist1_A - q_twist1_B\n  \n  \n  # Retourner les résultats\n  return(\"q_TWiST1 diff\" = q_twist1_diff)}\n\n\n# delta_qtwist_boot &lt;- censboot(qaly_cens, delta_qtwist, R = 1000, uTWiST = 1, uTOX = 0.5, uREL = 0.5, t_censure = t_censure)\n# boot.ci(delta_qtwist_boot,type=\"norm\")$normal\nlibrary(boot)\n# QTWiST\nresults1 &lt;- data.frame(uTOX=numeric(), uREL=numeric(), QTWiST1_Diff=numeric(), Lower=numeric(), Upper=numeric())\n\nfor (uTOX in seq(0, 1, by=0.25)) {\n  for (uREL in seq(0, 1, by=0.25)) {\n    # Bootstrap analysis\n    delta_qtwist_boot &lt;- censboot(qaly_cens, delta_qtwist, R = 1000, uTWiST = uTWiST, uTOX = uTOX, uREL = uREL, t_censure = t_censure)\n    boot_results &lt;- boot.ci(delta_qtwist_boot, type=\"norm\")\n    boot_results$normal[2]\n    # Ajouter les résultats dans le dataframe\nresults1 &lt;- rbind(results1, data.frame(\n      uTOX=uTOX, \n      uREL=uREL, \n      QTWiST1_Diff=q_twist1_diff, \n      Lower=boot_results$normal[2], # Lower CI\n      Upper=boot_results$normal[3]  # Upper CI\n))\n    }\n}"
  },
  {
    "objectID": "projets/LAL/partition.html#export-results-in-latex",
    "href": "projets/LAL/partition.html#export-results-in-latex",
    "title": "partition",
    "section": "Export results in latex",
    "text": "Export results in latex\n\n# if (!require(\"xtable\")) install.packages(\"xtable\")\n# library(xtable)\n# \n# rownames(results)\n# print(xtable(results), type = \"latex\", include.rownames = FALSE)\n# # %&gt;% \n#   as_gt()%&gt;% \n#   gtsave(\"name.text\")"
  },
  {
    "objectID": "articles/GDR/risque_def.html",
    "href": "articles/GDR/risque_def.html",
    "title": "Le risque, qu’est ce que c’est ?",
    "section": "",
    "text": "En finance, le risque peut être défini comme la survenance d’un événement incertain qui peut avoir des conséquences négatives sur le bilan, ou le compte de résultat d’une banque. Par exemple, une fraude aura un impact négative sur la réputation d’une banque ce qui peut entrainer des pertes importants ayant un impact négatif sur le résultat net de celle-ci. En économie, le risque est un événement probabilisable tandis que l’incertitude est non probabilisable.\nNous pouvons caractériser 3 grands types de risques établis par le comité de Bâle qui veille au renforcement et à la stabilité du système financier. (rangés par ordre d’importance pour une banque universelle ) :\nPour les distinguer, il faut dissocier la cause de la conséquence. Toutefois, certains risque sont difficiles à distinguer. Ils se trouvent à la frontière entre le risque de marché, de crédit et le risque opérationnel.\nIl est important de noter que le but d’une banque n’est pas de prendre le moins de risque, mais d’atteindre une rentabilité maximale pour un risque donné. La théorie financière nous apprend que seul le risque est rémunéré. La banque procède donc à une arbitrage entre risque et rentabilité. C’est pourquoi la gestion des risques est un élément clé de la stratégie de décision de la banque. La mesure du risque intervient pour calculer les fonds propres nécessaires pour assurer chaque opération financière. Elle est indispensable pour calculer des mesures de performance."
  },
  {
    "objectID": "articles/GDR/risque_def.html#les-mesures-de-risque",
    "href": "articles/GDR/risque_def.html#les-mesures-de-risque",
    "title": "Le risque, qu’est ce que c’est ?",
    "section": "Les mesures de risque",
    "text": "Les mesures de risque\nEn 1999, Artzner et al. ont défini les propriétés que devrait avoir une mesure de risque \\(\\mathcal{R}\\)  cohérente. Une mesure de risque est une fonction qui permet de quantifier le risque d’un portefeuille. Elle est cohérente si elle satisfait les propriétés suivantes :\n\nsous-additivité : \\(\\mathcal{R}(X+Y) \\leq \\mathcal{R}(X) + \\mathcal{R}(Y)\\)\nhomogénéité positive : \\(\\mathcal{R}(\\lambda X) = \\lambda \\mathcal{R}(X), \\quad \\lambda \\geq 0\\)\ninvariance par translation : \\(\\mathcal{R}(X+c) = \\mathcal{R}(X) - c, \\quad c \\in \\mathbb{R}\\)\nmonotonie : \\(F_1(x) \\leq F_2(x) \\Rightarrow \\mathcal{R}(X) \\leq \\mathcal{R}(Y)\\)\n\nLa sous-additivité signifie que le risque d’un portefeuille est inférieur ou égal à la somme des risques des actifs qui le composent. Ce phénomène est appelé effet de diversification. En effet, la diversification permet de réduire le risque d’un portefeuille en investissant dans des actifs non corrélés. Ainsi, en agrégeant deux porte-feuilles, il n’y a pas de création de risque supplémentaire.\nL’homogénéité positive signifie que le risque d’un portefeuille est proportionnel à la taille du portefeuille. Cette propriété ignore les problèmes de liquidité.\nL’invariance par translation signifie que l’addition au portefeuille initiale un montant sûr rémunéré au taux sans risque diminue la mesure du risque. En particulier, \\(\\mathcal{R}(L+\\mathcal{R}(L)) = 0\\). Ainsi pour couvrir un portefeuille, il suffit d’immobiliser des fonds propres égaux à la mesure du risque.\nLa monotonie signifie que le risque d’un portefeuille est inférieur ou égal au risque d’un autre portefeuille si la distribution de probabilité de la perte potentielle du premier portefeuille est inférieure ou égale à celle du deuxième portefeuille. Celà traduit l’ordre stochastique des distributions de pertes potentielles des portefeuilles.\n\nLa valeur en risque (VaR)\nSachant la valeur d’un portefeuille à un instant t donné, le risque est la variation négative de ce portefeuille dans le futur. Le risque se caractérisait donc par une perte relativfe (par rapport à la valeur initiale du portefeuille à un instant t). Pendant très longtemps, les banques utilisaient la volatilité (écart-type) pour mesurer le risque. Cette mesure du risque a notamment beaucoup évoluée et celle qui est la plus répandue est la Value at Risk (VaR). Statistiquement parlant, la VaR est le quantile de la perte potentielle d’un portefeuille à un horizon \\(h\\) donné et un seuil de confiance \\(\\alpha\\) :\n\\[VaR = F^{-1}(\\alpha)=inf(t \\in \\mathbb{R}, F(t) \\geq \\alpha),\\]\noù F est la distribution de probabilité de la perte potentielle du portefeuille.\nPar exemple, une VaR à \\(\\alpha=1\\%\\) de 1 million d’euros signifie que la probabilité que la banque perde plus de 1 million d’euros est égale à 1%. Autrement dit, la banque a 99% de chance de ne pas perdre plus de 1 million d’euros sur une période donnée (C’est la perte maximale encourue par la banque avec un intervalle de confiance à 99%). Nous allons préférer la deuxième formulation de l’interprétation.\nDeux éléments sont nécessaires pour déterminer la VaR : la distribution de la perte potentielle et le seuil de confiance. La distribution de la perte potentielle est souvent inconnue. Nous pouvons assimiler le seuil de confiance à un indicateur de tolérance pour le risque. Une couverture à 99% est beaucoup plus exigente et donc plus coûteuse qu’une couverture à 95%. En ce qui concerne la distribution de la perte potentielle, il faudrait définir l’horizon h. Par exemple, une couverture à 1 jour est moins coûteuse qu’une couverture à 1 mois. C’est la combinaison de ces deux éléments qui détermine le degré de la couverture qui peut être exprimé en temps de retour 1 \\(t°\\)qui est la durée moyenne entre deux dépassements de la VaR. Il permet de caractériser la rareté d’un évènement (dont la probabilité d’occurence est petite)\n\\[t°= \\frac{h}{1-\\alpha}\\]\nLorsqu’on entend parler de gestion de risque décennal, celà revient à considérer une valeur en risque (VaR) journalière (horizon = 1 jour) pour un seuil de confiance \\(\\alpha=99.96\\%\\).\n\nCritiques de la VaR\nLa VaR est une mesure de risque non cohérente car elle ne respecte pas la propriété de sous-additivité. De nombreux professionnels recommanderaient alors l’utilisation de la CVAR (Expected Shortfall - ES) qui est une mesure de risque cohérente. La CVAR est l’espérance de la perte au delà de la VaR. Toutefois, la VaR reste une mesure de risque très utilisée en pratique, qui ne respecte pas la propriété de sous-additivité que dans certains cas, par exemple lorsque les fonctions de distribution des facteurs de risque ne sont pas continues et lorsque les probabilités sont principalement localisées dans les quantiles extrêmes.\n\n\n\nD’autres mesures de risque\nD’autres mesures peuvent être définis comme celle de la perte exceptionnelle (Unexpected Loss - UL) définie par : \\[\\mathcal{R}=VaR(\\alpha)-\\mathbb{E}[L],\\] où L est la distribution de la perte potentielle.\nIl s’agit là de la différence entre la VaR et la perte moyenne (expected loss - EL). Il y a également le regret espéré défini par :\n\\[\\mathcal{R}=\\mathbb{E}[L|L\\geq H]\\] avec H un seuil donné représentant le montant de la perte tolérable par l’institut financier. Cette mesure de risque est donc la moyenne des pertes non supportables par l’institut financier. Lorsque H est endogène, c’est-à-dire dépendant de la distribution de la perte potentielle, et égale à la VaR, on obtient la Var conditionnelle CVAR (Expected Shortfall - ES) qui est l’espérance de la perte au delà de la VaR.:\n\\[\\mathcal{R}=\\mathbb{E}[L|L\\geq F^{-1}(\\alpha)]\\]\nLa CVAR est par ailleurs un cas particulier des mesures LPM (Lower Partial Moment) \\(\\mathcal{R}=\\mathbb{E}[max(0,L-H)^m]\\) avec \\(m=1\\). Ce sont des mesures de risque qui prennent en compte les pertes au delà d’un certain seuil. La CVAR est une mesure de risque plus conservatrice que la VaR car elle prend en compte les pertes au delà de la VaR. Lorsque H=0 est m=2, nous obtenons la variance des pertes sans prendre en compte les gains : c’est la semi variance."
  },
  {
    "objectID": "articles/GDR/risque_def.html#footnotes",
    "href": "articles/GDR/risque_def.html#footnotes",
    "title": "Le risque, qu’est ce que c’est ?",
    "section": "Footnotes",
    "text": "Footnotes\n\n\npériode de retour doit être interprétée comme la probabilité statistique qu’un évènement se produise↩︎"
  },
  {
    "objectID": "articles/GDR/bilan_entreprise.html",
    "href": "articles/GDR/bilan_entreprise.html",
    "title": "Comment fonctionne le bilan et le compte de résultat d’une entreprise",
    "section": "",
    "text": "L’analyse financière constitue l’ensemble des outils permettant de donner un avis objectif d’une organisation (entreprises, fondations, etc.) sur la santé finanière et les risques financiers auxquels elle sera confrontée. Il s’agit de determiner quels sont les critères d’une santé financière, qu’est le risque, comment formalise-t-on le risque, comment le mesure-t-on et comment le gère-t-on.\nOfficiellement, il existe deux documents comptables qui permettent de faire une analyse financière d’une entreprise. Il s’agit du bilan et du compte de résultat. Ces deux documents sont complémentaires et permettent de donner une vision globale de la situation financière de l’entreprise. Comprendre comment ils fonctionnent permet de mieux appréhender la situation financière d’une banque.\nLe bilan, issu de la loi comptable, est un document qui permet de faire un état des lieux de la situation patrimoniale de l’entreprise à un moment donné. Il est composé de deux parties : l’actif et le passif. L’actif ou l’emploi regroupe l’ensemble des biens et des droits de l’entreprise tandis que le passif regroupe l’ensemble des ressources de l’entreprise (d’où vient l’argent et où peut-on s’en procurer). Le bilan est équilibré en valeur nette, c’est-à-dire que l’actif est égal au passif.\nLe compte de résultats, quant à lui, est un document qui permet de faire un état des lieux des performances de l’entreprise sur une période donnée (il résume les bénéfices ou pertes générées). Il est composé du détail des produits et des charges de l’entreprise. Les produits sont les éléments qui génèrent des revenus pour l’entreprise tandis que les charges sont les éléments qui génèrent des dépenses pour l’entreprise. Le compte de résultat alimente par ailleurs la partie “résultat de l’exercice” du bilan comptable.\nLe coeur de l’entreprise à analyser comme ressources supplémentaires dans le compte de résultat est l’ensembles des charges financières & exceptionnelles ainsi que l’ensemble des produits d’exploitation et financiers. Ces éléments clés permettent de déterminer la rentabilité de l’entreprise. En effet, si les charges sont supérieures aux produits, l’entreprise est en perte. Si les produits sont supérieurs aux charges, l’entreprise est en bénéfice.\nIl est important de noter que ces deux documents sont complémentaires et permettent de donner une vision globale de la situation financière de l’entreprise."
  },
  {
    "objectID": "articles/GDR/bilan_entreprise.html#sec-bilan-fonctionnel",
    "href": "articles/GDR/bilan_entreprise.html#sec-bilan-fonctionnel",
    "title": "Comment fonctionne le bilan et le compte de résultat d’une entreprise",
    "section": "Le bilan fonctionnel",
    "text": "Le bilan fonctionnel\nLe bilan comptable, tel que construit, ne permet pas d’analyser la situation financière d’une entreprise, il faut donc le remodeler en un bilan “fonctionnel” pour pouvoir l’analyser. Le bilan fonctionnel est un document qui permet de faire un état des lieux de la situation financière de l’entreprise en fonction de son activité, et classe le bilan comptable en cycle.\n\nBilan fonctionnel\n\n\n\n\n\n\nCycle d’investissement à long terme\nEmplois stables\n\nactifs immobilisés en valeur brute\n\nCycle de financement à long terme\nRessources stables\n\nCapitaux propres,\nEmprunts à long terme,\nAmortissements et dépréciation,\nProvisions pour risques\n\n\n\nCycle d’exploitation\nEmplois d’exploitation\n\nStocks et encours\nCréances\n\nCycle d’exploitation\nRessources d’exploitation\n\nDettes circulantes\n\n\n\nTrésorerie active\n\nDisponibilités\n\nTrésorerie passive\n\nDécouverts bancaires\n\n\n\n\nLes ressources stables font référence aux ressources saines du bilan etfont face aux emplois stables. La trésorerie passive fait référence aux découverts bancaires. Il est important de souligner qu’une trésorerie passive est perçue négativement dans le bilan fonctionnel. En effet, une trésorerie passive signifie que l’entreprise a des dettes à court terme qui ne sont pas couvertes par des actifs à court terme d’où la nécessité d’avoir des découverts bancaires.\nNb : La provision pour le risque peuve être considérée comme une ressource stable ou une ressource d’exploitation en fonction de l’entreprise. Tout dépend de la longevité des provisions.\n\nEquilibre financier\nNous dirons qu’il y a équilibre financier lorsque :\n\nLes emplois stables soient entièrement financés par les ressources stables.\nLes ressources stables financent largement les emplois stables : il y a necéssité d’un fonds de roulement (\\(FDR= \\text{Ressources stables} - \\text{Emplois stables}\\)) le plus important possible.\n\nLe \\(FDR\\) dépend du cycle d’exploitation (entre autre, la rapidité de rotation des stocks et des créances). Il doit couvrir les besoins de financement du cycle d’exploitation, le besoin en fonds de roulement (\\(BFR = \\text{Stocks, créances} - \\text{Dettes circulantes}\\)). Le juge de paix entre le fonds et besoin de roulement est la trésorerie (\\(\\text{Trésorerie}=FDR-BFR\\)). Si la trésorerie est positive, il y a équilibre financier. Celà signifie que le fonds de roulement est suffisant pour couvrir les besoins de financement du cycle d’exploitation. Lorsqu’il est négatif, il faut trouver des ressources pour financer le cycle d’exploitation. Si la trésorerie est nulle, il y a équilibre financier.\nDans cette situation, il arrive que le fournisseur finance lui-même le cycle d’exploitation de l’entreprise. C’est ce qu’on appelle le crédit fournisseur. Il est important de noter que le crédit fournisseur est une source de financement gratuite pour l’entreprise. C’est le cas des E-commerce où les acteurs encaissent leurs clients avant même d’acheter les stocks auprès des fournisseurs. Dans ces cas, le \\(BFR\\) est donc transformé en ressources en fonds de roulement, celà est une situation très favorable pour l’entreprise et est appelée “crédit inter-entreprises”."
  },
  {
    "objectID": "articles/GDR/bilan_entreprise.html#analyse-du-compte-de-résultat",
    "href": "articles/GDR/bilan_entreprise.html#analyse-du-compte-de-résultat",
    "title": "Comment fonctionne le bilan et le compte de résultat d’une entreprise",
    "section": "Analyse du compte de résultat",
    "text": "Analyse du compte de résultat\nNous pouvons faire les mêmes critiques faites au bilan comptable sur le compte de résultat. En effet, le compte de résultat est conçu de sorte à fournir des informations au seul détenteur du capital, à savoir les actionnaires. Il fait apparaitre uniquement le bénéfice ou la perte. C’est un document d’intérêt pour l’Etat pour déterminer si un pays est en croissance ou en récession. Pour en faire un vrai diagnostic financier, il faut le découper en sous-soldes appelés “soldes intermédiaires de gestion” (SIG). Les SIG permettent de déterminer la rentabilité de l’entreprise, sa capacité d’autofinancement, sa capacité de remboursement, sa capacité de financement, etc.\nIl existe 9 soldes intermédiaires de gestion :\n\nLa marge commerciale\nLa production\nLa valeur ajoutée\nL’excédent brut d’exploitation (EBE)\nLe résultat d’exploitation\nLe résultat courant avant impôt\nLe résultat exceptionnel\nLe résultat net\nLa plus ou moins value de cession\n\nSelon la théorie de prise de décisions, il y a deux grands types de décisions : des décisions qui permettent de créer de la riches (Marge co., production et valeur ajoutée) et des décisions qui permettent de distribuer/dépenser de la richesse (EBE, résultat d’exploitation, résultat courant avant impôt, résultat exceptionnel, résultat net et plus ou moins value de cession). Lorsqu’on dépense la riches, il faudrait qu’elle soit bien dépensée.\n\nSoldes de création de richesse\nLes soldes qui contribuent à la création de richesse sont la marge commerciale, la production et la valeur ajoutée :\n\nLa marge commerciale est la différence entre les ventes de marchandises et les achats des marchandises (\\(\\pm\\) les variations de stocks). C’est un solde des entreprises commerciales (par exemple, les supermarchés). Pour une entreprise qui n’ont pas de marchandises, le marge commerciale est nulle.\nLa production de l’exercice est la somme des produits vendus(\\(\\pm\\) les produits stockées) et des produits immobilisées par l’entreprise (certaines entreprises peuvent se vendre des produits à elles-mêmes). C’est un solde des entreprises industrielles.\nLa valeur ajoutée est la richesse créée par l’entreprise. C’est la somme des marges commerciales, de la production de l’exercice moins les consommations sur honoraires (achat de MP, variation de stocks, prestations de services et autres services extérieurs).\n\nLa valeur ajouté est un indicateur très suivi par l’Etat pour déterminer le produit intérieur brut (PIB) afin de déterminer si un pays est en croissance ou en récession. Par ailleurs, la valeur ajoutée divisée par le nombre de salariés permet de déterminer le niveau de technicité de l’entreprise. Plus la valeur ajoutée par salarié est élevée, plus l’entreprise est techniquement avancée.\n\n\nLa richesse dédiée à l’activité économique\nIl existe 5 tiers à qui l’entreprise redistribue la VA (rangée par ordre de priorité) :\n\nLe personnel (à travers les salaires),\nL’Etat (à travers les impôts),\nLe capital technique (via les amortissements),\nLes banques (via les intérêts),\nLes actionnaires ou les associés (via le bénéfice comptable)\n\nLes soldes qui permettent de financer l’activité économique (Etat, personnel, capital technique) sont l’excédent brut d’exploitation et le résultat d’exploitation :\n\nLe solde EBE rémunère le personnel et l’Etat. Il représente la part de la VA qui appartient au monde du capital et est un indicateur de comparaison d’entreprise pour l’Etat. Un EBE positif signifie que l’entreprise est capable de rémunérer le personnel et l’Etat, et donc de financer l’emploi.\n\n\\[\\begin{align*}\nEBE &= \\text{Valeur ajoutée} - \\text{Impôts, tâxes et versements assimilés} \\\\\n&- \\text{(Charges de personnel - Subventions d'exploitation)}\n\\end{align*}\\]\n\nle solde de résultat d’exploitation(EBIT = Earning Before Interest & Taxes) est le plus important des soldes pour les anglos-saxons. Il permet de rémunérer le capital technique (machines etc.) et appartient à tout ceux qui dépendent du capital financier et mesure les performances industrielles et commerciales de l’entreprise.\n\n\\[\\begin{align*}\n\\text{Résultat d'exploitation} &= \\text{EBE} - \\text{Dotations aux amortissements et aux provisions sur immobilisations} \\\\\n&+ \\text{Reprises sur amortissements et provisions} - \\text{Autres charges d'exploitation} \\\\\n&+ \\text{Autres produits d'exploitation}\n\\end{align*}\\]\n\n\nLa richesse dédiée à l’activité financière\nLes soldes qui permettent de financer l’activité financière (banques, actionnaires) sont le résultat courant avant impôt, le résultat exceptionnel, le résultat net et la plus ou moins value de cession :\n\nLe résultat courant avant impôt est le solde qui permet de rémunérer les banques. Il est un indicateur de la capacité de l’entreprise à rembourser ses dettes et est un témoin de l’incidence de la politique financière de l’entreprise sur son résultat. Il faut distinguer les intérêts à long terme et ceux de court terme. Plus ceux ci sont liés à des dettes de court terme (ex. : découverts), on peut dire que l’entreprise est en difficulté financière tandis que l’endettement à long terme est un signe de bonne santé financière, car il est voulu plutôt que subi. Il est calculé comme suit :\n\n\\[\\begin{align*}\n\\text{Résultat courant avant impôt} &= \\text{Résultat d'exploitation} \\\\\n&+ \\text{Produits financiers} - \\text{Charges financières}\n\\end{align*}\\]\n\nLe résultat exceptionnel est le solde qui est le moins analysé car il est souvent lié à des évènements exceptionnels (ex. : vente d’un bien immobilier). Il est calculé comme étant la soustraction des charges exceptionnelles aux produits exceptionnels.\nLe résultat net est le solde qui permet de rémunérer les actionnaires. C’est le solde en bas du compte de résultat. Il est calculé comme suit :\n\n\\[\\begin{align*}\n\\text{Résultat net} &= \\text{Résultat courant avant impôt} + \\text{Résultat exceptionnel} \\\\\n&- \\text{participations des salariés} - \\text{Impôts sur les bénéfices}\n\\end{align*}\\]\n\nLa plus ou moins value de cession est le solde qui intervient lorsqu’une entreprise vend une immobilisation. Ce ratio permet de déterminer si l’entreprise a vendu une immobilisation à un prix supérieur ou inférieur à sa valeur comptable. Celà constitue un temoin d’alerte sur la santé de l’entreprise et permet de déterminer si l’entreprise est en difficulté financière (car rien ne l’oblige à vendre une immobilisation, surtout en dessous de sa valeur comptable)."
  },
  {
    "objectID": "articles/GDR/bilan_entreprise.html#la-capacité-dautofinancement",
    "href": "articles/GDR/bilan_entreprise.html#la-capacité-dautofinancement",
    "title": "Comment fonctionne le bilan et le compte de résultat d’une entreprise",
    "section": "La capacité d’autofinancement",
    "text": "La capacité d’autofinancement\nLa capacité d’autofinancement (CAF) est un indicateur qui permet de déterminer si l’entreprise est capable de financer ses investissements sans recourir à des financements extérieurs. Elle regroupe la capacité à dégager de la liquidité. Il n’y a pas de correspondance entre la trésorerie et le bénéfice. En effet, une entreprise peut être en bénéfice mais en difficulté financière. Pour la calculer, il faut éliminer les sommes non encaissanles et non décaissables (ex. : Dotations, provision, reprise sur amortissements, les écritures exceptionnelles).\nPour passer du bénéfice à la CAF, on ne conserve que les éléments qui sont encaissables et décaissables et est calculée comme suit :\n\\[\\text{CAF} = \\text{EBE} - \\text{Charges décaissables (intérêt bancaire, impôt sur bénéfice)}\\]\nSur la CAF, les actionnaires se servent pour leurs dividendes (le revenu versé par l’entreprise à ses actionnaires une ou plusieurs fois par an). Ainsi, la CAF ne permet pas à elle toute seule de déterminer l’autofinancement de l’entreprise. Dans le cadre légal, dans la limite de 10% du capital, les actionnaires peuvent retirer jusqu’à 95% du bénéfice comptable imposé par l’Etat et garder 5% à l’entreprise. C’est ce qu’on appelle le “dividende légal”. Au delà de 10%, les actionnaires peuvent retirer jusqu’à 100% du bénéfice comptable. C’est ce qu’on appelle le “dividende statutaire”.\nAinsi, l’autofinancement est la somme qui reste de la CAF après le dividende légal ou le dividende statutaire. Par ailleurs, le niveau de dividendes encaissés par les actionnaires détermine la politique d’autofinancement de l’entreprise.\nL’autofinancement est essentiel pour l’entreprise car il permet de:\n\nrembourser les emprunts,\naméliorer la trésorerie,\ncouvrir les risque de l’entreprise (provisions pour risque),\nfinancer l’exploitation (stocks & créances),\nfinancer les investissements (de maintien et croissance)."
  },
  {
    "objectID": "articles/GDR/reglementation_prudentielle.html",
    "href": "articles/GDR/reglementation_prudentielle.html",
    "title": "La réglementation prudentielle",
    "section": "",
    "text": "La réglementation prudentielle a été initiée par le développement des marchés financiers et des chocs alimentés par diverses crises financières. Face à ce constat, les autorités de contrôle bancaire ainsi que les autorités de marché ont pris des décisions pour réguler les marchés. C’est notamment le rôle qu’occupe le Comité de Bâle ou la Commission bancaire, qui ont pour objectif de renforcer la stabilité des marchés financiers (“Comité de Bâle” 2017). En France, l’ACPR (Autorité de Contrôle Prudentiel et de Résolution) et la Banque de France sont membres du Comité de Bâle et participent à ses travaux et décisions.\nIl existe par ailleurs plusieurs textes réglementaires ou documents relatifs au risque de marché. Parmi ces textes, on peut citer le document de référence pour calculer le ratio de solvabilité de la Commission bancaire, intitulé “Modalités de calcul et de publication des ratios prudentiels dans le cadre de la CRDIV et exigence de MREL”, actualisé tous les ans par l’ACPR en France."
  },
  {
    "objectID": "articles/GDR/reglementation_prudentielle.html#approche-standard-de-mesure-du-risque-de-marché",
    "href": "articles/GDR/reglementation_prudentielle.html#approche-standard-de-mesure-du-risque-de-marché",
    "title": "La réglementation prudentielle",
    "section": "Approche standard de mesure du risque de marché",
    "text": "Approche standard de mesure du risque de marché\nL’approche standard de mesure du risque de marché consiste à calculer les exigences en fonds propres pour chaque catégorie de risque, à savoir :\n\nle risque de taux (général et spécifique) calculé sur le périmètre du portefeuille de négociation ;\nle risque lié aux titres de propriété(général et spécifique) calculé sur le périmètre du portefeuille de négociation ;\nle risque de change calculé sur l’ensemble des opérations appartenant aussi bien au portefeuille de négociation ou non;\nle risque sur matières premières calculé sur l’ensemble des opérations du portefeuille de négociation ou non;\nles risques opérationnels calculés sur les options associées à chachune des catégories de risque citées ci-dessus.\n\nPar la suite, il s’agit de les additionner de manière arithmétique. Par exemple, pour les titres de propriété, l’exigence de fonds propres est la somme de l’exigence de fonds propres pour le risque général et l’exigence de fonds propres pour le risque spécifique.\nPour le calcul des exigences de fonds propres au titre des risques de marché, il faut tout d’abord déterminer les positions nettes. Les positions de titrisation logées dans le portefeuille de négociation sont traitées comme tout instrument de dette au titre du risque de taux.\nPour le risque spécifique, l’exigence en fonds propres sera la somme des positions nettes multipliées par un coefficient de pondération (2%, 4%, 8% ou 12%) choisi en fonction de la liquidité et la diversification de la position. Pour le risque général, l’exigence en fonds propres est la somme des positions nettes globales (pour chaque marché national) multipliées par 8%."
  },
  {
    "objectID": "articles/GDR/reglementation_prudentielle.html#approche-modèle-interne",
    "href": "articles/GDR/reglementation_prudentielle.html#approche-modèle-interne",
    "title": "La réglementation prudentielle",
    "section": "Approche modèle interne",
    "text": "Approche modèle interne\nL’approche modèle interne est une méthode de calcul des exigences en fonds propres pour le risque de marché qui permet aux établissements de calculer leurs propres exigences. L’exigence en fonds propres est généralement un calcul de la VaR. Cette approche est soumise à des conditions strictes et à une validation par l’ACPR.\nConcernant l’utilisation conjointe des modèles internes et de l’approche standard, la position de la commission prête une attention particulière à la permanence des méthodes ainsi qu’à leur évolution. L’objectif est de s’orienter vers un modèle global qui tient compte de l’ensemble des risques de marché.\n\nAinsi, un établissement commençant à utiliser des modèles pour une ou plusieurs catégories de facteurs de risque doit en principe étendre progressivement ce système à tous ses risques à la méthodologie standardisée (à moins que la Commission Bancaire ne lui ait retiré son agrément pour ses modèles).\n\nPour une banque, la construction d’un modèle interne doit permettre de fournir une mesure plus économique du risque de marché. Au titre de l’article 363 du CRR (Règlement sur les exigences de fonds propres), l’autorité compétente autorise les établissements assujettis à utiliser leurs modèles internes pour calculer les exigences de fonds propres pour risques de marché, après avoir vérifié qu’ils se conforment bien aux exigences des sections 2, 3 et 4 du chapitre 5 du titre IV de la 3ème partie du CRR (“Journal officiel de l’Union européenne,” n.d.). L’autorisation d’utiliser des modèles internes accordée par les autorités compétentes est requise pour chaque catégorie de risques (risque général et spécifique liés aux actions et titres de créance, risque de change et risque sur matières premières), et elle n’est accordée que si le modèle interne couvre une part importante des positions d’une certaine catégorie de risque.\nParmi ces exigences, nous notons des exigences qualitatives (article 368), relatives à la mesure du risque (articles 367) mais aussi d’ordre général (article 365).\n\nExigences générales\nLe calcul de la valeur en risque visée à l’article 364 est soumis aux exigences suivantes:\n\nun calcul quotidien de la valeur en risque;\nun intervalle de confiance, exprimé en centiles et unilatéral, de 99 %;\nune période de détention de dix jours;\nune période effective d’observation historique d’au moins un an, à moins qu’une période d’observation plus courte ne soit justifiée par une augmentation significative de la volatilité des prix;\ndes mises à jour au moins mensuelles des séries de données.\n\nL’établissement peut utiliser des mesures de la valeur en risque calculées sur la base de périodes de détention inférieures à dix jours, qu’il porte à dix jours selon une méthode appropriée qu’il revoit régulièrement.\nChaque établissement doit également calculer, au moins hebdomadairement, une “valeur en risque en situation de tensions” (Stressed VaR) pour son portefeuille courant. Cette Stressed VaR doit être calculée conformément aux mêmes exigences que la VaR standard énoncées plus haut (intervalle de confiance de 99% etc.). Cependant, les données d’entrée du modèle de Stressed VaR doivent être calibrées par rapport à une période historique de tensions financières significatives d’au moins 12 mois, pertinente pour le portefeuille de l’établissement. Le choix de cette période de tensions historiques fait l’objet d’un examen au moins annuel par l’établissement, qui en communique les résultats aux autorités compétentes. L’objectif est de s’assurer que la Stressed VaR reflète de manière adéquate les risques auxquels l’établissement serait exposé en période de crise financière.\nPour résumer, les établissements doivent calculer la perte potentielle quotidiennement pour une période de détention de 10 jours, avec un intervalle de confiance de 99%. Ils doivent également calculer une Stressed VaR au moins une fois par semaine, en utilisant des données historiques de périodes de tensions financières significatives.\nNotons \\(VaR(t)\\) la valeur en risque à la date \\(t\\) et \\(sVaR(t)\\) la valeur en risque en situation de tensions à la date \\(t\\). Les exigences en fonds propres \\(FP(t)\\) à la date t pour le risque de marché sont calculées comme suit:\n\\[FP(t)=max(VaR(t-1),m_c \\times \\frac{1}{60}\\sum_{i=1}^{60}VaR(t-i)) + max(sVaR(t-1),m_s \\times \\frac{1}{60}\\sum_{i=1}^{60}sVaR(t-i))\\]\noù \\(m_c\\) et \\(m_s\\) sont des facteurs multiplicatifs qu’on vera plus tard.\nDans des périodes normales, l’exigence en fonds propres sera donc la somme d’un multiple de la moyenne des valeurs en risque et de la moyenne des valeurs en risque en situation de tensions sur les 60 derniers jours. Ce n’est que dans les périodes de crises financières que l’exigence en fonds propres correspond à la VaR ou à la sVaR du jour précédent.\nChacun des facteurs de multiplication (\\(m_c\\)) et (\\(m_s\\)) est égal à la somme du chiffre 3, au minimum, et d’un cumulateur compris entre 0 et 1 conformément au tableau 1. Ce cumulateur dépend du nombre de dépassements, sur les 250 derniers jours ouvrés, mis en évidence par les contrôles a posteriori de la mesure de la valeur en risque, au sens de l’article 365, paragraphe 1, effectués par l’établissement.\n\n\n\n\n\n\nNombre.de.dépassements\nCumulateur\n\n\n\n\nmoins de 5\n0.00\n\n\n5\n0.40\n\n\n6\n0.50\n\n\n7\n0.65\n\n\n8\n0.75\n\n\n9\n0.85\n\n\n10 ou plus\n1.00\n\n\n\n\n\n\nEn ce qui concerne le risque spécifique, tout modèle interne utilisé pour calculer les exigences de fonds propres et tout modèle interne utilisé pour la négociation en corrélation satisfont aux exigences supplémentaires suivantes:\n\nle modèle interne explique la variation historique des prix à l’inté rieur du portefeuille;\nil reflète la concentration en termes de volume et de modifications de la composition du portefeuille;\nil peut supporter un environnement défavorable;\nil est validé par des contrôles a posteriori(backtesting) visant à établir si le risque spécifique a été correctement pris en compte. Si l’établissement effectue ces contrôles a posteriori sur la base de sous-portefeuilles pertinents, ces derniers sont choisis de manière cohérente;\nil tient compte du risque de base lié à la signature et, en particulier, il est sensible aux différences idiosyncratiques significatives existant entre des positions similaires, mais non identiques;\nil tient compte du risque d’événement.\n\nLe risque spécifique vise à tenir compte du risque de contrepartie lié à l’emetteur de l’instrument.\nPour en savoir plus, reportez au règlement (UE) No 575/2013 du parlement européen du journal officiel de l’Union Européenne, appelé aussi règlement CRR. (voir aussi la notice 2020 relative aux « Modalités de calcul et de publication des ratios prudentiels dans le cadre de la CRDIV»)."
  },
  {
    "objectID": "articles/4-3-2/fluency.html",
    "href": "articles/4-3-2/fluency.html",
    "title": "Improve Speaking Fluency",
    "section": "",
    "text": "Créateur : Romain Vignes ; Droits d’auteur : © Romain Vignes\n\n\n\n\nThis article goal is to provide a brief summary of the article “Improving Speaking Fluency” by Paul Nation written in 1989.\nHis paper analyses the improvement of learners of English using the 4-3-2 method, secribed by Maurice (1983), which consists of 3 time lapses during which the participants have to talk about a particular topic. This method is a way to bring about long-term improvement in English fluency.\n\n\nWhat is Fluency\nFor Filmore (1979), Fluency is the ability to fill time with talk… a person who is fluent in this way does not have to stop many times to think of what t say next or how to phrase it”. Hence, fluency is the ability to speak without hesitation, repetition, or self-correction.\nBrumfit (1984) sees fluency, rather, as the maximally effective operation of the language system so far acquired by the learner. To rephrase it, fluency is the ability to use the language system effectively as one may do for his mothertongue for example.\n\n\nWhat is required to be fluent\nTo develop fluency, one should focus on the message he wants to give. The speaker should not be worried, in a first time, about the grammatical accuracy of his speech. He should try to integrate encountered language itemps into an easily accessed, largely unconscious, language system. This is the result of focussing on the communication of the message.\n\n\nThe 4/3/2 technique\nThe 4/3/2 technique integrated all the previous requirements. It is a way to help learners to develop fluency by giving them the opportunity to speak about a topic for a longer period of time. The technique is based on the idea that the more one speaks, the more he will be able to speak.\nIt has three important step :\n\n4 minutes: the speaker talks about a topic for 4 minutes\n3 minutes: the speaker talks about the same topic for 3 minutes (if possible to another person)\n2 minutes: the speaker talks about the same topic for 2 minutes (if possible to another person)\n\nThe efficiency of this method is based on the fact that the speaker has to talk about the same topic for a longer period of time. This allows him to develop his fluency and to integrate language items into his language system. Also, the fact that the speaker has to talk to different people during the 3 steps allows him to develop his fluency in different contexts but also reduce hesitation, false start and gain confidence.\n\n\n\n\n Back to top"
  },
  {
    "objectID": "about.html",
    "href": "about.html",
    "title": "",
    "section": "",
    "text": "Présentation\nJe m’appelle Cheryl KOUADIO, et je suis fièrement devenue étudiante à l’ENSAI en 2022, admise sur titre. Mon parcours académique avant l’ENSAI inclut un DUT en statistique et informatique décisionnelle. Mon ambition est de me spécialiser dans la filière Gestion des Risques et Ingénierie Financière, un domaine qui m’attire particulièrement en raison de sa complexité et de son importance croissante dans le monde actuel.\n\n\nA propos de ce site\nCe site web a été conçu et généré à l’aide de Quarto, dans le but principal d’offrir un soutien aux étudiants de l’ENSAI qui pourraient se trouver face à des défis similaires à ceux que j’ai rencontrés au cours de mon parcours académique, notamment durant mes 2ème et 3ème années d’étude.\nL’intention derrière la création de ce site n’est pas de remplacer les enseignements prodigués par nos estimés professeurs. Au contraire, il vise à compléter leur travail remarquable en partageant mes expériences personnelles et les projets que j’ai réalisés. L’objectif est de fournir une ressource supplémentaire qui peut aider les étudiants à naviguer dans leurs propres projets et défis académiques.\nCompte tenu de cette orientation vers la gestion des risques, les contenus que je prévois de partager dans la section dédiée à la 3ème année (3A) porteront principalement sur les enseignements spécifiques à cette spécialisation. Je m’efforcerai de rendre ces partages aussi pertinents et enrichissants que possible, en espérant qu’ils serviront de guide précieux pour ceux qui suivront une voie similaire.\n\n\nMot de fin\nce site web est conçu pour être une ressource ouverte à tous. Peu importe votre spécialisation ou votre année d’étude, vous êtes les bienvenus ici. Mon souhait est que “We Stat” devienne un lieu d’échange, d’apprentissage et de soutien pour tous les étudiants de l’ENSAI.\nJe vous souhaite la bienvenue sur StatTogether et espère que vous trouverez les ressources partagées à la fois utiles et inspirantes dans votre parcours académique et professionnel.\n\nLet's stat !\n\n\n\n\n\n\n\n\n\n Back to top"
  }
]